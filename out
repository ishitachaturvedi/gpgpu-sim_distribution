checkpoint.md:**Suppose, each thread is executing 26 instructions and there are 256 threads in a block. You want to checkpoint after 13 instructions in each thread, then Y should be set to = 13\*256/warp\_size = 104, if 32 is the warp size**
checkpoint.md:**This will simulate 4,99,200 (50\*256\*26 + 50\*256\*13) instruction and only block 0 to 49 will pass in kernel 1**
checkpoint.md:**This will simulate 12,04,736 instructions in kernel 1 (50\*256\*0 + 50\*256\*13 + 156\*256\*26 ) and 17,03,936 (256\*256\*26) instructions in kernel 2 and block 0 to 255 will pass in both the kernels**
libcuda/cuda_api.h:      0x04, /**< Set blocking synchronization as default scheduling */
libcuda/cuda_api.h:      0x04, /**< Set blocking synchronization as default scheduling
libcuda/cuda_api.h:  CU_EVENT_BLOCKING_SYNC = 0x1,  /**< Event uses blocking synchronization */
libcuda/cuda_api.h:      1, /**< Maximum number of threads per block */
libcuda/cuda_api.h:  CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X = 2, /**< Maximum block dimension X */
libcuda/cuda_api.h:  CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y = 3, /**< Maximum block dimension Y */
libcuda/cuda_api.h:  CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z = 4, /**< Maximum block dimension Z */
libcuda/cuda_api.h:      8, /**< Maximum shared memory available per block in bytes */
libcuda/cuda_api.h:      12, /**< Maximum number of 32-bit registers available per block */
libcuda/cuda_api.h:      97, /**< Maximum optin shared memory per block */
libcuda/cuda_api.h:  int maxThreadsPerBlock;  /**< Maximum number of threads per block */
libcuda/cuda_api.h:  int maxThreadsDim[3];    /**< Maximum size of each dimension of a block */
libcuda/cuda_api.h:  int sharedMemPerBlock;   /**< Shared memory available per block in bytes */
libcuda/cuda_api.h:  int regsPerBlock; /**< 32-bit registers available per block */
libcuda/cuda_api.h:   * The maximum number of threads per block, beyond which a launch of the
libcuda/cuda_api.h:   * IN: Specifies minimum number of threads per block to target compilation
libcuda/cuda_api.h:   * registers) such that a block with the given number of threads should be
libcuda/cuda_api.h:  unsigned int gridDimX;       /**< Width of grid in blocks */
libcuda/cuda_api.h:  unsigned int gridDimY;       /**< Height of grid in blocks */
libcuda/cuda_api.h:  unsigned int gridDimZ;       /**< Depth of grid in blocks */
libcuda/cuda_api.h:  unsigned int blockDimX;      /**< X dimension of each thread block */
libcuda/cuda_api.h:  unsigned int blockDimY;      /**< Y dimension of each thread block */
libcuda/cuda_api.h:  unsigned int blockDimZ;      /**< Z dimension of each thread block */
libcuda/cuda_api.h:  unsigned int sharedMemBytes; /**< Dynamic shared-memory size per thread block
libcuda/cuda_api.h:   * This error indicates that the number of blocks launched per grid for a
libcuda/cuda_api.h:   * ::cuLaunchCooperativeKernelMultiDevice exceeds the maximum number of blocks
libcuda/cuda_api.h: * Block size to per-block dynamic shared memory mapping for a certain
libcuda/cuda_api.h: * kernel \param blockSize Block size of the kernel.
libcuda/cuda_api.h: * \return The dynamic shared memory needed by a block.
libcuda/cuda_api.h:typedef size_t(CUDA_CB *CUoccupancyB2DSize)(int blockSize);
libcuda/cuda_api.h:  unsigned int gridDimX;       /**< Width of grid in blocks */
libcuda/cuda_api.h:  unsigned int gridDimY;       /**< Height of grid in blocks */
libcuda/cuda_api.h:  unsigned int gridDimZ;       /**< Depth of grid in blocks */
libcuda/cuda_api.h:  unsigned int blockDimX;      /**< X dimension of each thread block */
libcuda/cuda_api.h:  unsigned int blockDimY;      /**< Y dimension of each thread block */
libcuda/cuda_api.h:  unsigned int blockDimZ;      /**< Z dimension of each thread block */
libcuda/cuda_api.h:  unsigned int sharedMemBytes; /**< Dynamic shared-memory size per thread block
libcuda/cuda_api.h: *   block;
libcuda/cuda_api.h: * - ::CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X: Maximum x-dimension of a block;
libcuda/cuda_api.h: * - ::CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y: Maximum y-dimension of a block;
libcuda/cuda_api.h: * - ::CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z: Maximum z-dimension of a block;
libcuda/cuda_api.h: *   shared memory available to a thread block in bytes;
libcuda/cuda_api.h: *   registers available to a thread block;
libcuda/cuda_api.h: * shared by all thread blocks simultaneously resident on a multiprocessor;
libcuda/cuda_api.h: * thread blocks simultaneously resident on a multiprocessor;
libcuda/cuda_api.h: * block shared memory size suported on this device. This is the maximum value
libcuda/cuda_api.h: * - ::maxThreadsPerBlock is the maximum number of threads per block;
libcuda/cuda_api.h: * - ::maxThreadsDim[3] is the maximum sizes of each dimension of a block;
libcuda/cuda_api.h: *   block in bytes;
libcuda/cuda_api.h: * - ::regsPerBlock is the total number of registers available per block;
libcuda/cuda_api.h: * - ::CU_CTX_SCHED_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
libcuda/cuda_api.h: * - ::CU_CTX_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
libcuda/cuda_api.h: * - ::CU_CTX_SCHED_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
libcuda/cuda_api.h: * - ::CU_CTX_BLOCKING_SYNC: Instruct CUDA to block the CPU thread on a
libcuda/cuda_api.h: * CPU thread will block until the GPU context has finished its work.
libcuda/cuda_api.h: * The callback will block later work in the stream until it is finished.
libcuda/cuda_api.h: * a blocking stream in the same context is capturing, it will return
libcuda/cuda_api.h: * after the call. The blocking stream capture is not invalidated.
libcuda/cuda_api.h: * When a blocking stream is capturing, the legacy stream is in an
libcuda/cuda_api.h: * unusable state until the blocking stream capture is terminated. The legacy
libcuda/cuda_api.h: * ::CU_CTX_SCHED_BLOCKING_SYNC flag, the CPU thread will block until the
libcuda/cuda_api.h: * blocking synchronization.  A CPU thread that uses ::cuEventSynchronize() to
libcuda/cuda_api.h: * wait on an event created with this flag will block until the event has
libcuda/cuda_api.h: * flag will cause the calling CPU thread to block until the event has
libcuda/cuda_api.h: * call does not block on completion of the event, and any associated
libcuda/cuda_api.h: * ordered after the operation will block until the given condition on the
libcuda/cuda_api.h: * ordered after the operation will block until the given condition on the
libcuda/cuda_api.h: *   per block, beyond which a launch of the function would fail. This number
libcuda/cuda_api.h: *   statically-allocated shared memory per block required by this function.
libcuda/cuda_api.h: * grid of blocks. Each block contains \p blockDimX x \p blockDimY x
libcuda/cuda_api.h: * \p blockDimZ threads.
libcuda/cuda_api.h: * available to each thread block.
libcuda/cuda_api.h: * block shape, shared size and parameter info associated with \p f
libcuda/cuda_api.h: * \param gridDimX       - Width of grid in blocks
libcuda/cuda_api.h: * \param gridDimY       - Height of grid in blocks
libcuda/cuda_api.h: * \param gridDimZ       - Depth of grid in blocks
libcuda/cuda_api.h: * \param blockDimX      - X dimension of each thread block
libcuda/cuda_api.h: * \param blockDimY      - Y dimension of each thread block
libcuda/cuda_api.h: * \param blockDimZ      - Z dimension of each thread block
libcuda/cuda_api.h: * \param sharedMemBytes - Dynamic shared-memory size per thread block in bytes
libcuda/cuda_api.h:                                unsigned int blockDimX, unsigned int blockDimY,
libcuda/cuda_api.h:                                unsigned int blockDimZ,
libcuda/cuda_api.h: * \brief Launches a CUDA function where thread blocks can cooperate and
libcuda/cuda_api.h: * grid of blocks. Each block contains \p blockDimX x \p blockDimY x
libcuda/cuda_api.h: * \p blockDimZ threads.
libcuda/cuda_api.h: * available to each thread block.
libcuda/cuda_api.h: * The total number of blocks launched cannot exceed the maximum number of
libcuda/cuda_api.h: * blocks per multiprocessor as returned by
libcuda/cuda_api.h: * previous block shape, shared size and parameter info associated with \p f is
libcuda/cuda_api.h: * \param gridDimX       - Width of grid in blocks
libcuda/cuda_api.h: * \param gridDimY       - Height of grid in blocks
libcuda/cuda_api.h: * \param gridDimZ       - Depth of grid in blocks
libcuda/cuda_api.h: * \param blockDimX      - X dimension of each thread block
libcuda/cuda_api.h: * \param blockDimY      - Y dimension of each thread block
libcuda/cuda_api.h: * \param blockDimZ      - Z dimension of each thread block
libcuda/cuda_api.h: * \param sharedMemBytes - Dynamic shared-memory size per thread block in bytes
libcuda/cuda_api.h:    unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY,
libcuda/cuda_api.h:    unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream,
libcuda/cuda_api.h: * \brief Launches CUDA functions on multiple devices where thread blocks can
libcuda/cuda_api.h: * The size of the grids as specified in blocks, the size of the blocks
libcuda/cuda_api.h: * and the amount of shared memory used by each thread block must also match
libcuda/cuda_api.h: * The total number of blocks launched per kernel cannot exceed the maximum
libcuda/cuda_api.h: number of blocks
libcuda/cuda_api.h: * total number of blocks launched per device has to match across all devices,
libcuda/cuda_api.h: * number of blocks that can be launched per device will be limited by the
libcuda/cuda_api.h:            unsigned int blockDimX;
libcuda/cuda_api.h:            unsigned int blockDimY;
libcuda/cuda_api.h:            unsigned int blockDimZ;
libcuda/cuda_api.h: * - ::CUDA_LAUNCH_PARAMS::gridDimX is the width of the grid in blocks. This
libcuda/cuda_api.h: * - ::CUDA_LAUNCH_PARAMS::gridDimY is the height of the grid in blocks. This
libcuda/cuda_api.h: * - ::CUDA_LAUNCH_PARAMS::gridDimZ is the depth of the grid in blocks. This
libcuda/cuda_api.h: * - ::CUDA_LAUNCH_PARAMS::blockDimX is the X dimension of each thread block.
libcuda/cuda_api.h: * - ::CUDA_LAUNCH_PARAMS::blockDimX is the Y dimension of each thread block.
libcuda/cuda_api.h: * - ::CUDA_LAUNCH_PARAMS::blockDimZ is the Z dimension of each thread block.
libcuda/cuda_api.h: thread block in bytes.
libcuda/cuda_api.h: * block shape, shared size and parameter info associated with each
libcuda/cuda_api.h: * after currently enqueued work and will block work added after it.
libcuda/cuda_api.h: * \brief Sets the block-dimensions for the function
libcuda/cuda_api.h: * Specifies the \p x, \p y, and \p z dimensions of the thread blocks that are
libcuda/cuda_api.h: * available to each thread block when the kernel given by \p hfunc is launched.
libcuda/cuda_api.h: * Invokes the kernel \p f on a 1 x 1 x 1 grid of blocks. The block
libcuda/cuda_api.h: * blocks. Each block contains the number of threads specified by a previous
libcuda/cuda_api.h: * \param grid_width  - Width of grid in blocks
libcuda/cuda_api.h: * \param grid_height - Height of grid in blocks
libcuda/cuda_api.h: * blocks. Each block contains the number of threads specified by a previous
libcuda/cuda_api.h: * \param grid_width  - Width of grid in blocks
libcuda/cuda_api.h: * \param grid_height - Height of grid in blocks
libcuda/cuda_api.h: *      unsigned int blockDimX;
libcuda/cuda_api.h: *      unsigned int blockDimY;
libcuda/cuda_api.h: *      unsigned int blockDimZ;
libcuda/cuda_api.h: * gridDimX x \p gridDimY x \p gridDimZ) grid of blocks. Each block contains
libcuda/cuda_api.h: * (\p blockDimX x \p blockDimY x \p blockDimZ) threads.
libcuda/cuda_api.h: * available to each thread block.
libcuda/cuda_api.h: * Returns in \p *numBlocks the number of the maximum active blocks per
libcuda/cuda_api.h: * \param blockSize       - Block size the kernel is intended to be launched
libcuda/cuda_api.h: * with \param dynamicSMemSize - Per-block dynamic shared memory usage intended,
libcuda/cuda_api.h:    int *numBlocks, CUfunction func, int blockSize, size_t dynamicSMemSize);
libcuda/cuda_api.h: * Returns in \p *numBlocks the number of the maximum active blocks per
libcuda/cuda_api.h: *   per-block SM resource usage would result in zero occupancy, the
libcuda/cuda_api.h: * \param blockSize       - Block size the kernel is intended to be launched
libcuda/cuda_api.h: * with \param dynamicSMemSize - Per-block dynamic shared memory usage intended,
libcuda/cuda_api.h:    int *numBlocks, CUfunction func, int blockSize, size_t dynamicSMemSize,
libcuda/cuda_api.h: * Returns in \p *blockSize a reasonable block size that can achieve
libcuda/cuda_api.h: * the fewest blocks per multiprocessor), and in \p *minGridSize the
libcuda/cuda_api.h: * If \p blockSizeLimit is 0, the configurator will use the maximum
libcuda/cuda_api.h: * block size permitted by the device / function instead.
libcuda/cuda_api.h: * If per-block dynamic shared memory allocation is not needed, the
libcuda/cuda_api.h: * user should leave both \p blockSizeToDynamicSMemSize and \p
libcuda/cuda_api.h: * If per-block dynamic shared memory allocation is needed, then if
libcuda/cuda_api.h: * the dynamic shared memory size is constant regardless of block
libcuda/cuda_api.h: * blockSizeToDynamicSMemSize should be NULL.
libcuda/cuda_api.h: * Otherwise, if the per-block dynamic shared memory size varies with
libcuda/cuda_api.h: * different block sizes, the user needs to provide a unary function
libcuda/cuda_api.h: * through \p blockSizeToDynamicSMemSize that computes the dynamic
libcuda/cuda_api.h: * shared memory needed by \p func for any given block size. \p
libcuda/cuda_api.h: *    // Take block size, returns dynamic shared memory needed
libcuda/cuda_api.h: *    size_t blockToSmem(int blockSize);
libcuda/cuda_api.h: * occupancy \param blockSize   - Returned maximum block size that can achieve
libcuda/cuda_api.h: * configuration is calculated \param blockSizeToDynamicSMemSize - A function
libcuda/cuda_api.h: * that calculates how much per-block dynamic shared memory \p func uses based
libcuda/cuda_api.h: * on the block size \param dynamicSMemSize - Dynamic shared memory usage
libcuda/cuda_api.h: * intended, in bytes \param blockSizeLimit  - The maximum block size \p func is
libcuda/cuda_api.h:    int *minGridSize, int *blockSize, CUfunction func,
libcuda/cuda_api.h:    CUoccupancyB2DSize blockSizeToDynamicSMemSize, size_t dynamicSMemSize,
libcuda/cuda_api.h:    int blockSizeLimit);
libcuda/cuda_api.h: * occupancy \param blockSize   - Returned maximum block size that can achieve
libcuda/cuda_api.h: * configuration is calculated \param blockSizeToDynamicSMemSize - A function
libcuda/cuda_api.h: * that calculates how much per-block dynamic shared memory \p func uses based
libcuda/cuda_api.h: * on the block size \param dynamicSMemSize - Dynamic shared memory usage
libcuda/cuda_api.h: * intended, in bytes \param blockSizeLimit  - The maximum block size \p func is
libcuda/cuda_api.h:    int *minGridSize, int *blockSize, CUfunction func,
libcuda/cuda_api.h:    CUoccupancyB2DSize blockSizeToDynamicSMemSize, size_t dynamicSMemSize,
libcuda/cuda_api.h:    int blockSizeLimit, unsigned int flags);
libcuda/cuda_api.h: data. If the resource view format is a block
libcuda/cuda_api.h: *   with 2 or 4 channels, depending on the block compressed format. For ex.,
libcuda/cuda_api.h: data. If the resource view format is a block
libcuda/cuda_api.h: resource. For non block compressed formats,
libcuda/cuda_api.h: data. If the resource view format is a block
libcuda/cuda_api.h: resource. For non block compressed formats,
libcuda/cuda_api.h:                                unsigned int blockDimX, unsigned int blockDimY,
libcuda/cuda_api.h:                                unsigned int blockDimZ,
libcuda/cuda_api.h:    unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY,
libcuda/cuda_api.h:    unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream,
libcuda/cuda_api_object.h:  dim3 block_dim() const { return m_BlockDim; }
libcuda/cuda_api_object.h:  void set_block_dim(dim3 *d) { m_BlockDim = *d; }
libcuda/cuda_api_object.h:                                              struct dim3 blockDim,
libcuda/cuda_runtime_api.cc:    prop->sharedMemPerBlock = the_gpu->shared_mem_per_block();
libcuda/cuda_runtime_api.cc:    prop->sharedMemPerBlock = the_gpu->shared_mem_per_block();
libcuda/cuda_runtime_api.cc:    prop->regsPerBlock = the_gpu->num_registers_per_block();
libcuda/cuda_runtime_api.cc:cudaError_t cudaConfigureCallInternal(dim3 gridDim, dim3 blockDim,
libcuda/cuda_runtime_api.cc:      kernel_config(gridDim, blockDim, sharedMem, s));
libcuda/cuda_runtime_api.cc:    dim3 blockDim = config.block_dim();
libcuda/cuda_runtime_api.cc:        blockDim.x * blockDim.y * blockDim.z == 0) {
libcuda/cuda_runtime_api.cc:      hostFun, config.get_args(), config.grid_dim(), config.block_dim(),
libcuda/cuda_runtime_api.cc:  dim3 blockDim = config.block_dim();
libcuda/cuda_runtime_api.cc:      "blockDim = (%u,%u,%u) \n",
libcuda/cuda_runtime_api.cc:      gridDim.z, blockDim.x, blockDim.y, blockDim.z);
libcuda/cuda_runtime_api.cc:    int *numBlocks, const char *hostFunc, int blockSize, size_t dynamicSMemSize,
libcuda/cuda_runtime_api.cc:      "Calculate Maxium Active Block with function ptr=%p, blockSize=%d, "
libcuda/cuda_runtime_api.cc:      hostFunc, blockSize, dynamicSMemSize);
libcuda/cuda_runtime_api.cc:    dim3 blockDim(blockSize);
libcuda/cuda_runtime_api.cc:    kernel_info_t result(gridDim, blockDim, entry);
libcuda/cuda_runtime_api.cc:    printf("Maximum size is %d with gridDim %d and blockDim %d\n", *numBlocks,
libcuda/cuda_runtime_api.cc:           gridDim.x, blockDim.x);
libcuda/cuda_runtime_api.cc:  // blocking
libcuda/cuda_runtime_api.cc:    const char *hostFun, dim3 gridDim, dim3 blockDim, const void **args,
libcuda/cuda_runtime_api.cc:  cudaConfigureCallInternal(gridDim, blockDim, sharedMem, stream, ctx);
libcuda/cuda_runtime_api.cc:    unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY,
libcuda/cuda_runtime_api.cc:    unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream,
libcuda/cuda_runtime_api.cc:                            dim3(blockDimX, blockDimY, blockDimZ),
libcuda/cuda_runtime_api.cc:    int *numBlocks, const char *hostFunc, int blockSize, size_t dynamicSMemSize,
libcuda/cuda_runtime_api.cc:      numBlocks, hostFunc, blockSize, dynamicSMemSize, flags);
libcuda/cuda_runtime_api.cc:                                                dim3 gridDim, dim3 blockDim,
libcuda/cuda_runtime_api.cc:  return cudaLaunchKernelInternal(hostFun, gridDim, blockDim, args, sharedMem,
libcuda/cuda_runtime_api.cc:unsigned CUDARTAPI __cudaPushCallConfiguration(dim3 gridDim, dim3 blockDim,
libcuda/cuda_runtime_api.cc:  return cudaConfigureCallInternal(gridDim, blockDim, sharedMem, stream);
libcuda/cuda_runtime_api.cc:cudaError_t CUDARTAPI __cudaPopCallConfiguration(dim3 *gridDim, dim3 *blockDim,
libcuda/cuda_runtime_api.cc:__host__ cudaError_t CUDARTAPI cudaConfigureCall(dim3 gridDim, dim3 blockDim,
libcuda/cuda_runtime_api.cc:  return cudaConfigureCallInternal(gridDim, blockDim, sharedMem, stream);
libcuda/cuda_runtime_api.cc: * block
libcuda/cuda_runtime_api.cc: * \ref ::cudaLaunchKernel(const T *func, dim3 gridDim, dim3 blockDim, void
libcuda/cuda_runtime_api.cc:    struct dim3 blockDim, CUctx_st *context) {
libcuda/cuda_runtime_api.cc:      new kernel_info_t(gridDim, blockDim, entry, gpu->getNameArrayMapping(),
libcuda/cuda_runtime_api.cc:                          gridDim, blockDim);
libcuda/cuda_runtime_api.cc:  // all cuLink* function are implemented to block until completion so nothing
libcuda/cuda_runtime_api.cc:                                unsigned int blockDimX, unsigned int blockDimY,
libcuda/cuda_runtime_api.cc:                                unsigned int blockDimZ,
libcuda/cuda_runtime_api.cc:  return cuLaunchKernelInternal(f, gridDimX, gridDimY, gridDimZ, blockDimX,
libcuda/cuda_runtime_api.cc:                                blockDimY, blockDimZ, sharedMemBytes, hStream,
libcuda/cuda_runtime_api.cc:    int *numBlocks, CUfunction func, int blockSize, size_t dynamicSMemSize) {
libcuda/cuda_runtime_api.cc:    int *numBlocks, CUfunction func, int blockSize, size_t dynamicSMemSize,
libcuda/cuda_runtime_api.cc:    int *minGridSize, int *blockSize, CUfunction func,
libcuda/cuda_runtime_api.cc:    CUoccupancyB2DSize blockSizeToDynamicSMemSize, size_t dynamicSMemSize,
libcuda/cuda_runtime_api.cc:    int blockSizeLimit) {
libcuda/cuda_runtime_api.cc:    int *minGridSize, int *blockSize, CUfunction func,
libcuda/cuda_runtime_api.cc:    CUoccupancyB2DSize blockSizeToDynamicSMemSize, size_t dynamicSMemSize,
libcuda/cuda_runtime_api.cc:    int blockSizeLimit, unsigned int flags) {
libcuda/cuda_runtime_api.cc:                                unsigned int blockDimX, unsigned int blockDimY,
libcuda/cuda_runtime_api.cc:                                unsigned int blockDimZ,
libcuda/cuda_runtime_api.cc:    unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY,
libcuda/cuda_runtime_api.cc:    unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream,
cuobjdump_to_ptxplus/ptx.l:call	TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
cuobjdump_to_ptxplus/ptx.l:\.func   TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
cuobjdump_to_ptxplus/ptx.l:<IN_FUNC_DECL>"{"	TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
cuobjdump_to_ptxplus/ptx.y:function_defn: function_decl { set_symtab($1); func_header(".skip"); } statement_block { end_function(); }
cuobjdump_to_ptxplus/ptx.y:	| function_decl { set_symtab($1); } block_spec_list { func_header(".skip"); } statement_block { end_function(); }
cuobjdump_to_ptxplus/ptx.y:block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND {func_header_info_int(".maxntid", $2);
cuobjdump_to_ptxplus/ptx.y:block_spec_list: block_spec
cuobjdump_to_ptxplus/ptx.y:	| block_spec_list block_spec
cuobjdump_to_ptxplus/ptx.y:statement_block: LEFT_BRACE statement_list RIGHT_BRACE 
cuobjdump_to_ptxplus/ptx.y:	| statement_list {start_inst_group();} statement_block {end_inst_group();}
cuobjdump_to_ptxplus/ptx.y:	| {start_inst_group();} statement_block {end_inst_group();}
README.md:	If the above command stops with the message "fatal: unable to access 'https://github.com/tgrogers/gpgpu-sim_simulations.git/': Could not resolve host: github.com" this likely means your computer sits behind a firewall which is blocking access to Google's name servers (e.g., 8.8.8.8).  To get around this you will need to modify th above command to point to your local DNS server.  Lookup your DNS server IP address which we will call <DNS_IP_ADDRESS> below.  On Ubuntu run "ifconfig" to lookup the network interface connecting your computer to the network.  Then run "nmcli device show <interface name>" to find the IP address of your DNS server.  Modify the above command to include "--dns <DNS_IP_ADDRESS>" after "run", E.g.,
Binary file build/gcc-9.4.0/cuda-11060/release/libcuda/libcuda.a matches
build/gcc-9.4.0/cuda-11060/release/libcuda/cuobjdump_lexer.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-9.4.0/cuda-11060/release/libcuda/cuda_runtime_api.o matches
build/gcc-9.4.0/cuda-11060/release/intersim2/lex.yy.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-9.4.0/cuda-11060/release/gpuwattch/XML_Parse.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpuwattch/mcpat matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpuwattch/io.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/cuobjdump_to_ptxplus matches
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/header_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.tab.o matches
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/sass_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/elf_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   14                | block_spec_list block_spec
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:block_spec (182)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:block_spec_list (183)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:statement_block (200)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: . function_decl $@1 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9              | . function_decl $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 99
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: . block_spec
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   14                | . block_spec_list block_spec
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    block_spec       go to state 103
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    block_spec_list  go to state 104
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55               | . statement_list $@12 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: . $@13 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    block_spec  go to state 131
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list . $@12 statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 . statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 157
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 175
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 . statement_block
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 291
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-9.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-9.4.0/cuda-11060/release/abstract_hardware_model.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/stream_manager.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/libgpgpusim.a matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpusim_entrypoint.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/cuda_device_printf.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx_sim.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx-stats.o matches
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.tab.c:  "prototype_block", "prototype_decl", "prototype_call", "prototype_param",
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/instructions.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/lex.ptxinfo_.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/cuda_device_runtime.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/memory.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx_ir.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.tab.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/lex.ptx_.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/libgpgpu_ptx_sim.a matches
build/gcc-9.4.0/cuda-11060/release/cuda-sim/lex.ptxinfo_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-9.4.0/cuda-11060/release/cuda-sim/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); yylval->int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-9.4.0/cuda-11060/release/cuda-sim/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-9.4.0/cuda-11060/release/cuda-sim/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx_parser.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx_loader.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/cuda-sim/cuda-sim.o matches
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   14                | block_spec_list block_spec
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   51               | statement_list prototype_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:block_spec (182)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:block_spec_list (183)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:statement_block (200)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:prototype_block (219)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    9              | function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    statement_block  go to state 99
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    block_spec       go to state 103
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    block_spec_list  go to state 104
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    block_spec  go to state 131
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   51 statement_list: statement_list . prototype_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   56               | statement_list . $@12 statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    prototype_block        go to state 157
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   58 statement_list: $@13 . statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    statement_block  go to state 160
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    statement_block  go to state 178
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 . statement_block
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    statement_block  go to state 295
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   51 statement_list: statement_list prototype_block .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl . prototype_call
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 133 (prototype_block)
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-9.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/l2cache.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/shader.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/power_stat.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/stat-tool.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/gpu-sim.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/scoreboard.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/libgpu_uarch_sim.a matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/dram.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/gpu-cache.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/mem_latency_stat.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/addrdec.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/dram_sched.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/hashing.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/mem_fetch.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/power_interface.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/gpgpu-sim/visualizer.o matches
Binary file build/gcc-9.4.0/cuda-11060/release/debug.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/libcuda/libcuda.a matches
build/gcc-9.4.0/cuda-10010/release/libcuda/cuobjdump_lexer.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-9.4.0/cuda-10010/release/libcuda/cuda_runtime_api.o matches
build/gcc-9.4.0/cuda-10010/release/intersim2/lex.yy.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-9.4.0/cuda-10010/release/gpuwattch/XML_Parse.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpuwattch/mcpat matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpuwattch/io.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/cuobjdump_to_ptxplus matches
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/header_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.o matches
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/sass_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/elf_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14                | block_spec_list block_spec
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:block_spec (182)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:block_spec_list (183)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:statement_block (200)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: . function_decl $@1 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9              | . function_decl $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 99
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: . block_spec
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14                | . block_spec_list block_spec
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec       go to state 103
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec_list  go to state 104
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55               | . statement_list $@12 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: . $@13 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec  go to state 131
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list . $@12 statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 . statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 157
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 175
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 . statement_block
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 291
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-9.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-9.4.0/cuda-10010/release/abstract_hardware_model.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/stream_manager.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/libgpgpusim.a matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpusim_entrypoint.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/cuda_device_printf.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx_sim.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx-stats.o matches
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "prototype_block", "prototype_decl", "prototype_call", "prototype_param",
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/instructions.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/lex.ptxinfo_.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/cuda_device_runtime.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/memory.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx_ir.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.tab.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/lex.ptx_.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/libgpgpu_ptx_sim.a matches
build/gcc-9.4.0/cuda-10010/release/cuda-sim/lex.ptxinfo_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-9.4.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); yylval->int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-9.4.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-9.4.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx_parser.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx_loader.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/cuda-sim/cuda-sim.o matches
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   14                | block_spec_list block_spec
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   51               | statement_list prototype_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:block_spec (182)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:block_spec_list (183)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:statement_block (200)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:prototype_block (219)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    9              | function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 99
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec       go to state 103
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec_list  go to state 104
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec  go to state 131
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   51 statement_list: statement_list . prototype_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   56               | statement_list . $@12 statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    prototype_block        go to state 157
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 . statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 160
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 178
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 . statement_block
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 295
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   51 statement_list: statement_list prototype_block .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl . prototype_call
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 133 (prototype_block)
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-9.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/l2cache.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/shader.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/power_stat.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/stat-tool.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/gpu-sim.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/scoreboard.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/libgpu_uarch_sim.a matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/dram.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/gpu-cache.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/mem_latency_stat.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/addrdec.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/dram_sched.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/hashing.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/mem_fetch.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/power_interface.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/gpgpu-sim/visualizer.o matches
Binary file build/gcc-9.4.0/cuda-10010/release/debug.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/libcuda/libcuda.a matches
build/gcc-8.4.0/cuda-11060/release/libcuda/cuobjdump_lexer.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-8.4.0/cuda-11060/release/libcuda/cuda_runtime_api.o matches
build/gcc-8.4.0/cuda-11060/release/intersim2/lex.yy.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-8.4.0/cuda-11060/release/gpuwattch/XML_Parse.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpuwattch/mcpat matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpuwattch/io.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/cuobjdump_to_ptxplus matches
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/header_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.tab.o matches
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/sass_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/elf_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   14                | block_spec_list block_spec
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:block_spec (182)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:block_spec_list (183)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:statement_block (200)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: . function_decl $@1 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9              | . function_decl $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 99
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: . block_spec
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   14                | . block_spec_list block_spec
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    block_spec       go to state 103
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    block_spec_list  go to state 104
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55               | . statement_list $@12 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: . $@13 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    block_spec  go to state 131
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list . $@12 statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 . statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 157
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 175
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 . statement_block
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 291
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-8.4.0/cuda-11060/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-8.4.0/cuda-11060/release/abstract_hardware_model.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/stream_manager.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/libgpgpusim.a matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpusim_entrypoint.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/cuda_device_printf.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx_sim.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx-stats.o matches
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.tab.c:  "prototype_block", "prototype_decl", "prototype_call", "prototype_param",
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/instructions.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/lex.ptxinfo_.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/cuda_device_runtime.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/memory.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx_ir.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.tab.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/lex.ptx_.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/libgpgpu_ptx_sim.a matches
build/gcc-8.4.0/cuda-11060/release/cuda-sim/lex.ptxinfo_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-8.4.0/cuda-11060/release/cuda-sim/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); yylval->int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-8.4.0/cuda-11060/release/cuda-sim/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-8.4.0/cuda-11060/release/cuda-sim/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx_parser.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx_loader.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/cuda-sim/cuda-sim.o matches
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   14                | block_spec_list block_spec
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   51               | statement_list prototype_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:block_spec (182)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:block_spec_list (183)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:statement_block (200)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:prototype_block (219)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    9              | function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    statement_block  go to state 99
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    block_spec       go to state 103
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    block_spec_list  go to state 104
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    block_spec  go to state 131
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   51 statement_list: statement_list . prototype_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   56               | statement_list . $@12 statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    prototype_block        go to state 157
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   58 statement_list: $@13 . statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    statement_block  go to state 160
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    statement_block  go to state 178
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 . statement_block
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    statement_block  go to state 295
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   51 statement_list: statement_list prototype_block .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl . prototype_call
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 133 (prototype_block)
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-8.4.0/cuda-11060/release/cuda-sim/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/l2cache.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/shader.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/power_stat.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/stat-tool.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/gpu-sim.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/scoreboard.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/libgpu_uarch_sim.a matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/dram.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/gpu-cache.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/mem_latency_stat.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/addrdec.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/dram_sched.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/hashing.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/mem_fetch.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/power_interface.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/gpgpu-sim/visualizer.o matches
Binary file build/gcc-8.4.0/cuda-11060/release/debug.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/libcuda/libcuda.a matches
build/gcc-8.4.0/cuda-10010/release/libcuda/cuobjdump_lexer.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-8.4.0/cuda-10010/release/libcuda/cuda_runtime_api.o matches
build/gcc-8.4.0/cuda-10010/release/intersim2/lex.yy.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-8.4.0/cuda-10010/release/gpuwattch/XML_Parse.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpuwattch/mcpat matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpuwattch/io.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/cuobjdump_to_ptxplus matches
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/header_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.o matches
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/sass_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/elf_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14                | block_spec_list block_spec
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:block_spec (182)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:block_spec_list (183)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:statement_block (200)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: . function_decl $@1 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9              | . function_decl $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 99
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: . block_spec
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14                | . block_spec_list block_spec
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec       go to state 103
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec_list  go to state 104
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55               | . statement_list $@12 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: . $@13 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec  go to state 131
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list . $@12 statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 . statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 157
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 175
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 . statement_block
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 291
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-8.4.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-8.4.0/cuda-10010/release/abstract_hardware_model.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/stream_manager.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/libgpgpusim.a matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpusim_entrypoint.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/cuda_device_printf.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx_sim.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx-stats.o matches
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "prototype_block", "prototype_decl", "prototype_call", "prototype_param",
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/instructions.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/lex.ptxinfo_.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/cuda_device_runtime.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/memory.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx_ir.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.tab.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/lex.ptx_.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/libgpgpu_ptx_sim.a matches
build/gcc-8.4.0/cuda-10010/release/cuda-sim/lex.ptxinfo_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-8.4.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); yylval->int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-8.4.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-8.4.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx_parser.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx_loader.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/cuda-sim/cuda-sim.o matches
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   14                | block_spec_list block_spec
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   51               | statement_list prototype_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:block_spec (182)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:block_spec_list (183)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:statement_block (200)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:prototype_block (219)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    9              | function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 99
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec       go to state 103
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec_list  go to state 104
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec  go to state 131
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   51 statement_list: statement_list . prototype_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   56               | statement_list . $@12 statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    prototype_block        go to state 157
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 . statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 160
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 178
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 . statement_block
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 295
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   51 statement_list: statement_list prototype_block .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl . prototype_call
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 133 (prototype_block)
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-8.4.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/l2cache.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/shader.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/power_stat.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/stat-tool.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/gpu-sim.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/scoreboard.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/libgpu_uarch_sim.a matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/dram.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/gpu-cache.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/mem_latency_stat.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/addrdec.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/dram_sched.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/hashing.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/mem_fetch.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/power_interface.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/gpgpu-sim/visualizer.o matches
Binary file build/gcc-8.4.0/cuda-10010/release/debug.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/libcuda/libcuda.a matches
build/gcc-7.5.0/cuda-10010/release/libcuda/cuobjdump_lexer.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-7.5.0/cuda-10010/release/libcuda/cuda_runtime_api.o matches
build/gcc-7.5.0/cuda-10010/release/intersim2/lex.yy.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-7.5.0/cuda-10010/release/gpuwattch/XML_Parse.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpuwattch/mcpat matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpuwattch/io.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/cuobjdump_to_ptxplus matches
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/header_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.tab.o matches
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/sass_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); ptx_lval.int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/elf_lexer.cc:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14                | block_spec_list block_spec
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:block_spec (182)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:block_spec_list (183)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:statement_block (200)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: . function_decl $@1 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9              | . function_decl $@2 block_spec_list $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 99
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: . block_spec
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14                | . block_spec_list block_spec
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec       go to state 103
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec_list  go to state 104
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55               | . statement_list $@12 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: . $@13 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   13 block_spec_list: block_spec .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: . MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    block_spec  go to state 131
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list . $@12 statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 . statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 157
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 175
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   49 statement_block: . LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 . statement_block
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    statement_block  go to state 291
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   57 statement_list: $@13 statement_block .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   55 statement_list: statement_list $@12 statement_block .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-7.5.0/cuda-10010/release/cuobjdump_to_ptxplus/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-7.5.0/cuda-10010/release/abstract_hardware_model.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/stream_manager.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/libgpgpusim.a matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpusim_entrypoint.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/cuda_device_printf.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx_sim.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx-stats.o matches
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "function_defn", "$@1", "$@2", "$@3", "block_spec", "block_spec_list",
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "statement_block", "statement_list", "$@12", "$@13",
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.tab.c:  "prototype_block", "prototype_decl", "prototype_call", "prototype_param",
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/instructions.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/lex.ptxinfo_.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/cuda_device_runtime.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/memory.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx_ir.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.tab.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/lex.ptx_.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/libgpgpu_ptx_sim.a matches
build/gcc-7.5.0/cuda-10010/release/cuda-sim/lex.ptxinfo_.c:	/* This block is copied from yy_switch_to_buffer. */
build/gcc-7.5.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(NOT_OPCODE); yylval->int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
build/gcc-7.5.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
build/gcc-7.5.0/cuda-10010/release/cuda-sim/lex.ptx_.c:TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/lex.ptx_.c:	/* This block is copied from yy_switch_to_buffer. */
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx_parser.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx_loader.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/cuda-sim/cuda-sim.o matches
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   14                | block_spec_list block_spec
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   51               | statement_list prototype_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:block_spec (182)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:block_spec_list (183)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:statement_block (200)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:prototype_block (219)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl . $@1 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    9              | function_decl . $@2 block_spec_list $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 . statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 99
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 . block_spec_list $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec       go to state 103
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec_list  go to state 104
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE . statement_list RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    6 function_defn: function_decl $@1 statement_block .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE . INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE . INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   13 block_spec_list: block_spec .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 13 (block_spec_list)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list . $@3 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list . block_spec
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    block_spec  go to state 131
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list . RIGHT_BRACE
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   51 statement_list: statement_list . prototype_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   56               | statement_list . $@12 statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    prototype_block        go to state 157
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 . statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 160
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   12 block_spec: MAXNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 12 (block_spec)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND . COMMA INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   11 block_spec: MINNCTAPERSM_DIRECTIVE INT_OPERAND .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 11 (block_spec)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 . statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 178
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   14 block_spec_list: block_spec_list block_spec .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 14 (block_spec_list)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   49 statement_block: LEFT_BRACE statement_list RIGHT_BRACE .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 49 (statement_block)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 . statement_block
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    statement_block  go to state 295
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   51 statement_list: statement_list prototype_block .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl . prototype_call
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   58 statement_list: $@13 statement_block .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA . INT_OPERAND COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    9 function_defn: function_decl $@2 block_spec_list $@3 statement_block .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   56 statement_list: statement_list $@12 statement_block .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:  133 prototype_block: prototype_decl prototype_call .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 133 (prototype_block)
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND . COMMA INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA . INT_OPERAND
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:   10 block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND .
build/gcc-7.5.0/cuda-10010/release/cuda-sim/ptx.output:    $default  reduce using rule 10 (block_spec)
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/l2cache.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/shader.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/power_stat.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/stat-tool.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/gpu-sim.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/scoreboard.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/libgpu_uarch_sim.a matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/dram.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/gpu-cache.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/mem_latency_stat.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/addrdec.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/dram_sched.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/hashing.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/mem_fetch.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/power_interface.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/gpgpu-sim/visualizer.o matches
Binary file build/gcc-7.5.0/cuda-10010/release/debug.o matches
src/abstract_hardware_model.h:  kernel_info_t(dim3 gridDim, dim3 blockDim, class function_info *entry);
src/abstract_hardware_model.h:      dim3 gridDim, dim3 blockDim, class function_info *entry,
src/abstract_hardware_model.h:  size_t num_blocks() const {
src/abstract_hardware_model.h:    return m_block_dim.x * m_block_dim.y * m_block_dim.z;
src/abstract_hardware_model.h:  dim3 get_cta_dim() const { return m_block_dim; }
src/abstract_hardware_model.h:    increment_x_then_y_then_z(m_next_tid, m_block_dim);
src/abstract_hardware_model.h:    return m_next_tid.x + m_block_dim.x * m_next_tid.y +
src/abstract_hardware_model.h:           m_block_dim.x * m_block_dim.y * m_next_tid.z;
src/abstract_hardware_model.h:    return m_next_tid.z < m_block_dim.z && m_next_tid.y < m_block_dim.y &&
src/abstract_hardware_model.h:           m_next_tid.x < m_block_dim.x;
src/abstract_hardware_model.h:  dim3 m_block_dim;
src/gpuwattch/basic_components.h:  double capacity, blockW, assoc, nbanks;
src/gpuwattch/Alpha21364.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Alpha21364.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy,  -->
src/gpuwattch/Alpha21364.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Alpha21364.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Alpha21364.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Alpha21364.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Alpha21364.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Alpha21364.xml:			<param name="block_size" value="16"/><!--B-->
src/gpuwattch/memoryctrl.cc:  // address block. TIDs are assumed to be 8 bits
src/gpuwattch/cacti/cache.cfg://-block size (bytes) 8
src/gpuwattch/cacti/cache.cfg:-block size (bytes) 64
src/gpuwattch/cacti/cache.cfg:#          final data block is broadcasted in data array h-tree 
src/gpuwattch/cacti/parameter.cc:				  g_ip->block_sz * g_ip->tag_assoc * Ndbl * Nspd));// + EPSILON);
src/gpuwattch/cacti/parameter.cc:				  g_ip->block_sz * g_ip->data_assoc * Ndbl * Nspd));// + EPSILON);
src/gpuwattch/cacti/parameter.cc:		  num_c_subarray = (int)ceil((8 * g_ip->block_sz * g_ip->data_assoc * Nspd / Ndwl));// + EPSILON); + EPSILON);
src/gpuwattch/cacti/parameter.cc:		  // burst_length = g_ip->block_sz * 8 / g_ip->out_w;
src/gpuwattch/cacti/parameter.cc:		  tag_num_r_subarray = (int)ceil(capacity_per_die / (g_ip->nbanks*tagbits/8.0 * Ndbl));//TODO: error check input of tagbits and blocksize //TODO: for pure CAM, g_ip->block should be number of entries.
src/gpuwattch/cacti/parameter.cc:			  tagbits = ADDRESS_BITS + EXTRA_TAG_BITS - _log2(g_ip->block_sz);//TODO: should be the page_offset=log2(page size), but this info is not avail with CACTI, for McPAT this is no problem.
src/gpuwattch/cacti/parameter.cc:		  tag_num_r_subarray = (int)(capacity_per_die / (g_ip->nbanks*g_ip->block_sz * Ndbl));
src/gpuwattch/cacti/parameter.cc:		  data_num_c_subarray = 8 * g_ip->block_sz;
src/gpuwattch/cacti/parameter.cc:		  num_so_b_subbank = 8 * g_ip->block_sz;//TODO:internal perfetch should be considered also for fa
src/gpuwattch/cacti/parameter.cc:		  num_di_b_bank_per_port = g_ip->out_w + tagbits;//TODO: out_w or block_sz?
src/gpuwattch/cacti/highradix.h:    Mat * buffer_(double block_sz, double sz);
src/gpuwattch/cacti/cacti_interface.cc:  // address is sent to read/write the appropriate block in the data
src/gpuwattch/cacti/cacti_interface.cc:  // appropriate block over the h-tree.
src/gpuwattch/cacti/uca.cc:  { // if DRAM, add contribution of power spent in row predecoder drivers, blocks and decoders to refresh power
src/gpuwattch/cacti/io.cc:    if (!strncmp("-block", line, strlen("-block"))) {
src/gpuwattch/cacti/io.cc:      sscanf(line, "-block size (bytes) %d", &(line_sz));
src/gpuwattch/cacti/io.cc:    block_sz=0;  // bytes
src/gpuwattch/cacti/io.cc:    cerr << " Need to either increase cache size, or decrease associativity or block size" << endl;
src/gpuwattch/cacti/io.cc:  block_sz = B;
src/gpuwattch/cacti/io.cc:        fr->data_array2->delay_row_predecode_driver_and_block * 1e9 +
src/gpuwattch/cacti/io.cc:        fr->tag_array2->delay_row_predecode_driver_and_block * 1e9 +
src/gpuwattch/cacti/io.cc:    	   fr->data_array2->power_row_predecoder_blocks.readOp.dynamic * 1e9 << endl;
src/gpuwattch/cacti/io.cc:    	   fr->data_array2->power_bit_mux_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:    	   fr->data_array2->power_senseamp_mux_lev_1_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:    	   fr->data_array2->power_senseamp_mux_lev_2_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:    	          	   fr->data_array2->power_row_predecoder_blocks.readOp.dynamic * 1e9 << endl;
src/gpuwattch/cacti/io.cc:    	          	   fr->data_array2->power_bit_mux_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:    	          	   fr->data_array2->power_senseamp_mux_lev_1_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:    	           	   fr->data_array2->power_senseamp_mux_lev_2_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:	          	   fr->data_array2->power_row_predecoder_blocks.readOp.dynamic * 1e9 << endl;
src/gpuwattch/cacti/io.cc:	          	   fr->data_array2->power_bit_mux_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:	          	   fr->data_array2->power_senseamp_mux_lev_1_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:	           	   fr->data_array2->power_senseamp_mux_lev_2_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:        fr->tag_array2->power_row_predecoder_blocks.readOp.dynamic * 1e9 << endl;
src/gpuwattch/cacti/io.cc:        fr->tag_array2->power_bit_mux_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:        fr->tag_array2->power_senseamp_mux_lev_1_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/io.cc:        fr->tag_array2->power_senseamp_mux_lev_2_predecoder_blocks.readOp.dynamic * 1e9 +
src/gpuwattch/cacti/README:block of data. This technique improves reliability at the cost of  
src/gpuwattch/cacti/README:a block to minimize power.
src/gpuwattch/cacti/Ucache.cc:    ptr_array->delay_row_predecode_driver_and_block = uca->bank.mat.r_predec->delay;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_row_predecoder_blocks = uca->bank.mat.r_predec->block_power;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_row_predecoder_blocks.readOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_row_predecoder_blocks.writeOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_row_predecoder_blocks.searchOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_bit_mux_predecoder_blocks  = uca->bank.mat.b_mux_predec->block_power;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_bit_mux_predecoder_blocks.readOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_bit_mux_predecoder_blocks.writeOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_bit_mux_predecoder_blocks.searchOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_senseamp_mux_lev_1_predecoder_blocks = uca->bank.mat.sa_mux_lev_1_predec->block_power;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_senseamp_mux_lev_1_predecoder_blocks.readOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_senseamp_mux_lev_1_predecoder_blocks.writeOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_senseamp_mux_lev_1_predecoder_blocks.searchOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_senseamp_mux_lev_2_predecoder_blocks = uca->bank.mat.sa_mux_lev_2_predec->block_power;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_senseamp_mux_lev_2_predecoder_blocks.readOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_senseamp_mux_lev_2_predecoder_blocks.writeOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    ptr_array->power_senseamp_mux_lev_2_predecoder_blocks.searchOp.dynamic *= num_act_mats_hor_dir;
src/gpuwattch/cacti/Ucache.cc:    	  calc_array[t].Nspd_min    = (double)(g_ip->out_w)/(double)(g_ip->block_sz*8);
src/gpuwattch/cacti/cacti_interface.h:    unsigned int block_sz;  // bytes
src/gpuwattch/cacti/cacti_interface.h:  double delay_row_predecode_driver_and_block;
src/gpuwattch/cacti/cacti_interface.h:  double delay_bit_mux_predecode_driver_and_block;
src/gpuwattch/cacti/cacti_interface.h:  double delay_senseamp_mux_lev_1_predecode_driver_and_block;
src/gpuwattch/cacti/cacti_interface.h:  double delay_senseamp_mux_lev_2_predecode_driver_and_block;
src/gpuwattch/cacti/cacti_interface.h:  powerDef power_row_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h:  powerDef power_bit_mux_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h:  powerDef power_senseamp_mux_lev_1_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h:  powerDef power_senseamp_mux_lev_2_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h://  double perc_power_dyn_row_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h://  double perc_power_dyn_bit_mux_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h://  double perc_power_dyn_senseamp_mux_lev_1_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h://  double perc_power_dyn_senseamp_mux_lev_2_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h://  double perc_power_leak_row_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h://  double perc_power_leak_bit_mux_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h://  double perc_power_leak_senseamp_mux_lev_1_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h://  double perc_power_leak_senseamp_mux_lev_2_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h:         delay_row_predecode_driver_and_block,
src/gpuwattch/cacti/cacti_interface.h:  powerDef power_row_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h:  powerDef power_bit_mux_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h:  powerDef power_senseamp_mux_lev_1_predecoder_blocks;
src/gpuwattch/cacti/cacti_interface.h:  powerDef power_senseamp_mux_lev_2_predecoder_blocks;
src/gpuwattch/cacti/highradix.cc:Mat * HighRadix::buffer_(double block_sz, double sz)
src/gpuwattch/cacti/highradix.cc:  dyn_p.num_r_subarray = (int) (sz/block_sz);
src/gpuwattch/cacti/highradix.cc:  dyn_p.num_c_subarray = (int) block_sz;
src/gpuwattch/cacti/highradix.cc:  dyn_p.num_do_b_subbank = (int)block_sz;
src/gpuwattch/cacti/highradix.cc:  dyn_p.num_do_b_mat = (int) block_sz;
src/gpuwattch/cacti/highradix.cc:  dyn_p.num_di_b_mat = (int) block_sz;
src/gpuwattch/cacti/highradix.cc:  dyn_p.out_w = (int) block_sz;
src/gpuwattch/cacti/mat.cc:      else //for pre-decode block's load is same for both FA and CAM
src/gpuwattch/cacti/decoder.cc:      // Just one predecoder block is required with NAND2 gates. No decoder required.
src/gpuwattch/cacti/decoder.cc:  { // find number of gates and widths in first level of predecoder block when there is no second level
src/gpuwattch/cacti/decoder.cc:  { // First check whether a predecoder block is needed
src/gpuwattch/cacti/decoder.cc:  // first check whether a predecoder block is required
src/gpuwattch/cacti/decoder.cc:    //Find delay in first level of predecoder block
src/gpuwattch/cacti/decoder.cc:  { // First check whether a predecoder block is needed
src/gpuwattch/cacti/decoder.cc:  // The predecode block driver accepts as input the address bits from the h-tree network. For
src/gpuwattch/cacti/decoder.cc:  { // first check whether a predecoder block driver is needed
src/gpuwattch/cacti/decoder.cc:    // Final inverter drives the predecoder block or the decoder output load
src/gpuwattch/cacti/decoder.cc:    // Final inverter drives the predecoder block or the decoder output load
src/gpuwattch/cacti/decoder.cc:  block_power.readOp.leakage = blk1->power_nand2_path.readOp.leakage +
src/gpuwattch/cacti/decoder.cc:  power.readOp.leakage = driver_power.readOp.leakage + block_power.readOp.leakage;
src/gpuwattch/cacti/decoder.cc:  block_power.readOp.gate_leakage = blk1->power_nand2_path.readOp.gate_leakage +
src/gpuwattch/cacti/decoder.cc:  power.readOp.gate_leakage = driver_power.readOp.gate_leakage + block_power.readOp.gate_leakage;
src/gpuwattch/cacti/decoder.cc:  { // first check whether a predecoder block driver is needed
src/gpuwattch/cacti/decoder.cc:  // TODO: Jung Ho thinks that predecoder block driver locates between decoder and predecoder block.
src/gpuwattch/cacti/decoder.cc:  block_power.readOp.dynamic =
src/gpuwattch/cacti/decoder.cc:  power.readOp.dynamic = driver_power.readOp.dynamic + block_power.readOp.dynamic;
src/gpuwattch/cacti/decoder.cc:  block_power.readOp.leakage = blk1->power_nand2_path.readOp.leakage +
src/gpuwattch/cacti/decoder.cc:  power.readOp.leakage = driver_power.readOp.leakage + block_power.readOp.leakage;
src/gpuwattch/cacti/decoder.cc:  block_power.readOp.gate_leakage = blk1->power_nand2_path.readOp.gate_leakage +
src/gpuwattch/cacti/decoder.cc:  power.readOp.gate_leakage = driver_power.readOp.gate_leakage + block_power.readOp.gate_leakage;
src/gpuwattch/cacti/nuca.cc:            (g_ip->block_sz*8 + 64) + avg_vhop *
src/gpuwattch/cacti/nuca.cc:            (g_ip->block_sz*8 + 64) + ures.power.readOp.dynamic;
src/gpuwattch/cacti/const.h://that each row predecode block is composed of at least one 2-4 decoder. When the outputs from the
src/gpuwattch/cacti/const.h://row predecode blocks are combined this means that there are at least 4*4=16 row decode outputs
src/gpuwattch/cacti/const.h:#define MAXSUBARRAYROWS 262144 //Each row predecode block produces a max of 2^9 outputs. So
src/gpuwattch/cacti/decoder.h:    powerDef block_power;
src/gpuwattch/Niagara1_sharing_DC.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Niagara1_sharing_DC.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_DC.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_DC.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_DC.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_DC.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_DC.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_DC.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/gpgpu.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/gpgpu.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/gpgpu.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/gpgpu.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/gpgpu.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/gpgpu.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/gpgpu.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/gpgpu.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/gpgpu.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/Niagara1.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Niagara1.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/sharedcache.cc:  line = cachep.blockW;
src/gpuwattch/sharedcache.cc:          ceil(XML->sys.number_of_cores / 8.0) * 8 / (cachep.blockW * 8);
src/gpuwattch/sharedcache.cc:      line = cachep.blockW * (1 + dir_overhead);
src/gpuwattch/sharedcache.cc:                                   // cache block
src/gpuwattch/sharedcache.cc:    cachep.blockW = XML->sys.L2[ithCache].L2_config[1];
src/gpuwattch/sharedcache.cc:    cachep.blockW = XML->sys.L3[ithCache].L3_config[1];
src/gpuwattch/sharedcache.cc:    cachep.blockW = XML->sys.L1Directory[ithCache].Dir_config[1];
src/gpuwattch/sharedcache.cc:    cachep.blockW = XML->sys.L2Directory[ithCache].Dir_config[1];
src/gpuwattch/Niagara1_sharing.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Niagara1_sharing.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/quadro.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/quadro.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/quadro.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/quadro.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/quadro.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/quadro.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/quadro.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/quadro.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/quadro.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/Xeon.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Xeon.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy,  -->
src/gpuwattch/Xeon.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Xeon.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Xeon.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Xeon.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Xeon.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Xeon.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/gpgpu_static.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/gpgpu_static.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/gpgpu_static.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/gpgpu_static.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/gpgpu_static.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/gpgpu_static.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/gpgpu_static.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/gpgpu_static.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/gpgpu_static.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/Niagara1_sharing_SBT.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Niagara1_sharing_SBT.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_SBT.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_SBT.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_SBT.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_SBT.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_SBT.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_SBT.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/Niagara2.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Niagara2.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara2.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara2.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara2.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara2.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara2.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara2.xml:			<param name="block_size" value="64"/><!--(B) the block size of last level cache, which is the unit for one memory burst transfer -->
src/gpuwattch/Niagara1_sharing_ST.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Niagara1_sharing_ST.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_ST.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_ST.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_ST.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Niagara1_sharing_ST.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_ST.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Niagara1_sharing_ST.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/arch_const.h:  unsigned int blocksize;
src/gpuwattch/arch_const.h:const int itlbblocksize = 8;
src/gpuwattch/arch_const.h:const int icacheblocksize = 32;
src/gpuwattch/arch_const.h:const int dtlbblocksize = 8;
src/gpuwattch/arch_const.h:const int dcacheblocksize = 32;
src/gpuwattch/arch_const.h:const int IBblocksize = 4;
src/gpuwattch/arch_const.h:const int IFBblocksize = 4;
src/gpuwattch/arch_const.h:const int regfileblocksize = 18;
src/gpuwattch/arch_const.h:const int regwinblocksize = 8;
src/gpuwattch/arch_const.h:const int lsqblocksize = 8;
src/gpuwattch/arch_const.h:const int dfqblocksize = 16;
src/gpuwattch/arch_const.h:const int l2cacheblocksize = 64;
src/gpuwattch/arch_const.h:const int l2dirblocksize = 2;
src/gpuwattch/arch_const.h:const int pcx_bufferblocksize = 32;
src/gpuwattch/arch_const.h:const int pcx_arbblocksize = 2;
src/gpuwattch/arch_const.h:const int cpx_bufferblocksize = 32;
src/gpuwattch/arch_const.h:const int cpx_arbblocksize = 2;
src/gpuwattch/arch_const.h:subblock_size=0
src/gpuwattch/Penryn.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/Penryn.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy,  -->
src/gpuwattch/Penryn.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Penryn.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Penryn.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Penryn.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/Penryn.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/Penryn.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/fermi.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
src/gpuwattch/fermi.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/fermi.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/fermi.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/fermi.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/fermi.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
src/gpuwattch/fermi.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/fermi.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
src/gpuwattch/fermi.xml:			<param name="block_size" value="64"/><!--B-->
src/gpuwattch/XML_Parse.cc:                   "block_size") == 0) {
src/gpuwattch/XML_Parse.cc:        //(strcmp(xNode3.getChildNode("param",k).getAttribute("name"),"block_size")==0)
src/abstract_hardware_model.cc:  new_addr_type cache_block_size = 0;  // in bytes
src/abstract_hardware_model.cc:      cache_block_size = m_config->gpgpu_cache_texl1_linesize;
src/abstract_hardware_model.cc:      cache_block_size = m_config->gpgpu_cache_constl1_linesize;
src/abstract_hardware_model.cc:  if (cache_block_size) {
src/abstract_hardware_model.cc:        accesses;  // block address -> set of thread offsets in warp
src/abstract_hardware_model.cc:      unsigned block_address = line_size_based_tag_func(addr, cache_block_size);
src/abstract_hardware_model.cc:      accesses[block_address].set(thread);
src/abstract_hardware_model.cc:      unsigned idx = addr - block_address;
src/abstract_hardware_model.cc:          access_type, a->first, cache_block_size, is_write, a->second,
src/abstract_hardware_model.cc:        unsigned block_address = line_size_based_tag_func(addr, segment_size);
src/abstract_hardware_model.cc:        transaction_info &info = subwarp_transactions[block_address];
src/abstract_hardware_model.cc:        // segment assert(block_address ==
src/abstract_hardware_model.cc:        if (block_address != line_size_based_tag_func(
src/abstract_hardware_model.cc:          unsigned block_address = line_size_based_tag_func(addr, segment_size);
src/abstract_hardware_model.cc:          transaction_info &info = subwarp_transactions[block_address];
src/abstract_hardware_model.cc:        subwarp_transactions;  // each block addr maps to a list of transactions
src/abstract_hardware_model.cc:      unsigned block_address = line_size_based_tag_func(addr, segment_size);
src/abstract_hardware_model.cc:      assert(block_address ==
src/abstract_hardware_model.cc:      for (it = subwarp_transactions[block_address].begin();
src/abstract_hardware_model.cc:           it != subwarp_transactions[block_address].end(); it++) {
src/abstract_hardware_model.cc:        subwarp_transactions[block_address].push_back(transaction_info());
src/abstract_hardware_model.cc:        info = &subwarp_transactions[block_address].back();
src/abstract_hardware_model.cc:      // For each block addr
src/abstract_hardware_model.cc:kernel_info_t::kernel_info_t(dim3 gridDim, dim3 blockDim,
src/abstract_hardware_model.cc:  m_block_dim = blockDim;
src/abstract_hardware_model.cc:      num_blocks() * entry->gpgpu_ctx->device_runtime->g_TB_launch_latency;
src/abstract_hardware_model.cc:    dim3 gridDim, dim3 blockDim, class function_info *entry,
src/abstract_hardware_model.cc:  m_block_dim = blockDim;
src/abstract_hardware_model.cc:      num_blocks() * entry->gpgpu_ctx->device_runtime->g_TB_launch_latency;
src/stream_manager.h:  CUevent_st(bool blocking) {
src/stream_manager.h:    m_blocking = blocking;
src/stream_manager.h:  bool m_blocking;
src/stream_manager.h:  stream_manager(gpgpu_sim *gpu, bool cuda_launch_blocking);
src/stream_manager.h:  bool is_blocking() { return m_cuda_launch_blocking; };
src/stream_manager.h:  bool m_cuda_launch_blocking;
src/stream_manager.cc:stream_manager::stream_manager(gpgpu_sim *gpu, bool cuda_launch_blocking) {
src/stream_manager.cc:  m_cuda_launch_blocking = cuda_launch_blocking;
src/stream_manager.cc:  // block if stream 0 (or concurrency disabled) and pending concurrent
src/stream_manager.cc:  bool block = !stream || m_cuda_launch_blocking;
src/stream_manager.cc:  while (block) {
src/stream_manager.cc:    block = !concurrent_streams_empty();
src/stream_manager.cc:    if (stream && !m_cuda_launch_blocking) {
src/stream_manager.cc:  if (m_cuda_launch_blocking || stream == NULL) {
src/cuda-sim/instructions.cc:  int x_block_coord, y_block_coord, memreqindex, blockoffset;
src/cuda-sim/instructions.cc:      x_block_coord = x >> (texInfo->Tx_numbits + texInfo->texel_size_numbits);
src/cuda-sim/instructions.cc:      y_block_coord = y >> texInfo->Ty_numbits;
src/cuda-sim/instructions.cc:          ((y_block_coord * cuArray->width / texInfo->Tx) + x_block_coord) << 6;
src/cuda-sim/instructions.cc:      blockoffset = (x % (texInfo->Tx * texInfo->texel_size) +
src/cuda-sim/instructions.cc:      memreqindex += blockoffset;
src/cuda-sim/memory.h:  void read_single_block(mem_addr_t blk_idx, mem_addr_t addr, size_t length,
src/cuda-sim/memory.h:  unsigned m_log2_block_size;
src/cuda-sim/cuda-sim.cc:   create_basic_blocks();
src/cuda-sim/cuda-sim.cc:   connect_basic_blocks();
src/cuda-sim/cuda-sim.cc:      print_basic_blocks();
src/cuda-sim/cuda-sim.cc:      print_basic_block_links();
src/cuda-sim/cuda-sim.cc:      print_basic_block_dot();
src/cuda-sim/cuda-sim.cc:    memory_space *param_mem, gpgpu_t *gpu, dim3 gridDim, dim3 blockDim) {
src/cuda-sim/cuda-sim.cc:          blockDim.x, blockDim.y, blockDim.z);
src/cuda-sim/cuda-sim.cc:    struct dim3 gridDim, struct dim3 blockDim, gpgpu_t *gpu) {
src/cuda-sim/cuda-sim.cc:      new kernel_info_t(gridDim, blockDim, entry, gpu->getNameArrayMapping(),
src/cuda-sim/cuda-sim.cc:  char *blocking = getenv("CUDA_LAUNCH_BLOCKING");
src/cuda-sim/cuda-sim.cc:  if (blocking && !strcmp(blocking, "1")) {
src/cuda-sim/cuda-sim.cc:    g_cuda_launch_blocking = true;
src/cuda-sim/cuda-sim.cc:  g_cuda_launch_blocking = true;
src/cuda-sim/cuda-sim.cc:  // functions work block wise
src/cuda-sim/memory.cc:  m_log2_block_size = -1;
src/cuda-sim/memory.cc:      assert(m_log2_block_size == (unsigned)-1);
src/cuda-sim/memory.cc:      m_log2_block_size = n;
src/cuda-sim/memory.cc:  assert(m_log2_block_size != (unsigned)-1);
src/cuda-sim/memory.cc:  mem_addr_t index = addr >> m_log2_block_size;
src/cuda-sim/memory.cc:    // fast route for intra-block access
src/cuda-sim/memory.cc:    // slow route for inter-block access
src/cuda-sim/memory.cc:      mem_addr_t page = current_addr >> m_log2_block_size;
src/cuda-sim/memory.cc:void memory_space_impl<BSIZE>::read_single_block(mem_addr_t blk_idx,
src/cuda-sim/memory.cc:  mem_addr_t index = addr >> m_log2_block_size;
src/cuda-sim/memory.cc:    // fast route for intra-block access
src/cuda-sim/memory.cc:    read_single_block(index, addr, length, data);
src/cuda-sim/memory.cc:    // slow route for inter-block access
src/cuda-sim/memory.cc:      mem_addr_t page = current_addr >> m_log2_block_size;
src/cuda-sim/memory.cc:      read_single_block(page, current_addr, tx_bytes,
src/cuda-sim/ptx.l:call	TC; BEGIN(NOT_OPCODE); yylval->int_value = CALL_OP; return OPCODE; // blocking opcode token in case the callee has the same name as an opcode
src/cuda-sim/ptx.l:\.func   TC; BEGIN(IN_FUNC_DECL); return FUNC_DIRECTIVE; // blocking opcode parsing in case the function has the same name as an opcode (e.g. sin(), cos())
src/cuda-sim/ptx.l:<IN_FUNC_DECL>"{"	TC; BEGIN(INITIAL); return LEFT_BRACE; // starting a statement block (allow next token to be parsed as an opcode)
src/cuda-sim/cuda_device_runtime.h:  device_launch_config_t(dim3 _grid_dim, dim3 _block_dim,
src/cuda-sim/cuda_device_runtime.h:        block_dim(_block_dim),
src/cuda-sim/cuda_device_runtime.h:  dim3 block_dim;
src/cuda-sim/cuda_device_runtime.cc:// blockDimension, unsigned int sharedMemSize)
src/cuda-sim/cuda_device_runtime.cc:  struct dim3 grid_dim, block_dim;
src/cuda-sim/cuda_device_runtime.cc:    } else if (arg == 2) {  // dim3 block_dim for the child kernel
src/cuda-sim/cuda_device_runtime.cc:      thread->m_local_mem->read(from_addr, size, &block_dim);
src/cuda-sim/cuda_device_runtime.cc:      DEV_RUNTIME_REPORT("block (" << block_dim.x << ", " << block_dim.y << ", "
src/cuda-sim/cuda_device_runtime.cc:                                   << block_dim.z << ")");
src/cuda-sim/cuda_device_runtime.cc:  device_launch_config_t device_launch_config(grid_dim, block_dim, shared_mem,
src/cuda-sim/cuda_device_runtime.cc:          config.grid_dim, config.block_dim, device_kernel_entry,
src/cuda-sim/ptx_ir.h:struct basic_block_t {
src/cuda-sim/ptx_ir.h:  basic_block_t(unsigned ID, ptx_instruction *begin, ptx_instruction *end,
src/cuda-sim/ptx_ir.h:      predecessor_ids;  // indices of other basic blocks in m_basic_blocks array
src/cuda-sim/ptx_ir.h:  // if this basic block dom B
src/cuda-sim/ptx_ir.h:  bool dom(const basic_block_t *B) {
src/cuda-sim/ptx_ir.h:  // if this basic block pdom B
src/cuda-sim/ptx_ir.h:  bool pdom(const basic_block_t *B) {
src/cuda-sim/ptx_ir.h:      basic_block_t *basic_block)  // assign instruction to a basic block
src/cuda-sim/ptx_ir.h:    m_basic_block = basic_block;
src/cuda-sim/ptx_ir.h:  basic_block_t *get_bb() { return m_basic_block; }
src/cuda-sim/ptx_ir.h:  basic_block_t *m_basic_block;
src/cuda-sim/ptx_ir.h:  void create_basic_blocks();
src/cuda-sim/ptx_ir.h:  void print_basic_blocks();
src/cuda-sim/ptx_ir.h:  void print_basic_block_links();
src/cuda-sim/ptx_ir.h:  void print_basic_block_dot();
src/cuda-sim/ptx_ir.h:  void connect_basic_blocks();  // iterate across m_basic_blocks of function,
src/cuda-sim/ptx_ir.h:                                // connecting basic blocks together
src/cuda-sim/ptx_ir.h:  // iterate across m_basic_blocks of function,
src/cuda-sim/ptx_ir.h:  // finding dominator blocks, using algorithm of
src/cuda-sim/ptx_ir.h:  // iterate across m_basic_blocks of function,
src/cuda-sim/ptx_ir.h:  // finding postdominator blocks, using algorithm of
src/cuda-sim/ptx_ir.h:  // iterate across m_basic_blocks of function,
src/cuda-sim/ptx_ir.h:  // finding immediate postdominator blocks, using algorithm of
src/cuda-sim/ptx_ir.h:                      dim3 blockDim);
src/cuda-sim/ptx_ir.h:  std::vector<basic_block_t *> m_basic_blocks;
src/cuda-sim/ptx_ir.h:      Ty;  // tiling factor dimensions of layout of texels per 64B cache block
src/cuda-sim/ptx_sim.h:  void set_single_thread_single_block() {
src/cuda-sim/ptx_ir.cc:void function_info::create_basic_blocks() {
src/cuda-sim/ptx_ir.cc:    printf("GPGPU-Sim PTX: Function \'%s\' has no basic blocks\n",
src/cuda-sim/ptx_ir.cc:  m_basic_blocks.push_back(
src/cuda-sim/ptx_ir.cc:      new basic_block_t(bb_id++, *find_next_real_instruction(i), NULL, 1, 0));
src/cuda-sim/ptx_ir.cc:      // found start of next basic block
src/cuda-sim/ptx_ir.cc:      m_basic_blocks.back()->ptx_end = last_real_inst;
src/cuda-sim/ptx_ir.cc:        m_basic_blocks.push_back(new basic_block_t(
src/cuda-sim/ptx_ir.cc:    pI->assign_bb(m_basic_blocks.back());
src/cuda-sim/ptx_ir.cc:  m_basic_blocks.back()->ptx_end = last_real_inst;
src/cuda-sim/ptx_ir.cc:  m_basic_blocks.push_back(
src/cuda-sim/ptx_ir.cc:      /*exit basic block*/ new basic_block_t(bb_id, NULL, NULL, 0, 1));
src/cuda-sim/ptx_ir.cc:void function_info::print_basic_blocks() {
src/cuda-sim/ptx_ir.cc:  printf("Printing basic blocks for function \'%s\':\n", m_name.c_str());
src/cuda-sim/ptx_ir.cc:  printf("\nSummary of basic blocks for \'%s\':\n", m_name.c_str());
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::iterator bb_itr;
src/cuda-sim/ptx_ir.cc:  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
src/cuda-sim/ptx_ir.cc:void function_info::print_basic_block_links() {
src/cuda-sim/ptx_ir.cc:  printf("Printing basic blocks links for function \'%s\':\n", m_name.c_str());
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::iterator bb_itr;
src/cuda-sim/ptx_ir.cc:  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
src/cuda-sim/ptx_ir.cc:  const basic_block_t *break_bb = p_break_insn->get_bb();
src/cuda-sim/ptx_ir.cc:  for (const basic_block_t *p_bb = break_bb; p_bb->immediatedominator_id != -1;
src/cuda-sim/ptx_ir.cc:       p_bb = m_basic_blocks[p_bb->immediatedominator_id]) {
src/cuda-sim/ptx_ir.cc:    // reverse search through instructions in basic block for breakaddr
src/cuda-sim/ptx_ir.cc:void function_info::connect_basic_blocks()  // iterate across m_basic_blocks of
src/cuda-sim/ptx_ir.cc:                                            // function, connecting basic blocks
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::iterator bb_itr;
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::iterator bb_target_itr;
src/cuda-sim/ptx_ir.cc:  basic_block_t *exit_bb = m_basic_blocks.back();
src/cuda-sim/ptx_ir.cc:  // start from first basic block, which we know is the entry point
src/cuda-sim/ptx_ir.cc:  bb_itr = m_basic_blocks.begin();
src/cuda-sim/ptx_ir.cc:  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
src/cuda-sim/ptx_ir.cc:    if ((*bb_itr)->is_exit)  // reached last basic block, no successors to link
src/cuda-sim/ptx_ir.cc:        // if predicated, add link to next block
src/cuda-sim/ptx_ir.cc:          basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
src/cuda-sim/ptx_ir.cc:      // find successor and link that basic_block to this one
src/cuda-sim/ptx_ir.cc:      basic_block_t *target_bb = target_pI->get_bb();
src/cuda-sim/ptx_ir.cc:      // if basic block does not end in an unpredicated branch,
src/cuda-sim/ptx_ir.cc:      // then next basic block is also successor
src/cuda-sim/ptx_ir.cc:      basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::iterator bb_itr;
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::iterator bb_target_itr;
src/cuda-sim/ptx_ir.cc:  // start from first basic block, which we know is the entry point
src/cuda-sim/ptx_ir.cc:  bb_itr = m_basic_blocks.begin();
src/cuda-sim/ptx_ir.cc:  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
src/cuda-sim/ptx_ir.cc:    basic_block_t *p_bb = *bb_itr;
src/cuda-sim/ptx_ir.cc:    if (p_bb->is_exit)  // reached last basic block, no successors to link
src/cuda-sim/ptx_ir.cc:        basic_block_t *successor_bb = m_basic_blocks[*succ_ids];
src/cuda-sim/ptx_ir.cc:      // find successor and link that basic_block to this one
src/cuda-sim/ptx_ir.cc:      basic_block_t *target_bb = target_pI->get_bb();
src/cuda-sim/ptx_ir.cc:        // predicated break - add link to next basic block
src/cuda-sim/ptx_ir.cc:        basic_block_t *next_bb = m_instr_mem[next_addr]->get_bb();
src/cuda-sim/ptx_ir.cc:  create_basic_blocks();
src/cuda-sim/ptx_ir.cc:  connect_basic_blocks();
src/cuda-sim/ptx_ir.cc:    print_basic_blocks();
src/cuda-sim/ptx_ir.cc:    print_basic_block_links();
src/cuda-sim/ptx_ir.cc:    print_basic_block_dot();
src/cuda-sim/ptx_ir.cc:  assert(m_basic_blocks.size() >= 2);  // must have a distinquished entry block
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::iterator bb_itr = m_basic_blocks.begin();
src/cuda-sim/ptx_ir.cc:      (*bb_itr)->bb_id);  // the only dominator of the entry block is the entry
src/cuda-sim/ptx_ir.cc:  // copy all basic blocks to all dominator lists EXCEPT for the entry block
src/cuda-sim/ptx_ir.cc:  for (++bb_itr; bb_itr != m_basic_blocks.end(); bb_itr++) {
src/cuda-sim/ptx_ir.cc:    for (unsigned i = 0; i < m_basic_blocks.size(); i++)
src/cuda-sim/ptx_ir.cc:    for (int h = 1 /*skip entry*/; h < m_basic_blocks.size(); ++h) {
src/cuda-sim/ptx_ir.cc:      assert(m_basic_blocks[h]->bb_id == (unsigned)h);
src/cuda-sim/ptx_ir.cc:      for (unsigned i = 0; i < m_basic_blocks.size(); i++) T.insert(i);
src/cuda-sim/ptx_ir.cc:               m_basic_blocks[h]->predecessor_ids.begin();
src/cuda-sim/ptx_ir.cc:           s != m_basic_blocks[h]->predecessor_ids.end(); s++)
src/cuda-sim/ptx_ir.cc:        intersect(T, m_basic_blocks[*s]->dominator_ids);
src/cuda-sim/ptx_ir.cc:      if (!is_equal(T, m_basic_blocks[h]->dominator_ids)) {
src/cuda-sim/ptx_ir.cc:        m_basic_blocks[h]->dominator_ids = T;
src/cuda-sim/ptx_ir.cc:  // clean the basic block of dominators of it has no predecessors -- except for
src/cuda-sim/ptx_ir.cc:  // entry block
src/cuda-sim/ptx_ir.cc:  bb_itr = m_basic_blocks.begin();
src/cuda-sim/ptx_ir.cc:  for (++bb_itr; bb_itr != m_basic_blocks.end(); bb_itr++) {
src/cuda-sim/ptx_ir.cc:  assert(m_basic_blocks.size() >= 2);  // must have a distinquished exit block
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::reverse_iterator bb_itr =
src/cuda-sim/ptx_ir.cc:      m_basic_blocks.rbegin();
src/cuda-sim/ptx_ir.cc:          ->bb_id);  // the only postdominator of the exit block is the exit
src/cuda-sim/ptx_ir.cc:  for (++bb_itr; bb_itr != m_basic_blocks.rend();
src/cuda-sim/ptx_ir.cc:       bb_itr++) {  // copy all basic blocks to all postdominator lists EXCEPT
src/cuda-sim/ptx_ir.cc:                    // for the exit block
src/cuda-sim/ptx_ir.cc:    for (unsigned i = 0; i < m_basic_blocks.size(); i++)
src/cuda-sim/ptx_ir.cc:    for (int h = m_basic_blocks.size() - 2 /*skip exit*/; h >= 0; --h) {
src/cuda-sim/ptx_ir.cc:      assert(m_basic_blocks[h]->bb_id == (unsigned)h);
src/cuda-sim/ptx_ir.cc:      for (unsigned i = 0; i < m_basic_blocks.size(); i++) T.insert(i);
src/cuda-sim/ptx_ir.cc:      for (std::set<int>::iterator s = m_basic_blocks[h]->successor_ids.begin();
src/cuda-sim/ptx_ir.cc:           s != m_basic_blocks[h]->successor_ids.end(); s++)
src/cuda-sim/ptx_ir.cc:        intersect(T, m_basic_blocks[*s]->postdominator_ids);
src/cuda-sim/ptx_ir.cc:      if (!is_equal(T, m_basic_blocks[h]->postdominator_ids)) {
src/cuda-sim/ptx_ir.cc:        m_basic_blocks[h]->postdominator_ids = T;
src/cuda-sim/ptx_ir.cc:  // find immediate postdominator blocks, using algorithm of
src/cuda-sim/ptx_ir.cc:  assert(m_basic_blocks.size() >= 2);  // must have a distinquished exit block
src/cuda-sim/ptx_ir.cc:  for (unsigned i = 0; i < m_basic_blocks.size();
src/cuda-sim/ptx_ir.cc:    m_basic_blocks[i]->Tmp_ids = m_basic_blocks[i]->postdominator_ids;
src/cuda-sim/ptx_ir.cc:    assert(m_basic_blocks[i]->bb_id == i);
src/cuda-sim/ptx_ir.cc:    m_basic_blocks[i]->Tmp_ids.erase(i);
src/cuda-sim/ptx_ir.cc:  for (int n = m_basic_blocks.size() - 2; n >= 0; --n) {
src/cuda-sim/ptx_ir.cc:    // point iterator to basic block before the exit
src/cuda-sim/ptx_ir.cc:    for (std::set<int>::iterator s = m_basic_blocks[n]->Tmp_ids.begin();
src/cuda-sim/ptx_ir.cc:         s != m_basic_blocks[n]->Tmp_ids.end(); s++) {
src/cuda-sim/ptx_ir.cc:      for (std::set<int>::iterator t = m_basic_blocks[n]->Tmp_ids.begin();
src/cuda-sim/ptx_ir.cc:           t != m_basic_blocks[n]->Tmp_ids.end();) {
src/cuda-sim/ptx_ir.cc:        if (m_basic_blocks[bb_s]->postdominator_ids.find(bb_t) !=
src/cuda-sim/ptx_ir.cc:            m_basic_blocks[bb_s]->postdominator_ids.end())
src/cuda-sim/ptx_ir.cc:          m_basic_blocks[n]->Tmp_ids.erase(bb_t);
src/cuda-sim/ptx_ir.cc:  for (int n = m_basic_blocks.size() - 1; n >= 0; --n) {
src/cuda-sim/ptx_ir.cc:    assert(m_basic_blocks[n]->Tmp_ids.size() <= 1);
src/cuda-sim/ptx_ir.cc:    if (!m_basic_blocks[n]->Tmp_ids.empty()) {
src/cuda-sim/ptx_ir.cc:      m_basic_blocks[n]->immediatepostdominator_id =
src/cuda-sim/ptx_ir.cc:          *m_basic_blocks[n]->Tmp_ids.begin();
src/cuda-sim/ptx_ir.cc:  assert(num_ipdoms == m_basic_blocks.size() - 1);
src/cuda-sim/ptx_ir.cc:  // find immediate dominator blocks, using algorithm of
src/cuda-sim/ptx_ir.cc:  assert(m_basic_blocks.size() >= 2);  // must have a distinquished entry block
src/cuda-sim/ptx_ir.cc:  for (unsigned i = 0; i < m_basic_blocks.size();
src/cuda-sim/ptx_ir.cc:    m_basic_blocks[i]->Tmp_ids = m_basic_blocks[i]->dominator_ids;
src/cuda-sim/ptx_ir.cc:    assert(m_basic_blocks[i]->bb_id == i);
src/cuda-sim/ptx_ir.cc:    m_basic_blocks[i]->Tmp_ids.erase(i);
src/cuda-sim/ptx_ir.cc:  for (int n = 0; n < m_basic_blocks.size(); ++n) {
src/cuda-sim/ptx_ir.cc:    // point iterator to basic block before the exit
src/cuda-sim/ptx_ir.cc:    for (std::set<int>::iterator s = m_basic_blocks[n]->Tmp_ids.begin();
src/cuda-sim/ptx_ir.cc:         s != m_basic_blocks[n]->Tmp_ids.end(); s++) {
src/cuda-sim/ptx_ir.cc:      for (std::set<int>::iterator t = m_basic_blocks[n]->Tmp_ids.begin();
src/cuda-sim/ptx_ir.cc:           t != m_basic_blocks[n]->Tmp_ids.end();) {
src/cuda-sim/ptx_ir.cc:        if (m_basic_blocks[bb_s]->dominator_ids.find(bb_t) !=
src/cuda-sim/ptx_ir.cc:            m_basic_blocks[bb_s]->dominator_ids.end())
src/cuda-sim/ptx_ir.cc:          m_basic_blocks[n]->Tmp_ids.erase(bb_t);
src/cuda-sim/ptx_ir.cc:  for (int n = 0; n < m_basic_blocks.size(); ++n) {
src/cuda-sim/ptx_ir.cc:    // assert( m_basic_blocks[n]->Tmp_ids.size() <= 1 );
src/cuda-sim/ptx_ir.cc:    if (!m_basic_blocks[n]->Tmp_ids.empty()) {
src/cuda-sim/ptx_ir.cc:      m_basic_blocks[n]->immediatedominator_id =
src/cuda-sim/ptx_ir.cc:          *m_basic_blocks[n]->Tmp_ids.begin();
src/cuda-sim/ptx_ir.cc:    } else if (m_basic_blocks[n]->predecessor_ids.empty()) {
src/cuda-sim/ptx_ir.cc:  assert(num_idoms == m_basic_blocks.size() - num_nopred);
src/cuda-sim/ptx_ir.cc:  for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
src/cuda-sim/ptx_ir.cc:    for (std::set<int>::iterator j = m_basic_blocks[i]->dominator_ids.begin();
src/cuda-sim/ptx_ir.cc:         j != m_basic_blocks[i]->dominator_ids.end(); j++)
src/cuda-sim/ptx_ir.cc:  for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
src/cuda-sim/ptx_ir.cc:             m_basic_blocks[i]->postdominator_ids.begin();
src/cuda-sim/ptx_ir.cc:         j != m_basic_blocks[i]->postdominator_ids.end(); j++)
src/cuda-sim/ptx_ir.cc:  for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
src/cuda-sim/ptx_ir.cc:    printf("%d\n", m_basic_blocks[i]->immediatepostdominator_id);
src/cuda-sim/ptx_ir.cc:  for (unsigned i = 0; i < m_basic_blocks.size(); i++) {
src/cuda-sim/ptx_ir.cc:    printf("%d\n", m_basic_blocks[i]->immediatedominator_id);
src/cuda-sim/ptx_ir.cc:    if (m_basic_blocks.size() == 0) return 0;
src/cuda-sim/ptx_ir.cc:    for (unsigned i = 0; i < (m_basic_blocks.size() - 1);
src/cuda-sim/ptx_ir.cc:         i++) {  // last basic block containing exit obviously won't have a pair
src/cuda-sim/ptx_ir.cc:      if (m_basic_blocks[i]->ptx_end->get_opcode() == BRA_OP) {
src/cuda-sim/ptx_ir.cc:  if (m_basic_blocks.size() == 0) return;
src/cuda-sim/ptx_ir.cc:  for (unsigned i = 0; i < (m_basic_blocks.size() - 1);
src/cuda-sim/ptx_ir.cc:       i++) {  // last basic block containing exit obviously won't have a pair
src/cuda-sim/ptx_ir.cc:    if (m_basic_blocks[i]->ptx_end->get_opcode() == BRA_OP) {
src/cuda-sim/ptx_ir.cc:      printf("\tbb_id=%d; ipdom=%d\n", m_basic_blocks[i]->bb_id,
src/cuda-sim/ptx_ir.cc:             m_basic_blocks[i]->immediatepostdominator_id);
src/cuda-sim/ptx_ir.cc:             m_basic_blocks[i]->ptx_end->get_m_instr_mem_index());
src/cuda-sim/ptx_ir.cc:      recon_points[idx].source_pc = m_basic_blocks[i]->ptx_end->get_PC();
src/cuda-sim/ptx_ir.cc:      recon_points[idx].source_inst = m_basic_blocks[i]->ptx_end;
src/cuda-sim/ptx_ir.cc:      if (m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]
src/cuda-sim/ptx_ir.cc:            m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]
src/cuda-sim/ptx_ir.cc:            m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]
src/cuda-sim/ptx_ir.cc:      m_basic_blocks[m_basic_blocks[i]->immediatepostdominator_id]
src/cuda-sim/ptx_ir.cc:void function_info::print_basic_block_dot() {
src/cuda-sim/ptx_ir.cc:  std::vector<basic_block_t *>::iterator bb_itr;
src/cuda-sim/ptx_ir.cc:  for (bb_itr = m_basic_blocks.begin(); bb_itr != m_basic_blocks.end();
src/cuda-sim/ptx.y:function_defn: function_decl { recognizer->set_symtab($1); recognizer->func_header(".skip"); } statement_block { recognizer->end_function(); }
src/cuda-sim/ptx.y:	| function_decl { recognizer->set_symtab($1); } block_spec_list { recognizer->func_header(".skip"); } statement_block { recognizer->end_function(); }
src/cuda-sim/ptx.y:block_spec: MAXNTID_DIRECTIVE INT_OPERAND COMMA INT_OPERAND COMMA INT_OPERAND {recognizer->func_header_info_int(".maxntid", $2);
src/cuda-sim/ptx.y:block_spec_list: block_spec
src/cuda-sim/ptx.y:	| block_spec_list block_spec
src/cuda-sim/ptx.y:statement_block: LEFT_BRACE statement_list RIGHT_BRACE 
src/cuda-sim/ptx.y:    | statement_list prototype_block {printf("Prototype statement detected. WARNING: this is not supported yet on GPGPU-SIM\n"); }
src/cuda-sim/ptx.y:	| statement_list {recognizer->start_inst_group();} statement_block {recognizer->end_inst_group();}
src/cuda-sim/ptx.y:	| {recognizer->start_inst_group();} statement_block {recognizer->end_inst_group();}
src/cuda-sim/ptx.y:prototype_block: prototype_decl prototype_call
src/cuda-sim/cuda-sim.h:  // initializes threads in the CTA block which we are executing
src/cuda-sim/cuda-sim.h:    g_cuda_launch_blocking = false;
src/cuda-sim/cuda-sim.h:  bool g_cuda_launch_blocking;
src/cuda-sim/cuda-sim.h:                                                struct dim3 blockDim,
src/gpgpusim_entrypoint.cc:      (the_gpgpusim->g_the_gpu), func_sim->g_cuda_launch_blocking);
src/gpgpu-sim/gpu-sim.h:  int shared_mem_per_block() const;
src/gpgpu-sim/gpu-sim.h:  int num_registers_per_block() const;
src/gpgpu-sim/gpu-sim.h:  void issue_block2core();
src/gpgpu-sim/shader.h:  unsigned gpgpu_shmem_per_block;
src/gpgpu-sim/shader.h:  unsigned gpgpu_registers_per_block;
src/gpgpu-sim/shader.h:  void issue_block2core(class kernel_info_t &kernel);
src/gpgpu-sim/shader.h:  unsigned m_n_active_cta;  // number of Cooperative Thread Arrays (blocks)
src/gpgpu-sim/shader.h:  bool can_issue_1block(kernel_info_t &kernel);
src/gpgpu-sim/shader.h:  bool occupy_shader_resource_1block(kernel_info_t &kernel, bool occupy);
src/gpgpu-sim/shader.h:  void release_shader_resource_1block(unsigned hw_ctaid, kernel_info_t &kernel);
src/gpgpu-sim/shader.h:  unsigned issue_block2core();
src/gpgpu-sim/gpu-cache.h:enum cache_block_state { INVALID = 0, RESERVED, VALID, MODIFIED };
src/gpgpu-sim/gpu-cache.h:struct evicted_block_info {
src/gpgpu-sim/gpu-cache.h:  new_addr_type m_block_addr;
src/gpgpu-sim/gpu-cache.h:  evicted_block_info() {
src/gpgpu-sim/gpu-cache.h:    m_block_addr = 0;
src/gpgpu-sim/gpu-cache.h:  void set_info(new_addr_type block_addr, unsigned modified_size) {
src/gpgpu-sim/gpu-cache.h:    m_block_addr = block_addr;
src/gpgpu-sim/gpu-cache.h:  evicted_block_info m_evicted_block;  // if it was write_back event, fill the
src/gpgpu-sim/gpu-cache.h:                                       // the evicted block info
src/gpgpu-sim/gpu-cache.h:              evicted_block_info evicted_block) {
src/gpgpu-sim/gpu-cache.h:    m_evicted_block = evicted_block;
src/gpgpu-sim/gpu-cache.h:struct cache_block_t {
src/gpgpu-sim/gpu-cache.h:  cache_block_t() {
src/gpgpu-sim/gpu-cache.h:    m_block_addr = 0;
src/gpgpu-sim/gpu-cache.h:  virtual void allocate(new_addr_type tag, new_addr_type block_addr,
src/gpgpu-sim/gpu-cache.h:  virtual enum cache_block_state get_status(
src/gpgpu-sim/gpu-cache.h:  virtual void set_status(enum cache_block_state m_status,
src/gpgpu-sim/gpu-cache.h:  virtual ~cache_block_t() {}
src/gpgpu-sim/gpu-cache.h:  new_addr_type m_block_addr;
src/gpgpu-sim/gpu-cache.h:struct line_cache_block : public cache_block_t {
src/gpgpu-sim/gpu-cache.h:  line_cache_block() {
src/gpgpu-sim/gpu-cache.h:  void allocate(new_addr_type tag, new_addr_type block_addr, unsigned time,
src/gpgpu-sim/gpu-cache.h:    m_block_addr = block_addr;
src/gpgpu-sim/gpu-cache.h:  virtual enum cache_block_state get_status(
src/gpgpu-sim/gpu-cache.h:  virtual void set_status(enum cache_block_state status,
src/gpgpu-sim/gpu-cache.h:    printf("m_block_addr is %llu, status = %u\n", m_block_addr, m_status);
src/gpgpu-sim/gpu-cache.h:  cache_block_state m_status;
src/gpgpu-sim/gpu-cache.h:struct sector_cache_block : public cache_block_t {
src/gpgpu-sim/gpu-cache.h:  sector_cache_block() { init(); }
src/gpgpu-sim/gpu-cache.h:  virtual void allocate(new_addr_type tag, new_addr_type block_addr,
src/gpgpu-sim/gpu-cache.h:    allocate_line(tag, block_addr, time, sector_mask);
src/gpgpu-sim/gpu-cache.h:  void allocate_line(new_addr_type tag, new_addr_type block_addr, unsigned time,
src/gpgpu-sim/gpu-cache.h:    // assert(m_block_addr != 0 && m_block_addr != block_addr);
src/gpgpu-sim/gpu-cache.h:    m_block_addr = block_addr;
src/gpgpu-sim/gpu-cache.h:  virtual enum cache_block_state get_status(
src/gpgpu-sim/gpu-cache.h:  virtual void set_status(enum cache_block_state status,
src/gpgpu-sim/gpu-cache.h:    printf("m_block_addr is %llu, status = %u %u %u %u\n", m_block_addr,
src/gpgpu-sim/gpu-cache.h:  cache_block_state m_status[SECTOR_CHUNCK_SIZE];
src/gpgpu-sim/gpu-cache.h:    // for hit/miss. Tag is now identical to the block address.
src/gpgpu-sim/gpu-cache.h:  new_addr_type block_addr(new_addr_type addr) const {
src/gpgpu-sim/gpu-cache.h:                                   evicted_block_info &evicted, mem_fetch *mf);
src/gpgpu-sim/gpu-cache.h:  cache_block_t *get_block(unsigned idx) { return m_lines[idx]; }
src/gpgpu-sim/gpu-cache.h:            cache_block_t **new_lines);
src/gpgpu-sim/gpu-cache.h:  cache_block_t **m_lines; /* nbanks x nset x assoc lines in total */
src/gpgpu-sim/gpu-cache.h:  bool probe(new_addr_type block_addr) const;
src/gpgpu-sim/gpu-cache.h:  bool full(new_addr_type block_addr) const;
src/gpgpu-sim/gpu-cache.h:  void add(new_addr_type block_addr, mem_fetch *mf);
src/gpgpu-sim/gpu-cache.h:  void mark_ready(new_addr_type block_addr, bool &has_atomic);
src/gpgpu-sim/gpu-cache.h:  bool is_read_after_write_pending(new_addr_type block_addr);
src/gpgpu-sim/gpu-cache.h:      m_block_addr = a;
src/gpgpu-sim/gpu-cache.h:    new_addr_type m_block_addr;
src/gpgpu-sim/gpu-cache.h:  void send_read_request(new_addr_type addr, new_addr_type block_addr,
src/gpgpu-sim/gpu-cache.h:  void send_read_request(new_addr_type addr, new_addr_type block_addr,
src/gpgpu-sim/gpu-cache.h:                         bool &do_miss, bool &wb, evicted_block_info &evicted,
src/gpgpu-sim/gpu-cache.h:  /// Marks block as MODIFIED and updates block LRU
src/gpgpu-sim/gpu-cache.h:  /// Marks block as INVALID and sends write request to lower level memory
src/gpgpu-sim/gpu-cache.h:/// the granularity of individual blocks
src/gpgpu-sim/gpu-cache.h:    m_cache = new data_block[config.get_num_lines()];
src/gpgpu-sim/gpu-cache.h:  /// Place returning cache block into reorder buffer
src/gpgpu-sim/gpu-cache.h:      m_block_addr = a;
src/gpgpu-sim/gpu-cache.h:    unsigned m_index;  // where in cache should block be placed?
src/gpgpu-sim/gpu-cache.h:    new_addr_type m_block_addr;
src/gpgpu-sim/gpu-cache.h:  struct data_block {
src/gpgpu-sim/gpu-cache.h:    data_block() { m_valid = false; }
src/gpgpu-sim/gpu-cache.h:    new_addr_type m_block_addr;
src/gpgpu-sim/gpu-cache.h:  data_block *m_cache;
src/gpgpu-sim/gpu-cache.cc:                     cache_block_t **new_lines)
src/gpgpu-sim/gpu-cache.cc:  m_lines = new cache_block_t *[cache_lines_num];
src/gpgpu-sim/gpu-cache.cc:      m_lines[i] = new line_cache_block();
src/gpgpu-sim/gpu-cache.cc:      m_lines[i] = new sector_cache_block();
src/gpgpu-sim/gpu-cache.cc:  new_addr_type addr = m_config.block_addr(mf->get_addr());
src/gpgpu-sim/gpu-cache.cc:  new_addr_type addr = m_config.block_addr(mf->get_addr());
src/gpgpu-sim/gpu-cache.cc:    cache_block_t *line = m_lines[index];
src/gpgpu-sim/gpu-cache.cc:    abort();  // if an unreserved block exists, it is either invalid or
src/gpgpu-sim/gpu-cache.cc:        pending_lines.find(m_config.block_addr(addr));
src/gpgpu-sim/gpu-cache.cc:  evicted_block_info evicted;
src/gpgpu-sim/gpu-cache.cc:                                            evicted_block_info &evicted,
src/gpgpu-sim/gpu-cache.cc:          evicted.set_info(m_lines[idx]->m_block_addr,
src/gpgpu-sim/gpu-cache.cc:        m_lines[idx]->allocate(m_config.tag(addr), m_config.block_addr(addr),
src/gpgpu-sim/gpu-cache.cc:        ((sector_cache_block *)m_lines[idx])
src/gpgpu-sim/gpu-cache.cc:    m_lines[idx]->allocate(m_config.tag(addr), m_config.block_addr(addr), time,
src/gpgpu-sim/gpu-cache.cc:    ((sector_cache_block *)m_lines[idx])->allocate_sector(time, mask);
src/gpgpu-sim/gpu-cache.cc:bool mshr_table::probe(new_addr_type block_addr) const {
src/gpgpu-sim/gpu-cache.cc:  table::const_iterator a = m_data.find(block_addr);
src/gpgpu-sim/gpu-cache.cc:bool mshr_table::full(new_addr_type block_addr) const {
src/gpgpu-sim/gpu-cache.cc:  table::const_iterator i = m_data.find(block_addr);
src/gpgpu-sim/gpu-cache.cc:void mshr_table::add(new_addr_type block_addr, mem_fetch *mf) {
src/gpgpu-sim/gpu-cache.cc:  m_data[block_addr].m_list.push_back(mf);
src/gpgpu-sim/gpu-cache.cc:  assert(m_data[block_addr].m_list.size() <= m_max_merged);
src/gpgpu-sim/gpu-cache.cc:    m_data[block_addr].m_has_atomic = true;
src/gpgpu-sim/gpu-cache.cc:bool mshr_table::is_read_after_write_pending(new_addr_type block_addr) {
src/gpgpu-sim/gpu-cache.cc:  std::list<mem_fetch *> my_list = m_data[block_addr].m_list;
src/gpgpu-sim/gpu-cache.cc:void mshr_table::mark_ready(new_addr_type block_addr, bool &has_atomic) {
src/gpgpu-sim/gpu-cache.cc:  table::iterator a = m_data.find(block_addr);
src/gpgpu-sim/gpu-cache.cc:  m_current_response.push_back(block_addr);
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_current_response.front();
src/gpgpu-sim/gpu-cache.cc:  assert(!m_data[block_addr].m_list.empty());
src/gpgpu-sim/gpu-cache.cc:  mem_fetch *result = m_data[block_addr].m_list.front();
src/gpgpu-sim/gpu-cache.cc:  m_data[block_addr].m_list.pop_front();
src/gpgpu-sim/gpu-cache.cc:  if (m_data[block_addr].m_list.empty()) {
src/gpgpu-sim/gpu-cache.cc:    m_data.erase(block_addr);
src/gpgpu-sim/gpu-cache.cc:    unsigned block_addr = e->first;
src/gpgpu-sim/gpu-cache.cc:    fprintf(fp, "MSHR: tag=0x%06x, atomic=%d %zu entries : ", block_addr,
src/gpgpu-sim/gpu-cache.cc:        unsigned data_cycles = ev.m_evicted_block.m_modified_size / port_width;
src/gpgpu-sim/gpu-cache.cc:    m_tag_array->fill(e->second.m_block_addr, time, mf);
src/gpgpu-sim/gpu-cache.cc:  m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);
src/gpgpu-sim/gpu-cache.cc:    cache_block_t *block = m_tag_array->get_block(e->second.m_cache_index);
src/gpgpu-sim/gpu-cache.cc:    block->set_status(MODIFIED,
src/gpgpu-sim/gpu-cache.cc:                                       new_addr_type block_addr,
src/gpgpu-sim/gpu-cache.cc:  evicted_block_info e;
src/gpgpu-sim/gpu-cache.cc:  send_read_request(addr, block_addr, cache_index, mf, time, do_miss, wb, e,
src/gpgpu-sim/gpu-cache.cc:                                       new_addr_type block_addr,
src/gpgpu-sim/gpu-cache.cc:                                       evicted_block_info &evicted,
src/gpgpu-sim/gpu-cache.cc:      m_tag_array->access(block_addr, time, cache_index, mf);
src/gpgpu-sim/gpu-cache.cc:      m_tag_array->access(block_addr, time, cache_index, wb, evicted, mf);
src/gpgpu-sim/gpu-cache.cc:      m_tag_array->access(block_addr, time, cache_index, mf);
src/gpgpu-sim/gpu-cache.cc:      m_tag_array->access(block_addr, time, cache_index, wb, evicted, mf);
src/gpgpu-sim/gpu-cache.cc:/// Write-back hit: Mark block as modified
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:  m_tag_array->access(block_addr, time, cache_index, mf);  // update LRU state
src/gpgpu-sim/gpu-cache.cc:  cache_block_t *block = m_tag_array->get_block(cache_index);
src/gpgpu-sim/gpu-cache.cc:  block->set_status(MODIFIED, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:  m_tag_array->access(block_addr, time, cache_index, mf);  // update LRU state
src/gpgpu-sim/gpu-cache.cc:  cache_block_t *block = m_tag_array->get_block(cache_index);
src/gpgpu-sim/gpu-cache.cc:  block->set_status(MODIFIED, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:/// corresponding block
src/gpgpu-sim/gpu-cache.cc:  cache_block_t *block = m_tag_array->get_block(cache_index);
src/gpgpu-sim/gpu-cache.cc:  // Invalidate block
src/gpgpu-sim/gpu-cache.cc:  block->set_status(INVALID, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:// and send a read request for the same block
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:  // if(!send_write_allocate(mf, addr, block_addr, cache_index, time, events))
src/gpgpu-sim/gpu-cache.cc:  evicted_block_info evicted;
src/gpgpu-sim/gpu-cache.cc:  send_read_request(addr, block_addr, cache_index, n_mf, time, do_miss, wb,
src/gpgpu-sim/gpu-cache.cc:    // If evicted block is modified and not a write-through
src/gpgpu-sim/gpu-cache.cc:          evicted.m_block_addr, m_wrbk_type, evicted.m_modified_size, true,
src/gpgpu-sim/gpu-cache.cc:      // the evicted block may have wrong chip id when advanced L2 hashing  is
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:    evicted_block_info evicted;
src/gpgpu-sim/gpu-cache.cc:        m_tag_array->access(block_addr, time, cache_index, wb, evicted, mf);
src/gpgpu-sim/gpu-cache.cc:    cache_block_t *block = m_tag_array->get_block(cache_index);
src/gpgpu-sim/gpu-cache.cc:    block->set_status(MODIFIED, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:      block->set_ignore_on_fill(true, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:      // If evicted block is modified and not a write-through
src/gpgpu-sim/gpu-cache.cc:            evicted.m_block_addr, m_wrbk_type, evicted.m_modified_size, true,
src/gpgpu-sim/gpu-cache.cc:        // the evicted block may have wrong chip id when advanced L2 hashing  is
src/gpgpu-sim/gpu-cache.cc:    new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:    evicted_block_info evicted;
src/gpgpu-sim/gpu-cache.cc:    send_read_request(addr, block_addr, cache_index, n_mf, time, do_miss, wb,
src/gpgpu-sim/gpu-cache.cc:    cache_block_t *block = m_tag_array->get_block(cache_index);
src/gpgpu-sim/gpu-cache.cc:    block->set_modified_on_fill(true, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:      // If evicted block is modified and not a write-through
src/gpgpu-sim/gpu-cache.cc:            evicted.m_block_addr, m_wrbk_type, evicted.m_modified_size, true,
src/gpgpu-sim/gpu-cache.cc:        // the evicted block may have wrong chip id when advanced L2 hashing  is
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:  evicted_block_info evicted;
src/gpgpu-sim/gpu-cache.cc:      m_tag_array->access(block_addr, time, cache_index, wb, evicted, mf);
src/gpgpu-sim/gpu-cache.cc:  cache_block_t *block = m_tag_array->get_block(cache_index);
src/gpgpu-sim/gpu-cache.cc:  block->set_status(MODIFIED, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:    block->set_ignore_on_fill(true, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:    block->set_modified_on_fill(true, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:    block->set_m_readable(true, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:    block->set_m_readable(false, mf->get_access_sector_mask());
src/gpgpu-sim/gpu-cache.cc:    // If evicted block is modified and not a write-through
src/gpgpu-sim/gpu-cache.cc:          evicted.m_block_addr, m_wrbk_type, evicted.m_modified_size, true,
src/gpgpu-sim/gpu-cache.cc:      // the evicted block may have wrong chip id when advanced L2 hashing  is
src/gpgpu-sim/gpu-cache.cc:/// Baseline read hit: Update LRU status of block.
src/gpgpu-sim/gpu-cache.cc:// Special case for atomic instructions -> Mark block as modified
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:  m_tag_array->access(block_addr, time, cache_index, mf);
src/gpgpu-sim/gpu-cache.cc:    cache_block_t *block = m_tag_array->get_block(cache_index);
src/gpgpu-sim/gpu-cache.cc:    block->set_status(MODIFIED,
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:  evicted_block_info evicted;
src/gpgpu-sim/gpu-cache.cc:  send_read_request(addr, block_addr, cache_index, mf, time, do_miss, wb,
src/gpgpu-sim/gpu-cache.cc:    // If evicted block is modified and not a write-through
src/gpgpu-sim/gpu-cache.cc:          evicted.m_block_addr, m_wrbk_type, evicted.m_modified_size, true,
src/gpgpu-sim/gpu-cache.cc:      // the evicted block may have wrong chip id when advanced L2 hashing  is
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:      m_tag_array->probe(block_addr, cache_index, mf);
src/gpgpu-sim/gpu-cache.cc:    cache_status = m_tag_array->access(block_addr, time, cache_index,
src/gpgpu-sim/gpu-cache.cc:      send_read_request(addr, block_addr, cache_index, mf, time, do_miss,
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:      m_tag_array->probe(block_addr, cache_index, mf, true);
src/gpgpu-sim/gpu-cache.cc:/// granularity of individual blocks (Set by GPGPU-Sim configuration file)
src/gpgpu-sim/gpu-cache.cc:  new_addr_type block_addr = m_config.block_addr(addr);
src/gpgpu-sim/gpu-cache.cc:      m_tags.access(block_addr, time, cache_index, mf);
src/gpgpu-sim/gpu-cache.cc:    unsigned rob_index = m_rob.push(rob_entry(cache_index, mf, block_addr));
src/gpgpu-sim/gpu-cache.cc:    m_tags.fill(cache_index, time, mf);  // mark block as valid
src/gpgpu-sim/gpu-cache.cc:      // assert( r.m_block_addr == m_config.block_addr(e.m_request->get_addr())
src/gpgpu-sim/gpu-cache.cc:        m_cache[r.m_index].m_block_addr = r.m_block_addr;
src/gpgpu-sim/gpu-cache.cc:      assert(m_cache[e.m_cache_index].m_block_addr ==
src/gpgpu-sim/gpu-cache.cc:             m_config.block_addr(e.m_request->get_addr()));
src/gpgpu-sim/gpu-cache.cc:/// Place returning cache block into reorder buffer
src/gpgpu-sim/gpu-cache.cc:  assert(r.m_block_addr == m_config.block_addr(mf->get_addr()));
src/gpgpu-sim/gpu-sim.cc:      opp, "-gpgpu_registers_per_block", OPT_UINT32, &gpgpu_registers_per_block,
src/gpgpu-sim/gpu-sim.cc:      opp, "-gpgpu_shmem_per_block", OPT_UINT32, &gpgpu_shmem_per_block,
src/gpgpu-sim/gpu-sim.cc:      "Size of shared memory per thread block or CTA (default 48kB)", "49152");
src/gpgpu-sim/gpu-sim.cc:                         "thread block launch latency in cycles. Default: 0",
src/gpgpu-sim/gpu-sim.cc:        "Execution error: Shader kernel CTA (block) size is too large for "
src/gpgpu-sim/gpu-sim.cc:        "                 modify the CUDA source to decrease the kernel block "
src/gpgpu-sim/gpu-sim.cc:int gpgpu_sim::shared_mem_per_block() const {
src/gpgpu-sim/gpu-sim.cc:  return m_shader_config->gpgpu_shmem_per_block;
src/gpgpu-sim/gpu-sim.cc:int gpgpu_sim::num_registers_per_block() const {
src/gpgpu-sim/gpu-sim.cc:  return m_shader_config->gpgpu_registers_per_block;
src/gpgpu-sim/gpu-sim.cc:bool shader_core_ctx::can_issue_1block(kernel_info_t &kernel) {
src/gpgpu-sim/gpu-sim.cc:    return occupy_shader_resource_1block(kernel, false);
src/gpgpu-sim/gpu-sim.cc:bool shader_core_ctx::occupy_shader_resource_1block(kernel_info_t &k,
src/gpgpu-sim/gpu-sim.cc:void shader_core_ctx::release_shader_resource_1block(unsigned hw_ctaid,
src/gpgpu-sim/gpu-sim.cc:void shader_core_ctx::issue_block2core(kernel_info_t &kernel) {
src/gpgpu-sim/gpu-sim.cc:    assert(occupy_shader_resource_1block(kernel, true));
src/gpgpu-sim/gpu-sim.cc:  unsigned nthreads_in_block = 0;
src/gpgpu-sim/gpu-sim.cc:    nthreads_in_block += sim_init_thread(
src/gpgpu-sim/gpu-sim.cc:  assert(nthreads_in_block > 0 &&
src/gpgpu-sim/gpu-sim.cc:         nthreads_in_block <=
src/gpgpu-sim/gpu-sim.cc:  m_cta_status[free_cta_hw_id] = nthreads_in_block;
src/gpgpu-sim/gpu-sim.cc:void gpgpu_sim::issue_block2core() {
src/gpgpu-sim/gpu-sim.cc:    unsigned num = m_cluster[idx]->issue_block2core();
src/gpgpu-sim/gpu-sim.cc:    issue_block2core();
src/gpgpu-sim/stat-tool.cc:// gpgpu_spread_blocks_across_cores)
src/gpgpu-sim/shader.cc:    block_id
src/gpgpu-sim/shader.cc:          unsigned offset_in_block =
src/gpgpu-sim/shader.cc:          if ((offset_in_block + nbytes) > m_config->m_L1I_config.get_line_sz())
src/gpgpu-sim/shader.cc:            nbytes = (m_config->m_L1I_config.get_line_sz() - offset_in_block);
src/gpgpu-sim/shader.cc:    stallData[m_shader->get_sid()][warp_id][block_id] = warp(warp_id).get_cta_id();
src/gpgpu-sim/shader.cc:    release_shader_resource_1block(cta_num, *kernel);
src/gpgpu-sim/shader.cc:  if (k.num_blocks() < result * num_shader()) {
src/gpgpu-sim/shader.cc:    result = k.num_blocks() / num_shader();
src/gpgpu-sim/shader.cc:    if (k.num_blocks() % num_shader()) result++;
src/gpgpu-sim/shader.cc:unsigned simt_core_cluster::issue_block2core() {
src/gpgpu-sim/shader.cc:  unsigned num_blocks_issued = 0;
src/gpgpu-sim/shader.cc:        m_core[core]->can_issue_1block(*kernel)) {
src/gpgpu-sim/shader.cc:      m_core[core]->issue_block2core(*kernel);
src/gpgpu-sim/shader.cc:      num_blocks_issued++;
src/gpgpu-sim/shader.cc:  return num_blocks_issued;
CHANGES:      both the cache set index generation and for storing tag/block address. 
CHANGES:      the block size or some other constants) then recompiled itself, the 
CHANGES:- Fixed bug comparing strings in basic block formation code
CHANGES:- Fixed bug with additional basic block (which lead to incorrect postdominator 
run-clang-format.py:    # hopefully the stderr pipe won't get full and block the process
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_histogram.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_discontinuity.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_exchange.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_radix_rank.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_radix_sort.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_reduce.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_scan.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh:#include "block/block_store.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/cub.cuh://#include "block/block_shift.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_arch.cuh:/// Define both nominal threads-per-block and items-per-thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh: * cub::AgentSegmentFixup implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:#include "../block/block_store.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:#include "../block/block_scan.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:#include "../block/block_discontinuity.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:    int                         _BLOCK_THREADS,                 ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:        BLOCK_THREADS           = _BLOCK_THREADS,               ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh: * \brief AgentSegmentFixup implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:    // Callback type for obtaining tile prefix during block scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:    // Shared memory type for this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:        // Blocks are launched in increasing order, so just assign one tile per block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_segment_fixup.cuh:        int     tile_idx        = (blockIdx.x * gridDim.y) + blockIdx.y;    // Current tile index
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh: * cub::AgentReduce implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduction .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:#include "../block/block_reduce.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:    int                     _BLOCK_THREADS,         ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:    BlockReduceAlgorithm    _BLOCK_ALGORITHM,       ///< Cooperative block-wide reduction algorithm to use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        BLOCK_THREADS       = _BLOCK_THREADS,       ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:    static const BlockReduceAlgorithm  BLOCK_ALGORITHM      = _BLOCK_ALGORITHM;     ///< Cooperative block-wide reduction algorithm to use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh: * \brief AgentReduce implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduction .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:    /// Shared memory type required by this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        OffsetT                 block_offset,       ///< The offset the tile to consume
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        LoadDirectStriped<BLOCK_THREADS>(threadIdx.x, d_wrapped_in + block_offset, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        OffsetT                 block_offset,       ///< The offset the tile to consume
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        InputT *d_in_unqualified = const_cast<InputT*>(d_in) + block_offset + (threadIdx.x * VECTOR_LOAD_LENGTH);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        OffsetT                 block_offset,       ///< The offset the tile to consume
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:            thread_aggregate = d_wrapped_in[block_offset + thread_offset];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        // Continue reading items (block-striped)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:            OutputT item        = d_wrapped_in[block_offset + thread_offset];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        if (even_share.block_offset + TILE_ITEMS > even_share.block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:            int valid_items = even_share.block_end - even_share.block_offset;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:            ConsumeTile<true>(thread_aggregate, even_share.block_offset, valid_items, Int2Type<false>(), can_vectorize);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        // At least one full block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        ConsumeTile<true>(thread_aggregate, even_share.block_offset, TILE_ITEMS, Int2Type<true>(), can_vectorize);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        even_share.block_offset += even_share.block_stride;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        while (even_share.block_offset + TILE_ITEMS <= even_share.block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:            ConsumeTile<false>(thread_aggregate, even_share.block_offset, TILE_ITEMS, Int2Type<true>(), can_vectorize);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:            even_share.block_offset += even_share.block_stride;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        if (even_share.block_offset < even_share.block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:            int valid_items = even_share.block_end - even_share.block_offset;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:            ConsumeTile<false>(thread_aggregate, even_share.block_offset, valid_items, Int2Type<false>(), can_vectorize);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        // Compute block-wide reduction (all threads have valid items)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        OffsetT block_offset,                       ///< [in] Threadblock begin offset (inclusive)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        OffsetT block_end)                          ///< [in] Threadblock end offset (exclusive)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        even_share.template BlockInit<TILE_ITEMS>(block_offset, block_end);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        return (IsAligned(d_in + block_offset, Int2Type<ATTEMPT_VECTORIZATION>())) ?
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce.cuh:        // Initialize GRID_MAPPING_STRIP_MINE even-share descriptor for this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh: * cub::AgentSpmv implements a stateful abstraction of CUDA thread blocks for participating in device-wide SpMV.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:#include "../block/block_reduce.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:#include "../block/block_scan.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:#include "../block/block_exchange.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:    int                             _BLOCK_THREADS,                         ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:        BLOCK_THREADS                                                   = _BLOCK_THREADS,                       ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh: * \brief AgentSpmv implements a stateful abstraction of CUDA thread blocks for participating in device-wide SpMV.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:    /// Shared memory type required by this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:            // Smem needed for block exchange
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:            // Smem needed for block-wide reduction
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:        KeyValuePairT*  d_tile_carry_pairs,     ///< [out] Pointer to the temporary array carry-out dot product row-ids, one per block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_spmv_orig.cuh:        int tile_idx = (blockIdx.x * gridDim.y) + blockIdx.y;    // Current tile index
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh: * region independent of other thread blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:    T           running_total;      ///< Running block-wide prefix
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:     * Prefix callback operator.  Returns the block-wide running_total in thread-0.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        const T &block_aggregate)              ///< The aggregate sum of the BlockScan inputs
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        running_total = op(running_total, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh: * Generic tile status interface types for block-cooperative scans
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        int tile_idx = (blockIdx.x * blockDim.x) + threadIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        if ((blockIdx.x == 0) && (threadIdx.x < TILE_STATUS_PADDING))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:            __threadfence_block(); // prevent hoisting loads from loop
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        int tile_idx = (blockIdx.x * blockDim.x) + threadIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        if ((blockIdx.x == 0) && (threadIdx.x < TILE_STATUS_PADDING))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh: * ReduceByKey tile status interface types for block-cooperative scans
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        int             tile_idx    = (blockIdx.x * blockDim.x) + threadIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        if ((blockIdx.x == 0) && (threadIdx.x < TILE_STATUS_PADDING))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh://            __threadfence_block(); // prevent hoisting loads from loop
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:            __threadfence_block(); // prevent hoisting loads from loop
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh: * Prefix call-back operator for coupling local block scan within a
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh: * block-cooperative scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh: * Stateful block-scan prefix functor.  Provides the the running prefix for
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        T                                   block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:    T operator()(T block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:            temp_storage.block_aggregate = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:            tile_status.SetPartial(tile_idx, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:            inclusive_prefix = scan_op(exclusive_prefix, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:    // Get the block aggregate stored in temporary storage
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/single_pass_scan_operators.cuh:        return temp_storage.block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh: * AgentRadixSortUpsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort upsweep .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:    int                 _BLOCK_THREADS,     ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        BLOCK_THREADS       = _BLOCK_THREADS,       ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh: * \brief AgentRadixSortUpsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort upsweep .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        OffsetT         block_counters[WARP_THREADS][RADIX_DIGITS];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:    __device__ __forceinline__ void ProcessFullTile(OffsetT block_offset)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        LoadDirectStriped<BLOCK_THREADS>(threadIdx.x, d_keys_in + block_offset, keys);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        OffsetT block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        const OffsetT &block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        block_offset += threadIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        while (block_offset < block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:            UnsignedBits key = d_keys_in[block_offset];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:            block_offset += BLOCK_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        OffsetT          block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        const OffsetT    &block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        while (block_offset + UNROLLED_ELEMENTS <= block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:                ProcessFullTile(block_offset);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:                block_offset += TILE_ITEMS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        while (block_offset + TILE_ITEMS <= block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:            ProcessFullTile(block_offset);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:            block_offset += TILE_ITEMS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:            block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:            block_end);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:                    temp_storage.block_counters[warp_tid][bin_idx] =
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:        // Whole blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:                bin_count += temp_storage.block_counters[i][bin_idx];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:                bin_count += temp_storage.block_counters[i][bin_idx];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:                    temp_storage.block_counters[warp_tid][bin_idx] =
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_upsweep.cuh:                    bin_count[track] += temp_storage.block_counters[i][bin_idx];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh: * cub::AgentRle implements a stateful abstraction of CUDA thread blocks for participating in device-wide run-length-encode.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:#include "../block/block_store.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:#include "../block/block_scan.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:#include "../block/block_exchange.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:#include "../block/block_discontinuity.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:    int                         _BLOCK_THREADS,                 ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:    bool                        _STORE_WARP_TIME_SLICING,       ///< Whether or not only one warp's worth of shared memory should be allocated and time-sliced among block-warps during any store-related data transpositions (versus each warp having its own storage)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:        BLOCK_THREADS           = _BLOCK_THREADS,               ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:        STORE_WARP_TIME_SLICING = _STORE_WARP_TIME_SLICING,     ///< Whether or not only one warp's worth of shared memory should be allocated and time-sliced among block-warps during any store-related data transpositions (versus each warp having its own storage)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh: * \brief AgentRle implements a stateful abstraction of CUDA thread blocks for participating in device-wide run-length-encode 
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:        /// Whether or not only one warp's worth of shared memory should be allocated and time-sliced among block-warps during any store-related data transpositions (versus each warp having its own storage)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:    // Callback type for obtaining tile prefix during block scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:    // Shared memory type for this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:        // Blocks are launched in increasing order, so just assign one tile per block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_rle.cuh:        int     tile_idx        = (blockIdx.x * gridDim.y) + blockIdx.y;    // Current tile index
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh: * cub::AgentHistogram implements a stateful abstraction of CUDA thread blocks for participating in device-wide histogram .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:    int                             _BLOCK_THREADS,                 ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        BLOCK_THREADS           = _BLOCK_THREADS,                   ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh: * \brief AgentHistogram implements a stateful abstraction of CUDA thread blocks for participating in device-wide histogram .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:    /// Shared memory type required by this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        CounterT histograms[NUM_ACTIVE_CHANNELS][PRIVATIZED_SMEM_BINS + 1];     // Smem needed for block-privatized smem histogram (with 1 word of padding)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        OffsetT                         block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        WrappedPixelIteratorT d_wrapped_pixels((PixelT*) (d_native_samples + block_offset));
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        OffsetT                         block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        WrappedQuadIteratorT d_wrapped_quads((QuadT*) (d_native_samples + block_offset));
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        OffsetT         block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        LoadFullAlignedTile(block_offset, valid_samples, samples, Int2Type<NUM_ACTIVE_CHANNELS>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        OffsetT         block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:            d_wrapped_samples + block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        OffsetT         block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        WrappedPixelIteratorT d_wrapped_pixels((PixelT*) (d_native_samples + block_offset));
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        OffsetT         block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:            d_wrapped_samples + block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:    __device__ __forceinline__ void ConsumeTile(OffsetT block_offset, int valid_samples)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:            block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        int         tile_idx                    = (blockIdx.y  * gridDim.x) + blockIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:    // Consume row tiles.  Specialized for even-share (striped across thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        for (int row = blockIdx.y; row < num_rows; row += gridDim.y)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:            OffsetT tile_offset = row_begin + (blockIdx.x * TILE_SAMPLES);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:                blockIdx.x & 1)                 // prefer blended privatized histograms
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        int blockId = (blockIdx.y * gridDim.x) + blockIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        // Initialize the locations of this block's privatized histograms
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:            this->d_privatized_histograms[CHANNEL] = d_privatized_histograms[CHANNEL] + (blockId * num_privatized_bins[CHANNEL]);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_histogram.cuh:        GridQueue<int>      tile_queue)                 ///< Queue descriptor for assigning tiles of work to thread blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh: * cub::AgentSelectIf implements a stateful abstraction of CUDA thread blocks for participating in device-wide select.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:#include "../block/block_store.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:#include "../block/block_scan.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:#include "../block/block_exchange.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:#include "../block/block_discontinuity.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:    int                         _BLOCK_THREADS,                 ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:        BLOCK_THREADS           = _BLOCK_THREADS,               ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh: * \brief AgentSelectIf implements a stateful abstraction of CUDA thread blocks for participating in device-wide selection
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:    // Callback type for obtaining tile prefix during block scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:    // Shared memory type for this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:        // Blocks are launched in increasing order, so just assign one tile per block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_select_if.cuh:        int     tile_idx        = (blockIdx.x * gridDim.y) + blockIdx.y;    // Current tile index
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh: * AgentRadixSortDownsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort downsweep .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:#include "../block/block_store.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:#include "../block/block_radix_rank.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:#include "../block/block_exchange.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:    int                         _BLOCK_THREADS,         ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:    BlockScanAlgorithm          _SCAN_ALGORITHM,        ///< The block scan algorithm to use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        BLOCK_THREADS           = _BLOCK_THREADS,           ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh: * \brief AgentRadixSortDownsweep implements a stateful abstraction of CUDA thread blocks for participating in device-wide radix sort downsweep .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT                     block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            d_keys_in + block_offset, keys);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT                     block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            d_keys_in + block_offset, keys, valid_items, oob_item);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT                     block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        LoadDirectWarpStriped(threadIdx.x, d_keys_in + block_offset, keys);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT                     block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        LoadDirectWarpStriped(threadIdx.x, d_keys_in + block_offset, keys, valid_items, oob_item);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT                     block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            d_values_in + block_offset, values);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT                     block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            d_values_in + block_offset, values, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT                     block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        LoadDirectWarpStriped(threadIdx.x, d_values_in + block_offset, values);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT                     block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        LoadDirectWarpStriped(threadIdx.x, d_values_in + block_offset, values, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT         block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT         /*block_offset*/,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        GatherScatterValues<FULL_TILE>(relative_bin_offsets , ranks, block_offset, valid_items, Int2Type<KEYS_ONLY>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT         block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT         block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        while (block_offset + TILE_ITEMS <= block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            LoadDirectStriped<BLOCK_THREADS>(threadIdx.x, d_in + block_offset, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            StoreDirectStriped<BLOCK_THREADS>(threadIdx.x, d_out + block_offset, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            block_offset += TILE_ITEMS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        if (block_offset < block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            OffsetT valid_items = block_end - block_offset;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            LoadDirectStriped<BLOCK_THREADS>(threadIdx.x, d_in + block_offset, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            StoreDirectStriped<BLOCK_THREADS>(threadIdx.x, d_out + block_offset, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT         /*block_offset*/,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT         /*block_end*/)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:                // Short circuit if the first block's histogram has only bin counts of only zeros or problem-size
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:                OffsetT first_block_bin_offset = d_spine[gridDim.x * bin_idx];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:                short_circuit = short_circuit && ((first_block_bin_offset == 0) || (first_block_bin_offset == num_items));
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:                // Load my block's bin offset for my bin
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:                bin_offset[track] = d_spine[(gridDim.x * bin_idx) + blockIdx.x];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT   block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:        OffsetT   block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            Copy(d_keys_in, d_keys_out, block_offset, block_end);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            Copy(d_values_in, d_values_out, block_offset, block_end);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            while (block_offset + TILE_ITEMS <= block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:                ProcessTile<true>(block_offset);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:                block_offset += TILE_ITEMS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:            if (block_offset < block_end)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_radix_sort_downsweep.cuh:                ProcessTile<false>(block_offset, block_end - block_offset);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh: * cub::AgentScan implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:#include "../block/block_store.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:#include "../block/block_scan.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:    int                         _BLOCK_THREADS,                 ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        BLOCK_THREADS           = _BLOCK_THREADS,               ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh: * \brief AgentScan implements a stateful abstraction of CUDA thread blocks for participating in device-wide prefix scan .
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:    // Callback type for obtaining tile prefix during block scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:    // Shared memory type for this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        OutputT             &block_aggregate,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        BlockScanT(temp_storage.scan).ExclusiveScan(items, items, init_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        block_aggregate = scan_op(init_value, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        OutputT             &block_aggregate,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        BlockScanT(temp_storage.scan).InclusiveScan(items, items, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:            OutputT block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:            ScanTile(items, init_value, scan_op, block_aggregate, Int2Type<IS_INCLUSIVE>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:                tile_state.SetInclusive(0, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        // Blocks are launched in increasing order, so just assign one tile per block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        int     tile_idx        = start_tile + blockIdx.x;          // Current tile index
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:    // Scan an sequence of consecutive tiles (independent of other thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:            OutputT block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:            ScanTile(items, init_value, scan_op, block_aggregate, Int2Type<IS_INCLUSIVE>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:            prefix_op.running_total = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        OffsetT  range_offset,      ///< [in] Threadblock begin offset (inclusive)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        OffsetT  range_end)         ///< [in] Threadblock end offset (exclusive)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        OffsetT range_offset,                       ///< [in] Threadblock begin offset (inclusive)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_scan.cuh:        OffsetT range_end,                          ///< [in] Threadblock end offset (exclusive)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh: * cub::AgentReduceByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:#include "../block/block_load.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:#include "../block/block_store.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:#include "../block/block_scan.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:#include "../block/block_discontinuity.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:    int                         _BLOCK_THREADS,                 ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:        BLOCK_THREADS           = _BLOCK_THREADS,               ///< Threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh: * Thread block abstractions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh: * \brief AgentReduceByKey implements a stateful abstraction of CUDA thread blocks for participating in device-wide reduce-value-by-key
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:    // Callback type for obtaining tile prefix during block scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:    // Shared memory type for this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:        OffsetValuePairT    block_aggregate;        // Inclusive block-wide scan aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:        OffsetValuePairT    total_aggregate;        // The tile prefix folded with block_aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:            BlockScanT(temp_storage.scan).ExclusiveScan(scan_items, scan_items, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:            total_aggregate         = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:                tile_state.SetInclusive(0, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:            block_aggregate         = prefix_op.GetBlockAggregate();
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:        OffsetT num_tile_segments = block_aggregate.key;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:        // Blocks are launched in increasing order, so just assign one tile per block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/agent/agent_reduce_by_key.cuh:        int     tile_idx        = start_tile + blockIdx.x;          // Current tile index
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh: * cub::GridEvenShare is a descriptor utility for distributing input among CUDA thread blocks in an "even-share" fashion.  Each thread block gets roughly the same number of fixed-size work units (grains).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh: * CUDA thread blocks in an "even-share" fashion.  Each thread block gets roughly
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh: * Each thread block is assigned a consecutive sequence of input tiles.  To help
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh: * last thread block, to GridEvenShare assigns one of three different amounts of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh: * work to a given thread block: "big", "normal", or "last".  The "big" workloads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh: * last thread block may be partially-full if the input is not an even multiple of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh: * instance of GridEvenShare.  The instance can be passed to child thread blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh: * which can initialize their per-thread block offsets using \p BlockInit().
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:    /// Grid size in thread blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:    /// OffsetT into input marking the beginning of the owning thread block's segment of input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:    OffsetT     block_offset;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:    /// OffsetT into input of marking the end (one-past) of the owning thread block's segment of input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:    OffsetT     block_end;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:    OffsetT     block_stride;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        block_offset(0),
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        block_end(0),
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        block_stride(0)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        this->block_offset          = num_items;    // Initialize past-the-end
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        this->block_end             = num_items;    // Initialize past-the-end
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        OffsetT avg_tiles_per_block = total_tiles / grid_size;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        this->big_shares            = total_tiles - (avg_tiles_per_block * grid_size);        // leftover grains go to big blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        this->normal_share_items    = avg_tiles_per_block * tile_items;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:     * \brief Initializes ranges for the specified thread block index.  Specialized
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:     * for a "raking" access pattern in which each thread block is assigned a
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        int block_id,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        block_stride = TILE_ITEMS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        if (block_id < big_shares)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:            // This thread block gets a big share of grains (avg_tiles_per_block + 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:            block_offset = (block_id * big_share_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:            block_end = block_offset + big_share_items;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        else if (block_id < total_tiles)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:            // This thread block gets a normal share of grains (avg_tiles_per_block)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:            block_offset = normal_base_offset + (block_id * normal_share_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:            block_end = CUB_MIN(num_items, block_offset + normal_share_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:     * pattern in which each thread block is assigned a consecutive sequence
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        int block_id,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        block_stride = grid_size * TILE_ITEMS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        block_offset = (block_id * TILE_ITEMS);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        block_end = num_items;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:     * pattern in which the input tiles assigned to each thread block are
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        BlockInit<TILE_ITEMS>(blockIdx.x, Int2Type<STRATEGY>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:     * pattern in which each thread block is assigned a consecutive sequence
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        OffsetT block_offset,                       ///< [in] Threadblock begin offset (inclusive)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        OffsetT block_end)                          ///< [in] Threadblock end offset (exclusive)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        this->block_offset = block_offset;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        this->block_end = block_end;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_even_share.cuh:        this->block_stride = TILE_ITEMS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh: * cub::GridMappingStrategy enumerates alternative strategies for mapping constant-sized tiles of device-wide data onto a grid of CUDA thread blocks.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh: * \brief cub::GridMappingStrategy enumerates alternative strategies for mapping constant-sized tiles of device-wide data onto a grid of CUDA thread blocks.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * \brief An a "raking" access pattern in which each thread block is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * constant and corresponds loosely to the number of thread blocks that may
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * to be processed to completion before the thread block terminates or
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * obtains more work.  The kernel invokes \p p thread blocks, each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * to each thread block are separated by a stride equal to the the extent of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * constant and corresponds loosely to the number of thread blocks that may
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * thread block terminates or obtains more work.  The kernel invokes \p p
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * thread blocks, each of which iteratively consumes a segment of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * \brief A dynamic "queue-based" strategy for assigning input tiles to thread blocks.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * thread blocks.  Work is atomically dequeued in tiles, where a tile is a
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * unit of input to be processed to completion before the thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_mapping.cuh:     * loosely corresponding to the number of thread blocks that may actively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh: * cub::GridBarrier implements a software global barrier among thread blocks within a CUDA grid
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh: * \brief GridBarrier implements a software global barrier among thread blocks within a CUDA grid
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:        if (blockIdx.x == 0)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:                d_vol_sync[blockIdx.x] = 1;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:            for (int peer_block = threadIdx.x; peer_block < gridDim.x; peer_block += blockDim.x)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:                while (ThreadLoad<LOAD_CG>(d_sync + peer_block) == 0)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:                    __threadfence_block();
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:            for (int peer_block = threadIdx.x; peer_block < gridDim.x; peer_block += blockDim.x)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:                d_vol_sync[peer_block] = 0;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:                d_vol_sync[blockIdx.x] = 1;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:                while (ThreadLoad<LOAD_CG>(d_sync + blockIdx.x) == 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_barrier.cuh:                    __threadfence_block();
debug_tools/WatchYourStep/ptxjitplus/inc/cub/grid/grid_queue.cuh: * Reset grid queue (call with 1 block of 1 thread)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:#include "../../block/block_radix_sort.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh: * Upsweep digit-counting kernel entry point (multi-block).  Computes privatized digit histograms, one per block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    OffsetT                 *d_spine,                       ///< [out] Privatized (per block) digit histograms (striped, i.e., 0s counts from each block, then 1s counts from each block, etc.)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    GridEvenShare<OffsetT>  even_share)                     ///< [in] Even-share descriptor for mapan equal number of tiles onto each thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    // Initialize GRID_MAPPING_RAKE even-share descriptor for this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    upsweep.ProcessRegion(even_share.block_offset, even_share.block_end);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    upsweep.template ExtractCounts<IS_DESCENDING>(d_spine, gridDim.x, blockIdx.x);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh: * Spine scan kernel entry point (single-block).  Computes an exclusive prefix sum over the privatized digit histograms
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    OffsetT                 *d_spine,                       ///< [in,out] Privatized (per block) digit histograms (striped, i.e., 0s counts from each block, then 1s counts from each block, etc.)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    AgentScanT block_scan(temp_storage, d_spine, d_spine, cub::Sum(), OffsetT(0)) ;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    int block_offset = 0;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    while (block_offset + AgentScanT::TILE_ITEMS <= num_counts)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:        block_scan.template ConsumeTile<false, false>(block_offset, prefix_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:        block_offset += AgentScanT::TILE_ITEMS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh: * Downsweep pass kernel entry point (multi-block).  Scatters keys (and values) into corresponding bins for the current digit place.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    OffsetT                 *d_spine,                       ///< [in] Scan of privatized (per block) digit histograms (striped, i.e., 0s counts from each block, then 1s counts from each block, etc.)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    GridEvenShare<OffsetT>  even_share)                     ///< [in] Even-share descriptor for mapan equal number of tiles onto each thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    // Initialize even-share descriptor for this thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:        even_share.block_offset,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:        even_share.block_end);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh: * Single pass kernel entry point (single-block).  Fully sorts a tile of input.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    // Keys and values for the block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh: * Segmented radix sorting pass (one block per segment)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    OffsetT segment_begin   = d_begin_offsets[blockIdx.x];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    OffsetT segment_end     = d_end_offsets[blockIdx.x];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:    /// Invoke a single block to sort in-core
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:                pass_config.even_share.grid_size, pass_config.upsweep_config.block_threads, (long long) stream,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:            pass_config.upsweep_kernel<<<pass_config.even_share.grid_size, pass_config.upsweep_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:                1, pass_config.scan_config.block_threads, (long long) stream, pass_config.scan_config.items_per_thread);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:            pass_config.scan_kernel<<<1, pass_config.scan_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:                pass_config.even_share.grid_size, pass_config.downsweep_config.block_threads, (long long) stream,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:            pass_config.downsweep_kernel<<<pass_config.even_share.grid_size, pass_config.downsweep_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:                spine_length * sizeof(OffsetT),                                         // bytes needed for privatized block digit histograms
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:                    num_segments, pass_config.segmented_config.block_threads, (long long) stream,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_radix_sort.cuh:            pass_config.segmented_kernel<<<num_segments, pass_config.segmented_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh: * Select kernel entry point (multi-block)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh:    // Thread block type for selecting data from input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh:        int                     block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh:            block_threads               = AgentRlePolicyT::BLOCK_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh:                block_threads,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh:            int tile_size = device_rle_config.block_threads * device_rle_config.items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh:                device_rle_config.block_threads))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh:                scan_grid_size.x, scan_grid_size.y, scan_grid_size.z, device_rle_config.block_threads, (long long) stream, device_rle_config.items_per_thread, device_rle_kernel_sm_occupancy);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_rle.cuh:            device_rle_sweep_kernel<<<scan_grid_size, device_rle_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh: * Multi-block reduce-by-key sweep kernel entry point
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh:    // Thread block type for reducing tiles of value segments
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh:        int block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh:            block_threads       = PolicyT::BLOCK_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh:            tile_items          = block_threads * items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh:            int tile_size = reduce_by_key_config.block_threads * reduce_by_key_config.items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh:                reduce_by_key_config.block_threads))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh:                    start_tile, scan_grid_size, reduce_by_key_config.block_threads, (long long) stream, reduce_by_key_config.items_per_thread, reduce_by_key_sm_occupancy);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce_by_key.cuh:                reduce_by_key_kernel<<<scan_grid_size, reduce_by_key_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh: * Initialization kernel for tile status initialization (multi-block)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh: * Initialization kernel for tile status initialization (multi-block)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:    if ((blockIdx.x == 0) && (threadIdx.x == 0))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh: * Scan kernel entry point (multi-block)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:    // Thread block type for scanning input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:            CUB_SCALED_GRANULARITIES(128, 15, OutputT),      ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                CUB_SCALED_GRANULARITIES(128, 12, OutputT),      ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                CUB_SCALED_GRANULARITIES(128, 12, OutputT),      ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                CUB_SCALED_GRANULARITIES(256, 9, OutputT),      ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                CUB_SCALED_GRANULARITIES(128, 12, OutputT),      ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                CUB_SCALED_GRANULARITIES(96, 21, OutputT),      ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                CUB_SCALED_GRANULARITIES(64, 9, OutputT),      ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:        int block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:            block_threads       = PolicyT::BLOCK_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:            tile_items          = block_threads * items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:            int tile_size = scan_kernel_config.block_threads * scan_kernel_config.items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                scan_kernel_config.block_threads))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                    start_tile, scan_grid_size, scan_kernel_config.block_threads, (long long) stream, scan_kernel_config.items_per_thread, scan_sm_occupancy);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_scan.cuh:                scan_kernel<<<scan_grid_size, scan_kernel_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh: * Reduce region kernel entry point (multi-block).  Computes privatized reductions, one per thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    GridEvenShare<OffsetT>  even_share,                 ///< [in] Even-share descriptor for mapping an equal number of tiles onto each thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    // Thread block type for reducing input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    OutputT block_aggregate = AgentReduceT(temp_storage, d_in, reduction_op).ConsumeTiles(even_share);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:        d_out[blockIdx.x] = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh: * Reduce a single tile kernel entry point (single-block).  Can be used to aggregate privatized thread block reductions from a previous multi-block reduction pass.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    // Thread block type for reducing input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    OuputT block_aggregate = AgentReduceT(temp_storage, d_in, reduction_op).ConsumeRange(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:        *d_out = reduction_op(init, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh: * Segmented reduction (one block per segment)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    // Thread block type for reducing input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    OffsetT segment_begin   = d_begin_offsets[blockIdx.x];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    OffsetT segment_end     = d_end_offsets[blockIdx.x];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:            d_out[blockIdx.x] = init;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    OutputT block_aggregate = AgentReduceT(temp_storage, d_in, reduction_op).ConsumeRange(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    NormalizeReductionOutput(block_aggregate, segment_begin, d_in);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:        d_out[blockIdx.x] = reduction_op(init, block_aggregate);;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                CUB_SCALED_GRANULARITIES(128, 8, OuputT), ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                BLOCK_REDUCE_RAKING,                ///< Cooperative block-wide reduction algorithm to use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                CUB_SCALED_GRANULARITIES(128, 8, OuputT),     ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                BLOCK_REDUCE_RAKING,                    ///< Cooperative block-wide reduction algorithm to use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                CUB_SCALED_GRANULARITIES(256, 20, OuputT),    ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                BLOCK_REDUCE_WARP_REDUCTIONS,           ///< Cooperative block-wide reduction algorithm to use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                CUB_SCALED_GRANULARITIES(256, 20, OuputT),    ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                BLOCK_REDUCE_WARP_REDUCTIONS,           ///< Cooperative block-wide reduction algorithm to use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                CUB_SCALED_GRANULARITIES(256, 16, OuputT),    ///< Threads per block, items per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                BLOCK_REDUCE_WARP_REDUCTIONS,           ///< Cooperative block-wide reduction algorithm to use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:    /// Invoke a single block block to reduce in-core
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:            int max_blocks = reduce_device_occupancy * CUB_SUBSCRIPTION_FACTOR(ptx_version);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:            even_share.DispatchInit(num_items, max_blocks, reduce_config.tile_size);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                max_blocks * sizeof(OutputT)    // bytes needed for privatized block reductions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:            // Alias the allocation for the privatized per-block reductions
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:            OutputT *d_block_reductions = (OutputT*) allocations[0];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                d_block_reductions,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_reduce.cuh:                d_block_reductions,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:    GridQueue<int>                                  tile_queue)                     ///< Drain queue descriptor for dynamically mapping tile data onto thread blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:    if ((threadIdx.x == 0) && (blockIdx.x == 0))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:    int output_bin = (blockIdx.x * blockDim.x) + threadIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh: * Histogram privatized sweep kernel entry point (multi-block).  Computes privatized histograms, one per thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:    GridQueue<int>                                          tile_queue)                         ///< Drain queue descriptor for dynamically mapping tile data onto thread blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:    // Thread block type for compositing input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:        int                             block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            block_threads               = BlockPolicy::BLOCK_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:                histogram_sweep_config.block_threads))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            // Get grid dimensions, trying to keep total blocks ~histogram_sweep_occupancy
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            int pixels_per_tile     = histogram_sweep_config.block_threads * histogram_sweep_config.pixels_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            int blocks_per_row      = CUB_MIN(histogram_sweep_occupancy, tiles_per_row);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            int blocks_per_col      = (blocks_per_row > 0) ?
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:                                        int(CUB_MIN(histogram_sweep_occupancy / blocks_per_row, num_rows)) :
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            int num_thread_blocks   = blocks_per_row * blocks_per_col;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            sweep_grid_dims.x = (unsigned int) blocks_per_row;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            sweep_grid_dims.y = (unsigned int) blocks_per_col;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:                allocation_sizes[CHANNEL] = size_t(num_thread_blocks) * (num_privatized_levels[CHANNEL] - 1) * sizeof(CounterT);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            // Setup array wrapper for privatized per-block histogram channel output (because we can't pass static arrays as kernel parameters)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            int histogram_init_block_threads    = 256;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            int histogram_init_grid_dims        = (max_num_output_bins + histogram_init_block_threads - 1) / histogram_init_block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:                histogram_init_grid_dims, histogram_init_block_threads, (long long) stream);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            histogram_init_kernel<<<histogram_init_grid_dims, histogram_init_block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            if ((blocks_per_row == 0) || (blocks_per_col == 0))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:                histogram_sweep_config.block_threads, (long long) stream, histogram_sweep_config.pixels_per_thread, histogram_sweep_sm_occupancy);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_histogram.cuh:            histogram_sweep_kernel<<<sweep_grid_dims, histogram_sweep_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh: * Select kernel entry point (multi-block)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh:    // Thread block type for selecting data from input tiles
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh:        int block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh:            block_threads       = PolicyT::BLOCK_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh:            tile_items          = block_threads * items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh:            int tile_size = select_if_config.block_threads * select_if_config.items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh:                select_if_config.block_threads))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh:                scan_grid_size.x, scan_grid_size.y, scan_grid_size.z, select_if_config.block_threads, (long long) stream, select_if_config.items_per_thread, range_select_sm_occupancy);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_select_if.cuh:            select_if_kernel<<<scan_grid_size, select_if_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:    int row_idx = (blockIdx.x * blockDim.x) + threadIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:    int tile_idx = (blockIdx.x * blockDim.x) + threadIdx.x;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:    KeyValuePair<OffsetT,ValueT>*   d_tile_carry_pairs,         ///< [out] Pointer to the temporary array carry-out dot product row-ids, one per block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh: * Multi-block reduce-by-key sweep kernel entry point
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:    PairsInputIteratorT         d_pairs_in,         ///< [in] Pointer to the array carry-out dot product row-ids, one per spmv block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:    // Thread block type for reducing tiles of value segments
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:        int block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:            block_threads       = PolicyT::BLOCK_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:            tile_items          = block_threads * items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                int degen_col_kernel_block_size     = INIT_KERNEL_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                int degen_col_kernel_grid_size      = (spmv_params.num_rows + degen_col_kernel_block_size - 1) / degen_col_kernel_block_size;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                    degen_col_kernel_grid_size, degen_col_kernel_block_size, (long long) stream);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                spmv_1col_kernel<<<degen_col_kernel_grid_size, degen_col_kernel_block_size, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:            int merge_tile_size              = spmv_config.block_threads * spmv_config.items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:            int segment_fixup_tile_size     = segment_fixup_config.block_threads * segment_fixup_config.items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                spmv_config.block_threads))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                segment_fixup_config.block_threads))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:            allocation_sizes[1] = num_merge_tiles * sizeof(KeyValuePairT);       // bytes needed for block carry-out pairs
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:            int search_block_size   = INIT_KERNEL_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:            int search_grid_size    = (num_merge_tiles + 1 + search_block_size - 1) / search_block_size;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                // Not enough spmv tiles to saturate the device: have spmv blocks search their own staring coords
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                    search_grid_size, search_block_size, (long long) stream);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                spmv_search_kernel<<<search_grid_size, search_block_size, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                spmv_grid_size.x, spmv_grid_size.y, spmv_grid_size.z, spmv_config.block_threads, (long long) stream, spmv_config.items_per_thread, spmv_sm_occupancy);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:            spmv_kernel<<<spmv_grid_size, spmv_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                    segment_fixup_grid_size.x, segment_fixup_grid_size.y, segment_fixup_grid_size.z, segment_fixup_config.block_threads, (long long) stream, segment_fixup_config.items_per_thread, segment_fixup_sm_occupancy);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/device/dispatch/dispatch_spmv_orig.cuh:                segment_fixup_kernel<<<segment_fixup_grid_size, segment_fixup_config.block_threads, 0, stream>>>(
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh: * The code snippet below illustrates four concurrent warp prefix sums within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh: * Suppose the set of input \p thread_data across the block of threads is <tt>{1, 1, 1, 1, ...}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh: * The code snippet below illustrates a single warp prefix sum within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide inclusive prefix sums within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{1, 1, 1, 1, ...}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide inclusive prefix sums within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{1, 1, 1, 1, ...}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide exclusive prefix sums within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{1, 1, 1, 1, ...}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide exclusive prefix sums within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{1, 1, 1, 1, ...}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide inclusive prefix max scans within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, -1, 2, -3, ..., 126, -127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide inclusive prefix max scans within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, -1, 2, -3, ..., 126, -127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide exclusive prefix max scans within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, -1, 2, -3, ..., 126, -127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide exclusive prefix max scans within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, -1, 2, -3, ..., 126, -127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide exclusive prefix max scans within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, -1, 2, -3, ..., 126, -127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide exclusive prefix max scans within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, -1, 2, -3, ..., 126, -127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide exclusive prefix max scans within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, -1, 2, -3, ..., 126, -127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * The code snippet below illustrates four concurrent warp-wide prefix max scans within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, -1, 2, -3, ..., 126, -127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, 1, 2, 3, ..., 127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh: * The code snippet below illustrates four concurrent warp sum reductions within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh: * Suppose the set of input \p thread_data across the block of threads is <tt>{0, 1, 2, 3, ..., 127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh: * The code snippet below illustrates a single warp sum reduction within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * The code snippet below illustrates four concurrent warp sum reductions within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, 1, 2, 3, ..., 127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * block of 32 threads (one warp).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * reduction within a block of 32 threads (one warp).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * Suppose the set of input \p thread_data and \p head_flag across the block of threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * reduction within a block of 32 threads (one warp).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * Suppose the set of input \p thread_data and \p tail_flag across the block of threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * The code snippet below illustrates four concurrent warp max reductions within a block of
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{0, 1, 2, 3, ..., 127}</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * block of 32 threads (one warp).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * reduction within a block of 32 threads (one warp).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * Suppose the set of input \p thread_data and \p head_flag across the block of threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * reduction within a block of 32 threads (one warp).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/warp/warp_reduce.cuh:     * Suppose the set of input \p thread_data and \p tail_flag across the block of threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        cudaEvent_t     ready_event;        // Signal when associated stream has run to the point at which this block was freed
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        // Constructor (suitable for searching maps for a specific block, given its pointer and device)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        // Constructor (suitable for searching maps for a range of suitable blocks, given a device)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:    /// Set type for cached blocks (ordered by size)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:    /// Set type for live blocks (ordered by ptr)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:    CachedBlocks    cached_blocks;      /// Set of cached device allocations available for reuse
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:    BusyBlocks      live_blocks;        /// Set of live device allocations currently in use
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        cached_blocks(BlockDescriptor::SizeCompare),
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        live_blocks(BlockDescriptor::PtrCompare)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        cached_blocks(BlockDescriptor::SizeCompare),
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        live_blocks(BlockDescriptor::PtrCompare)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        // Create a block descriptor for the requested allocation
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            // Iterate through the range of cached blocks on the same device in the same bin
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            CachedBlocks::iterator block_itr = cached_blocks.lower_bound(search_key);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            while ((block_itr != cached_blocks.end())
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    && (block_itr->device == device)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    && (block_itr->bin == search_key.bin))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                // To prevent races with reusing blocks returned by the host but still
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                // in use by the device, only consider cached blocks that are
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                if ((active_stream == block_itr->associated_stream) ||
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    (cudaEventQuery(block_itr->ready_event) != cudaErrorNotReady))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    // Reuse existing cache block.  Insert into live blocks.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    search_key = *block_itr;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    live_blocks.insert(search_key);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    // Remove from free blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    if (debug) _CubLog("\tDevice %d reused cached block at %p (%lld bytes) for stream %lld (previously associated with stream %lld).\n",
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                        device, search_key.d_ptr, (long long) search_key.bytes, (long long) search_key.associated_stream, (long long)  block_itr->associated_stream);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    cached_blocks.erase(block_itr);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                block_itr++;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        // Allocate the block if necessary
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                // The allocation attempt failed: free all cached blocks on device and retry
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                // Iterate the range of free blocks on the same device
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                CachedBlocks::iterator block_itr = cached_blocks.lower_bound(free_key);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                while ((block_itr != cached_blocks.end()) && (block_itr->device == device))
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    // blocking and will synchronize across all kernels executing
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    if (CubDebug(error = cudaFree(block_itr->d_ptr))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    if (CubDebug(error = cudaEventDestroy(block_itr->ready_event))) break;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    cached_bytes[device].free -= block_itr->bytes;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    if (debug) _CubLog("\tDevice %d freed %lld bytes.\n\t\t  %lld available blocks cached (%lld bytes), %lld live blocks (%lld bytes) outstanding.\n",
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                        device, (long long) block_itr->bytes, (long long) cached_blocks.size(), (long long) cached_bytes[device].free, (long long) live_blocks.size(), (long long) cached_bytes[device].live);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    cached_blocks.erase(block_itr);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    block_itr++;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            // Insert into live blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            live_blocks.insert(search_key);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            if (debug) _CubLog("\tDevice %d allocated new device block at %p (%lld bytes associated with stream %lld).\n",
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        if (debug) _CubLog("\t\t%lld available blocks cached (%lld bytes), %lld live blocks outstanding(%lld bytes).\n",
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            (long long) cached_blocks.size(), (long long) cached_bytes[device].free, (long long) live_blocks.size(), (long long) cached_bytes[device].live);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        // Find corresponding block descriptor
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        BusyBlocks::iterator block_itr = live_blocks.find(search_key);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        if (block_itr != live_blocks.end())
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            // Remove from live blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            search_key = *block_itr;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            live_blocks.erase(block_itr);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                // Insert returned allocation into free blocks
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                cached_blocks.insert(search_key);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                if (debug) _CubLog("\tDevice %d returned %lld bytes from associated stream %lld.\n\t\t %lld available blocks cached (%lld bytes), %lld live blocks outstanding. (%lld bytes)\n",
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    device, (long long) search_key.bytes, (long long) search_key.associated_stream, (long long) cached_blocks.size(),
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                    (long long) cached_bytes[device].free, (long long) live_blocks.size(), (long long) cached_bytes[device].live);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            if (debug) _CubLog("\tDevice %d freed %lld bytes from associated stream %lld.\n\t\t  %lld available blocks cached (%lld bytes), %lld live blocks (%lld bytes) outstanding.\n",
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                device, (long long) search_key.bytes, (long long) search_key.associated_stream, (long long) cached_blocks.size(), (long long) cached_bytes[device].free, (long long) live_blocks.size(), (long long) cached_bytes[device].live);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:        while (!cached_blocks.empty())
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            // Get first block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            CachedBlocks::iterator begin = cached_blocks.begin();
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            if (debug) _CubLog("\tDevice %d freed %lld bytes.\n\t\t  %lld available blocks cached (%lld bytes), %lld live blocks (%lld bytes) outstanding.\n",
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:                current_device, (long long) begin->bytes, (long long) cached_blocks.size(), (long long) cached_bytes[current_device].free, (long long) live_blocks.size(), (long long) cached_bytes[current_device].live);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_allocator.cuh:            cached_blocks.erase(begin);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_raking_layout.cuh: * cub::BlockRakingLayout provides a conflict-free shared memory layout abstraction for warp-raking across thread block data.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_raking_layout.cuh: * \brief BlockRakingLayout provides a conflict-free shared memory layout abstraction for 1D raking across thread block data.    ![](raking.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_raking_layout.cuh: * This type facilitates a shared memory usage pattern where a block of CUDA
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_raking_layout.cuh: * \tparam BLOCK_THREADS            The thread block size in threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_raking_layout.cuh:        // Incorporating a block of padding partials every shared memory segment
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh: * The cub::BlockShuffle class provides [<em>collective</em>](index.html#sec0) methods for shuffling data partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh: * \brief The BlockShuffle class provides [<em>collective</em>](index.html#sec0) methods for shuffling data partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh: * \tparam BLOCK_DIM_X          The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh: * \tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh: * \tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh: * It is commonplace for blocks of threads to rearrange data items between
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:     * \brief The thread block rotates its [<em>blocked arrangement</em>](index.html#sec5sec3) of \p input items, shifting it up by one item
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:     * \brief The thread block rotates its [<em>blocked arrangement</em>](index.html#sec5sec3) of \p input items, shifting it up by one item.  All threads receive the \p input provided by <em>thread</em><sub><tt>BLOCK_THREADS-1</tt></sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:        T &block_suffix)                ///< [out] The item \p input[ITEMS_PER_THREAD-1] from <em>thread</em><sub><tt>BLOCK_THREADS-1</tt></sub>, provided to all threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:        block_suffix = temp_storage[BLOCK_THREADS - 1].prev;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:     * \brief The thread block rotates its [<em>blocked arrangement</em>](index.html#sec5sec3) of \p input items, shifting it down by one item
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:     * \brief The thread block rotates its [<em>blocked arrangement</em>](index.html#sec5sec3) of input items, shifting it down by one item.  All threads receive \p input[0] provided by <em>thread</em><sub><tt>0</tt></sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:        T &block_prefix)                ///< [out] The item \p input[0] from <em>thread</em><sub><tt>0</tt></sub>, provided to all threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_shuffle.cuh:        block_prefix = temp_storage[BLOCK_THREADS - 1].prev;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * The cub::BlockExchange class provides [<em>collective</em>](index.html#sec0) methods for rearranging data partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * \brief The BlockExchange class provides [<em>collective</em>](index.html#sec0) methods for rearranging data partitioned across a CUDA thread block. ![](transpose_logo.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * \tparam BLOCK_DIM_X          The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * \tparam WARP_TIME_SLICING    <b>[optional]</b> When \p true, only use enough shared memory for a single warp's worth of tile data, time-slicing the block-wide exchange over multiple synchronized rounds.  Yields a smaller memory footprint at the expense of decreased parallelism.  (Default: false)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * \tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * \tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * - It is commonplace for blocks of threads to rearrange data items between
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: *   yet most block-wide operations prefer a "blocked" partitioning of items across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: *   - Transposing between [<em>blocked</em>](index.html#sec5sec3) and [<em>striped</em>](index.html#sec5sec3) arrangements
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: *   - Transposing between [<em>blocked</em>](index.html#sec5sec3) and [<em>warp-striped</em>](index.html#sec5sec3) arrangements
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: *   - Scattering ranked items to a [<em>blocked arrangement</em>](index.html#sec5sec3)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * \blockcollective{BlockExchange}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * The code snippet below illustrates the conversion from a "blocked" to a "striped" arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: *     // Collectively exchange data into a blocked arrangement across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh: * Suppose the set of striped input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Transposes data items from <em>blocked</em> arrangement to <em>striped</em> arrangement.  Specialized for no timeslicing.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Transposes data items from <em>blocked</em> arrangement to <em>striped</em> arrangement.  Specialized for warp-timeslicing.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Transposes data items from <em>blocked</em> arrangement to <em>warp-striped</em> arrangement. Specialized for no timeslicing
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Transposes data items from <em>blocked</em> arrangement to <em>warp-striped</em> arrangement. Specialized for warp-timeslicing
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Transposes data items from <em>striped</em> arrangement to <em>blocked</em> arrangement.  Specialized for no timeslicing.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Transposes data items from <em>striped</em> arrangement to <em>blocked</em> arrangement.  Specialized for warp-timeslicing.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Transposes data items from <em>warp-striped</em> arrangement to <em>blocked</em> arrangement.  Specialized for no timeslicing
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Transposes data items from <em>warp-striped</em> arrangement to <em>blocked</em> arrangement.  Specialized for warp-timeslicing
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Exchanges data items annotated by rank into <em>blocked</em> arrangement.  Specialized for no timeslicing.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Exchanges data items annotated by rank into <em>blocked</em> arrangement.  Specialized for warp-timeslicing.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT          input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT         output_items[ITEMS_PER_THREAD],     ///< [out] Items to exchange, converting between <em>blocked</em> and <em>striped</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * \brief Transposes data items from <em>striped</em> arrangement to <em>blocked</em> arrangement.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * The code snippet below illustrates the conversion from a "striped" to a "blocked" arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Load a tile of ordered data into a striped arrangement across block threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Collectively exchange data into a blocked arrangement across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Suppose the set of striped input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      input_items[ITEMS_PER_THREAD],    ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT     output_items[ITEMS_PER_THREAD])   ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * \brief Transposes data items from <em>blocked</em> arrangement to <em>striped</em> arrangement.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * The code snippet below illustrates the conversion from a "blocked" to a "striped" arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Store data striped across block threads into an ordered tile
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Suppose the set of blocked input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      input_items[ITEMS_PER_THREAD],    ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT     output_items[ITEMS_PER_THREAD])   ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * \brief Transposes data items from <em>warp-striped</em> arrangement to <em>blocked</em> arrangement.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * The code snippet below illustrates the conversion from a "warp-striped" to a "blocked" arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Collectively exchange data into a blocked arrangement across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Suppose the set of warp-striped input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      input_items[ITEMS_PER_THREAD],    ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT     output_items[ITEMS_PER_THREAD])   ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * \brief Transposes data items from <em>blocked</em> arrangement to <em>warp-striped</em> arrangement.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * The code snippet below illustrates the conversion from a "blocked" to a "warp-striped" arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_exchange.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Specialize BlockExchange for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * Suppose the set of blocked input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      input_items[ITEMS_PER_THREAD],    ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT     output_items[ITEMS_PER_THREAD])   ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:     * \brief Exchanges data items annotated by rank into <em>blocked</em> arrangement.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT     output_items[ITEMS_PER_THREAD],     ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT     output_items[ITEMS_PER_THREAD],     ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT     output_items[ITEMS_PER_THREAD],     ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      input_items[ITEMS_PER_THREAD],      ///< [in] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        OutputT     output_items[ITEMS_PER_THREAD],     ///< [out] Items from exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      items[ITEMS_PER_THREAD])   ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      items[ITEMS_PER_THREAD])   ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      items[ITEMS_PER_THREAD])    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      items[ITEMS_PER_THREAD])    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      items[ITEMS_PER_THREAD],    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      items[ITEMS_PER_THREAD],    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      items[ITEMS_PER_THREAD],    ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_exchange.cuh:        InputT      items[ITEMS_PER_THREAD],        ///< [in-out] Items to exchange, converting between <em>striped</em> and <em>blocked</em> arrangements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * The cub::BlockScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix sum/scan of items partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:#include "specializations/block_scan_raking.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:#include "specializations/block_scan_warp_scans.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * \brief BlockScanAlgorithm enumerates alternative algorithms for cub::BlockScan to compute a parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \image html block_scan_raking.png
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * <div class="centercaption">\p BLOCK_SCAN_RAKING data flow for a hypothetical 16-thread thread block and 4-thread raking warp.</div>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \image html block_scan_warpscans.png
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * <div class="centercaption">\p BLOCK_SCAN_WARP_SCANS data flow for a hypothetical 16-thread thread block and 4-thread raking warp.</div>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * \brief The BlockScan class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel prefix sum/scan of items partitioned across a CUDA thread block. ![](block_scan_logo.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * \tparam BLOCK_DIM_X      The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * \tparam BLOCK_DIM_Y      <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * \tparam BLOCK_DIM_Z      <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * - Invokes a minimal number of minimal block-wide synchronization barriers (only
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: *   - \blocksize
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * \blockcollective{BlockScan}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: *     // Collectively compute the block-wide exclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * cannot be used with thread block sizes not a multiple of the
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes one input element.  The value of 0 is applied as the initial value, and is assigned to \p output in <em>thread</em><sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide exclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>1, 1, ..., 1</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes one input element.  The value of 0 is applied as the initial value, and is assigned to \p output in <em>thread</em><sub>0</sub>.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide exclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     BlockScan(temp_storage).ExclusiveSum(thread_data, thread_data, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>1, 1, ..., 1</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Furthermore the value \p 128 will be stored in \p block_aggregate for all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T               &block_aggregate)               ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        ExclusiveScan(input, output, initial_value, cub::Sum(), block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes one input element.  Instead of using 0 as the block-wide prefix, the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - The \p block_prefix_callback_op functor must implement a member function <tt>T operator()(T block_aggregate)</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor's input parameter \p block_aggregate is the same value also returned by the scan operation.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor will be invoked by the first warp of threads in the block, however only the return value from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   <em>lane</em><sub>0</sub> is applied as the block-wide prefix.  Can be stateful.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * The code snippet below illustrates a single thread block that progressively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * prefix functor to maintain a running total between block-wide scans.  Each tile consists
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Callback operator to be entered by the first warp of threads in the block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Thread-0 is responsible for returning a value for seeding the block-wide scan.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     __device__ int operator()(int block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         running_total += block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Have the block iterate over segments of items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     for (int block_offset = 0; block_offset < num_items; block_offset += 128)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         int thread_data = d_data[block_offset];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Collectively compute the block-wide exclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         d_data[block_offset] = thread_data;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \tparam BlockPrefixCallbackOp        <b>[inferred]</b> Call-back functor type having member <tt>T operator()(T block_aggregate)</tt>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a block-wide prefix to be applied to the logical input sequence.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        ExclusiveScan(input, output, cub::Sum(), block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes an array of consecutive input elements.  The value of 0 is applied as the initial value, and is assigned to \p output[0] in <em>thread</em><sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide exclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{ [1,1,1,1], [1,1,1,1], ..., [1,1,1,1] }</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes an array of consecutive input elements.  The value of 0 is applied as the initial value, and is assigned to \p output[0] in <em>thread</em><sub>0</sub>.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide exclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     BlockScan(temp_storage).ExclusiveSum(thread_data, thread_data, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{ [1,1,1,1], [1,1,1,1], ..., [1,1,1,1] }</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Furthermore the value \p 512 will be stored in \p block_aggregate for all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T                 &block_aggregate)                 ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        ExclusiveScan(input, output, initial_value, cub::Sum(), block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes an array of consecutive input elements.  Instead of using 0 as the block-wide prefix, the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - The \p block_prefix_callback_op functor must implement a member function <tt>T operator()(T block_aggregate)</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor's input parameter \p block_aggregate is the same value also returned by the scan operation.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor will be invoked by the first warp of threads in the block, however only the return value from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   <em>lane</em><sub>0</sub> is applied as the block-wide prefix.  Can be stateful.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * The code snippet below illustrates a single thread block that progressively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * prefix functor to maintain a running total between block-wide scans.  Each tile consists
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * of 512 integer items that are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Callback operator to be entered by the first warp of threads in the block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Thread-0 is responsible for returning a value for seeding the block-wide scan.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     __device__ int operator()(int block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         running_total += block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockLoad, BlockStore, and BlockScan for a 1D block of 128 threads, 4 ints per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Have the block iterate over segments of items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     for (int block_offset = 0; block_offset < num_items; block_offset += 128 * 4)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         BlockLoad(temp_storage.load).Load(d_data + block_offset, thread_data);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Collectively compute the block-wide exclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         BlockStore(temp_storage.store).Store(d_data + block_offset, thread_data);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \tparam BlockPrefixCallbackOp        <b>[inferred]</b> Call-back functor type having member <tt>T operator()(T block_aggregate)</tt>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)    ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a block-wide prefix to be applied to the logical input sequence.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        ExclusiveScan(input, output, cub::Sum(), block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide exclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>0, -1, 2, -3, ..., 126, -127</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide exclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     BlockScan(temp_storage).ExclusiveScan(thread_data, thread_data, INT_MIN, cub::Max(), block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>0, -1, 2, -3, ..., 126, -127</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Furthermore the value \p 126 will be stored in \p block_aggregate for all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T               &block_aggregate)   ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        InternalBlockScan(temp_storage).ExclusiveScan(input, output, initial_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - The \p block_prefix_callback_op functor must implement a member function <tt>T operator()(T block_aggregate)</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor's input parameter \p block_aggregate is the same value also returned by the scan operation.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor will be invoked by the first warp of threads in the block, however only the return value from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   <em>lane</em><sub>0</sub> is applied as the block-wide prefix.  Can be stateful.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * The code snippet below illustrates a single thread block that progressively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * prefix functor to maintain a running total between block-wide scans.  Each tile consists
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Callback operator to be entered by the first warp of threads in the block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Thread-0 is responsible for returning a value for seeding the block-wide scan.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     __device__ int operator()(int block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         running_total = (block_aggregate > old_prefix) ? block_aggregate : old_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Have the block iterate over segments of items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     for (int block_offset = 0; block_offset < num_items; block_offset += 128)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         int thread_data = d_data[block_offset];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Collectively compute the block-wide exclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         d_data[block_offset] = thread_data;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \tparam BlockPrefixCallbackOp        <b>[inferred]</b> Call-back functor type having member <tt>T operator()(T block_aggregate)</tt>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a block-wide prefix to be applied to the logical input sequence.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        InternalBlockScan(temp_storage).ExclusiveScan(input, output, scan_op, block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes an array of consecutive input elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide exclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes an array of consecutive input elements.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide exclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     BlockScan(temp_storage).ExclusiveScan(thread_data, thread_data, INT_MIN, cub::Max(), block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{ [0,-1,2,-3], [4,-5,6,-7], ..., [508,-509,510,-511] }</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Furthermore the value \p 510 will be stored in \p block_aggregate for all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T                 &block_aggregate)             ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        ExclusiveScan(thread_prefix, thread_prefix, initial_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes an array of consecutive input elements.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - The \p block_prefix_callback_op functor must implement a member function <tt>T operator()(T block_aggregate)</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor's input parameter \p block_aggregate is the same value also returned by the scan operation.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor will be invoked by the first warp of threads in the block, however only the return value from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   <em>lane</em><sub>0</sub> is applied as the block-wide prefix.  Can be stateful.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * The code snippet below illustrates a single thread block that progressively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * prefix functor to maintain a running total between block-wide scans.  Each tile consists
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Callback operator to be entered by the first warp of threads in the block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Thread-0 is responsible for returning a value for seeding the block-wide scan.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     __device__ int operator()(int block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         running_total = (block_aggregate > old_prefix) ? block_aggregate : old_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockLoad, BlockStore, and BlockScan for a 1D block of 128 threads, 4 ints per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Have the block iterate over segments of items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     for (int block_offset = 0; block_offset < num_items; block_offset += 128 * 4)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         BlockLoad(temp_storage.load).Load(d_data + block_offset, thread_data);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Collectively compute the block-wide exclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         BlockStore(temp_storage.store).Store(d_data + block_offset, thread_data);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \tparam BlockPrefixCallbackOp    <b>[inferred]</b> Call-back functor type having member <tt>T operator()(T block_aggregate)</tt>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a block-wide prefix to be applied to the logical input sequence.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        ExclusiveScan(thread_prefix, thread_prefix, scan_op, block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T               &block_aggregate)               ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        InternalBlockScan(temp_storage).ExclusiveScan(input, output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes an array of consecutive input elements.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an exclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes an array of consecutive input elements.  Also provides every thread with the block-wide \p block_aggregate of all inputs.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T               &block_aggregate)               ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        ExclusiveScan(thread_partial, thread_partial, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide inclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>1, 1, ..., 1</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide inclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     BlockScan(temp_storage).InclusiveSum(thread_data, thread_data, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>1, 1, ..., 1</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Furthermore the value \p 128 will be stored in \p block_aggregate for all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T               &block_aggregate)               ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        InclusiveScan(input, output, cub::Sum(), block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes one input element.  Instead of using 0 as the block-wide prefix, the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - The \p block_prefix_callback_op functor must implement a member function <tt>T operator()(T block_aggregate)</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor's input parameter \p block_aggregate is the same value also returned by the scan operation.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor will be invoked by the first warp of threads in the block, however only the return value from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   <em>lane</em><sub>0</sub> is applied as the block-wide prefix.  Can be stateful.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * The code snippet below illustrates a single thread block that progressively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * prefix functor to maintain a running total between block-wide scans.  Each tile consists
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Callback operator to be entered by the first warp of threads in the block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Thread-0 is responsible for returning a value for seeding the block-wide scan.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     __device__ int operator()(int block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         running_total += block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Have the block iterate over segments of items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     for (int block_offset = 0; block_offset < num_items; block_offset += 128)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         int thread_data = d_data[block_offset];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Collectively compute the block-wide inclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         d_data[block_offset] = thread_data;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \tparam BlockPrefixCallbackOp          <b>[inferred]</b> Call-back functor type having member <tt>T operator()(T block_aggregate)</tt>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a block-wide prefix to be applied to the logical input sequence.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        InclusiveScan(input, output, cub::Sum(), block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes an array of consecutive input elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide inclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{ [1,1,1,1], [1,1,1,1], ..., [1,1,1,1] }</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes an array of consecutive input elements.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide inclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     BlockScan(temp_storage).InclusiveSum(thread_data, thread_data, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Furthermore the value \p 512 will be stored in \p block_aggregate for all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T               &block_aggregate)               ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            InclusiveSum(input[0], output[0], block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            ExclusiveSum(thread_prefix, thread_prefix, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using addition (+) as the scan operator.  Each thread contributes an array of consecutive input elements.  Instead of using 0 as the block-wide prefix, the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - The \p block_prefix_callback_op functor must implement a member function <tt>T operator()(T block_aggregate)</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor's input parameter \p block_aggregate is the same value also returned by the scan operation.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor will be invoked by the first warp of threads in the block, however only the return value from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   <em>lane</em><sub>0</sub> is applied as the block-wide prefix.  Can be stateful.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * The code snippet below illustrates a single thread block that progressively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * prefix functor to maintain a running total between block-wide scans.  Each tile consists
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * of 512 integer items that are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Callback operator to be entered by the first warp of threads in the block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Thread-0 is responsible for returning a value for seeding the block-wide scan.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     __device__ int operator()(int block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         running_total += block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockLoad, BlockStore, and BlockScan for a 1D block of 128 threads, 4 ints per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Have the block iterate over segments of items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     for (int block_offset = 0; block_offset < num_items; block_offset += 128 * 4)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         BlockLoad(temp_storage.load).Load(d_data + block_offset, thread_data);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Collectively compute the block-wide inclusive prefix sum
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         BlockStore(temp_storage.store).Store(d_data + block_offset, thread_data);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \tparam BlockPrefixCallbackOp        <b>[inferred]</b> Call-back functor type having member <tt>T operator()(T block_aggregate)</tt>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a block-wide prefix to be applied to the logical input sequence.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            InclusiveSum(input[0], output[0], block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            ExclusiveSum(thread_prefix, thread_prefix, block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide inclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>0, -1, 2, -3, ..., 126, -127</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide inclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     BlockScan(temp_storage).InclusiveScan(thread_data, thread_data, cub::Max(), block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>0, -1, 2, -3, ..., 126, -127</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Furthermore the value \p 126 will be stored in \p block_aggregate for all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T               &block_aggregate)               ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        InternalBlockScan(temp_storage).InclusiveScan(input, output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - The \p block_prefix_callback_op functor must implement a member function <tt>T operator()(T block_aggregate)</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor's input parameter \p block_aggregate is the same value also returned by the scan operation.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor will be invoked by the first warp of threads in the block, however only the return value from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   <em>lane</em><sub>0</sub> is applied as the block-wide prefix.  Can be stateful.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * The code snippet below illustrates a single thread block that progressively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * prefix functor to maintain a running total between block-wide scans.  Each tile consists
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Callback operator to be entered by the first warp of threads in the block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Thread-0 is responsible for returning a value for seeding the block-wide scan.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     __device__ int operator()(int block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         running_total = (block_aggregate > old_prefix) ? block_aggregate : old_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Have the block iterate over segments of items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     for (int block_offset = 0; block_offset < num_items; block_offset += 128)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         int thread_data = d_data[block_offset];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Collectively compute the block-wide inclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         d_data[block_offset] = thread_data;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \tparam BlockPrefixCallbackOp        <b>[inferred]</b> Call-back functor type having member <tt>T operator()(T block_aggregate)</tt>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a block-wide prefix to be applied to the logical input sequence.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        InternalBlockScan(temp_storage).InclusiveScan(input, output, scan_op, block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes an array of consecutive input elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide inclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is <tt>{ [0,-1,2,-3], [4,-5,6,-7], ..., [508,-509,510,-511] }</tt>.  The
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes an array of consecutive input elements.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockScan for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Collectively compute the block-wide inclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     int block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     BlockScan(temp_storage).InclusiveScan(thread_data, thread_data, cub::Max(), block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * Furthermore the value \p 510 will be stored in \p block_aggregate for all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        T               &block_aggregate)               ///< [out] block-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            InclusiveScan(input[0], output[0], scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            // Exclusive thread block-scan (with no initial value)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            ExclusiveScan(thread_prefix, thread_prefix, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \brief Computes an inclusive block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes an array of consecutive input elements.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - The \p block_prefix_callback_op functor must implement a member function <tt>T operator()(T block_aggregate)</tt>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor's input parameter \p block_aggregate is the same value also returned by the scan operation.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   The functor will be invoked by the first warp of threads in the block, however only the return value from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *   <em>lane</em><sub>0</sub> is applied as the block-wide prefix.  Can be stateful.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * The code snippet below illustrates a single thread block that progressively
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * prefix functor to maintain a running total between block-wide scans.  Each tile consists
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_scan.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Callback operator to be entered by the first warp of threads in the block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Thread-0 is responsible for returning a value for seeding the block-wide scan.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     __device__ int operator()(int block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         running_total = (block_aggregate > old_prefix) ? block_aggregate : old_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Specialize BlockLoad, BlockStore, and BlockScan for a 1D block of 128 threads, 4 ints per thread
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     // Have the block iterate over segments of items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *     for (int block_offset = 0; block_offset < num_items; block_offset += 128 * 4)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         BlockLoad(temp_storage.load).Load(d_data + block_offset, thread_data);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         // Collectively compute the block-wide inclusive prefix max scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     *         BlockStore(temp_storage.store).Store(d_data + block_offset, thread_data);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:     * \tparam BlockPrefixCallbackOp    <b>[inferred]</b> Call-back functor type having member <tt>T operator()(T block_aggregate)</tt>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a block-wide prefix to be applied to the logical input sequence.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            InclusiveScan(input[0], output[0], scan_op, block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            // Exclusive thread block-scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh:            ExclusiveScan(thread_prefix, thread_prefix, scan_op, block_prefix_callback_op);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_scan.cuh: * \example example_block_scan.cu
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_adjacent_difference.cuh: * The cub::BlockDiscontinuity class provides [<em>collective</em>](index.html#sec0) methods for flagging discontinuities within an ordered set of items partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_adjacent_difference.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * Operations for writing linear segments of data from the CUDA thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:#include "block_exchange.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief Store a blocked arrangement of items across a thread block into a linear segment of items.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    int                 linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT thread_itr = block_itr + (linear_tid * ITEMS_PER_THREAD);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    // Store directly in thread-blocked order
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief Store a blocked arrangement of items across a thread block into a linear segment of items, guarded by range
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    int                 linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT thread_itr = block_itr + (linear_tid * ITEMS_PER_THREAD);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    // Store directly in thread-blocked order
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief Store a blocked arrangement of items across a thread block into a linear segment of items.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * The output offset (\p block_ptr + \p block_offset) must be quad-item aligned,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    int                 linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    T                   *block_ptr,                 ///< [in] Input pointer for storing from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    Vector *block_ptr_vectors = reinterpret_cast<Vector*>(const_cast<T*>(block_ptr));
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    StoreDirectBlocked(linear_tid, block_ptr_vectors, raw_vector);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief Store a striped arrangement of data across the thread block into a linear segment of items.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \tparam BLOCK_THREADS        The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    int                 linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT thread_itr = block_itr + linear_tid;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief Store a striped arrangement of data across the thread block into a linear segment of items, guarded by range
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \tparam BLOCK_THREADS        The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    int                 linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT thread_itr = block_itr + linear_tid;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief Store a warp-striped arrangement of data across the thread block into a linear segment of items.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * The number of threads in the thread block must be a multiple of the architecture's warp size.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    int                 linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT thread_itr = block_itr + warp_offset + tid;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief Store a warp-striped arrangement of data across the thread block into a linear segment of items, guarded by range
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * The number of threads in the thread block must be a multiple of the architecture's warp size.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    int                 linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:    OutputIteratorT thread_itr = block_itr + warp_offset + tid;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief cub::BlockStoreAlgorithm enumerates alternative algorithms for cub::BlockStore to write a blocked arrangement of items across a CUDA thread block to a linear segment of memory.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * A [<em>blocked arrangement</em>](index.html#sec5sec3) of data is written
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * A [<em>blocked arrangement</em>](index.html#sec5sec3) of data is written directly
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     *   - The block output offset is not quadword-aligned
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * A [<em>blocked arrangement</em>](index.html#sec5sec3) is locally
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * A [<em>blocked arrangement</em>](index.html#sec5sec3) is locally
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * A [<em>blocked arrangement</em>](index.html#sec5sec3) is locally
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \brief The BlockStore class provides [<em>collective</em>](index.html#sec0) data movement methods for writing a [<em>blocked arrangement</em>](index.html#sec5sec3) of items partitioned across a CUDA thread block to a linear segment of memory.  ![](block_store_logo.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \tparam BLOCK_DIM_X          The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \tparam WARP_TIME_SLICING    <b>[optional]</b> Whether or not only one warp's worth of shared memory should be allocated and time-sliced among block-warps during any load-related data transpositions (versus each warp having its own storage). (default: false)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: *   -# <b>cub::BLOCK_STORE_DIRECT</b>.  A [<em>blocked arrangement</em>](index.html#sec5sec3) of data is written
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: *   -# <b>cub::BLOCK_STORE_VECTORIZE</b>.  A [<em>blocked arrangement</em>](index.html#sec5sec3)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: *   -# <b>cub::BLOCK_STORE_TRANSPOSE</b>.  A [<em>blocked arrangement</em>](index.html#sec5sec3)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: *   -# <b>cub::BLOCK_STORE_WARP_TRANSPOSE</b>.  A [<em>blocked arrangement</em>](index.html#sec5sec3)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * \blockcollective{BlockStore}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * The code snippet below illustrates the storing of a "blocked" arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * #include <cub/cub.cuh>   // or equivalently <cub/block/block_store.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: *     // Specialize BlockStore for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh: * Suppose the set of \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectBlocked(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectBlocked(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            T                   *block_ptr,                 ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectBlockedVectorized(linear_tid, block_ptr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT    block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectBlocked(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectBlocked(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            /// Temporary storage for partially-full block guard
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT   block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, temp_storage.valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            /// Temporary storage for partially-full block guard
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT   block_itr,                    ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectWarpStriped(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT   block_itr,                    ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectWarpStriped(linear_tid, block_itr, items, temp_storage.valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            /// Temporary storage for partially-full block guard
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectWarpStriped(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            OutputIteratorT   block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:            StoreDirectWarpStriped(linear_tid, block_itr, items, temp_storage.valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * The code snippet below illustrates the storing of a "blocked" arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_store.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     *     // Specialize BlockStore for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * Suppose the set of \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:        OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:        InternalStore(temp_storage, linear_tid).Store(block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * The code snippet below illustrates the guarded storing of a "blocked" arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_store.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     *     // Specialize BlockStore for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:     * Suppose the set of \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:        OutputIteratorT     block_itr,                  ///< [in] The thread block's base output iterator for storing to
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_store.cuh:        InternalStore(temp_storage, linear_tid).Store(block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh: * cub::BlockRadixRank provides operations for ranking unsigned integer types within a CUDA thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:#include "../block/block_scan.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh: * \brief BlockRadixRank provides operations for ranking unsigned integer types within a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh: * \tparam BLOCK_DIM_X          The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh: * \tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh: * \tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh: * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:        // The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:        typename BlockScan::TempStorage block_scan;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:        __device__ __forceinline__ PackedCounter operator()(PackedCounter block_aggregate)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:            PackedCounter block_prefix = 0;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:                block_prefix += block_aggregate << (sizeof(DigitCounter) * 8 * PACKED);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:            return block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:        BlockScan(temp_storage.block_scan).ExclusiveSum(raking_partial, exclusive_partial, prefix_call_back);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:            // Add in thread block exclusive prefix
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:        // The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:        typename BlockScanT::TempStorage            block_scan;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_rank.cuh:        BlockScanT(temp_storage.block_scan).ExclusiveSum(scan_counters, scan_counters);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * Operations for reading linear tiles of data into the CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:#include "block_exchange.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a blocked arrangement across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT thread_itr = block_itr + (linear_tid * ITEMS_PER_THREAD);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    // Load directly in thread-blocked order
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a blocked arrangement across the thread block, guarded by range.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT thread_itr = block_itr + (linear_tid * ITEMS_PER_THREAD);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a blocked arrangement across the thread block, guarded by range, with a fall-back assignment of out-of-bound elements..
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    LoadDirectBlocked(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int    linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    T      *block_ptr,                 ///< [in] Input pointer for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    Vector* vec_ptr = reinterpret_cast<Vector*>(block_ptr) + (linear_tid * VECTORS_PER_THREAD);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    // Load directly in thread-blocked order
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a blocked arrangement across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * The input offset (\p block_ptr + \p block_offset) must be quad-item aligned
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    T   *block_ptr,                 ///< [in] Input pointer for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InternalLoadDirectBlockedVectorized<LOAD_DEFAULT>(linear_tid, block_ptr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a striped arrangement across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \tparam BLOCK_THREADS        The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT thread_itr = block_itr + linear_tid;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a striped arrangement across the thread block, guarded by range
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \tparam BLOCK_THREADS        The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT thread_itr = block_itr + linear_tid;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a striped arrangement across the thread block, guarded by range, with a fall-back assignment of out-of-bound elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \tparam BLOCK_THREADS        The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a warp-striped arrangement across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * The number of threads in the thread block must be a multiple of the architecture's warp size.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT thread_itr = block_itr + warp_offset + tid ;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a warp-striped arrangement across the thread block, guarded by range
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * The number of threads in the thread block must be a multiple of the architecture's warp size.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT thread_itr = block_itr + warp_offset + tid ;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief Load a linear segment of items into a warp-striped arrangement across the thread block, guarded by range, with a fall-back assignment of out-of-bound elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * The number of threads in the thread block must be a multiple of the architecture's warp size.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    int             linear_tid,                 ///< [in] A suitable 1D thread-identifier for the calling thread (e.g., <tt>(threadIdx.y * blockDim.x) + linear_tid</tt> for 2D thread blocks)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:    LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief cub::BlockLoadAlgorithm enumerates alternative algorithms for cub::BlockLoad to read a linear segment of data from memory into a blocked arrangement across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief cub::BlockLoadAlgorithm enumerates alternative algorithms for cub::BlockLoad to read a linear segment of data from memory into a blocked arrangement across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * A [<em>blocked arrangement</em>](index.html#sec5sec3) of data is read
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * A [<em>blocked arrangement</em>](index.html#sec5sec3) of data is read
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     *   - The block input offset is not quadword-aligned
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * [<em>blocked arrangement</em>](index.html#sec5sec3).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * [<em>blocked arrangement</em>](index.html#sec5sec3).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * [<em>blocked arrangement</em>](index.html#sec5sec3). To reduce the shared memory
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \brief The BlockLoad class provides [<em>collective</em>](index.html#sec0) data movement methods for loading a linear segment of items from memory into a [<em>blocked arrangement</em>](index.html#sec5sec3) across a CUDA thread block.  ![](block_load_logo.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \tparam BLOCK_DIM_X          The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \tparam WARP_TIME_SLICING    <b>[optional]</b> Whether or not only one warp's worth of shared memory should be allocated and time-sliced among block-warps during any load-related data transpositions (versus each warp having its own storage). (default: false)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: *   -# <b>cub::BLOCK_LOAD_DIRECT</b>.  A [<em>blocked arrangement</em>](index.html#sec5sec3)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: *   -# <b>cub::BLOCK_LOAD_VECTORIZE</b>.  A [<em>blocked arrangement</em>](index.html#sec5sec3)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: *      [<em>blocked arrangement</em>](index.html#sec5sec3).  [More...](\ref cub::BlockLoadAlgorithm)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: *      [<em>blocked arrangement</em>](index.html#sec5sec3).  [More...](\ref cub::BlockLoadAlgorithm)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: *      [<em>blocked arrangement</em>](index.html#sec5sec3) one warp at a time.  [More...](\ref cub::BlockLoadAlgorithm)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * \blockcollective{BlockLoad}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * segment of 512 integers into a "blocked" arrangement across 128 threads where each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * #include <cub/cub.cuh>   // or equivalently <cub/block/block_load.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: *     // Specialize BlockLoad for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: *     // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh: * The set of \p thread_data across the block of threads in those threads will be
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectBlocked(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectBlocked(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectBlocked(linear_tid, block_itr, items, valid_items, oob_default);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputT               *block_ptr,                     ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InternalLoadDirectBlockedVectorized<LOAD_DEFAULT>(linear_tid, block_ptr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            const InputT         *block_ptr,                     ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InternalLoadDirectBlockedVectorized<LOAD_DEFAULT>(linear_tid, block_ptr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            CacheModifiedInputIterator<MODIFIER, ValueType, OffsetT>    block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InternalLoadDirectBlockedVectorized<MODIFIER>(linear_tid, block_itr.ptr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            _InputIteratorT   block_itr,                    ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectBlocked(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectBlocked(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectBlocked(linear_tid, block_itr, items, valid_items, oob_default);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectStriped<BLOCK_THREADS>(linear_tid, block_itr, items, valid_items, oob_default);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectWarpStriped(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items, oob_default);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectWarpStriped(linear_tid, block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            InputIteratorT  block_itr,                      ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:            LoadDirectWarpStriped(linear_tid, block_itr, items, valid_items, oob_default);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * segment of 512 integers into a "blocked" arrangement across 128 threads where each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_load.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     *     // Specialize BlockLoad for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     *     // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * The set of \p thread_data across the block of threads in those threads will be
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:        InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:        InternalLoad(temp_storage, linear_tid).Load(block_itr, items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * segment of 512 integers into a "blocked" arrangement across 128 threads where each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_load.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     *     // Specialize BlockLoad for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     *     // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * The set of \p thread_data across the block of threads in those threads will be
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:        InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:        InternalLoad(temp_storage, linear_tid).Load(block_itr, items, valid_items);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * segment of 512 integers into a "blocked" arrangement across 128 threads where each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_load.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     *     // Specialize BlockLoad for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     *     // Load a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:     * The set of \p thread_data across the block of threads in those threads will be
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:        InputIteratorT  block_itr,                  ///< [in] The thread block's base input iterator for loading from
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_load.cuh:        InternalLoad(temp_storage, linear_tid).Load(block_itr, items, valid_items, oob_default);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * The cub::BlockDiscontinuity class provides [<em>collective</em>](index.html#sec0) methods for flagging discontinuities within an ordered set of items partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * \brief The BlockDiscontinuity class provides [<em>collective</em>](index.html#sec0) methods for flagging discontinuities within an ordered set of items partitioned across a CUDA thread block. ![](discont_logo.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * \tparam BLOCK_DIM_X      The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * \tparam BLOCK_DIM_Y      <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * \tparam BLOCK_DIM_Z      <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * \blockcollective{BlockDiscontinuity}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh: * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * \brief Sets head flags indicating discontinuities between items partitioned across the thread block, for which the first item has no reference and is always flagged.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * \brief Sets head flags indicating discontinuities between items partitioned across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * \brief Sets tail flags indicating discontinuities between items partitioned across the thread block, for which the last item has no reference and is always flagged.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * \brief Sets tail flags indicating discontinuities between items partitioned across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * \brief Sets both head and tail flags indicating discontinuities between items partitioned across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * \brief Sets both head and tail flags indicating discontinuities between items partitioned across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * \brief Sets both head and tail flags indicating discontinuities between items partitioned across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * \brief Sets both head and tail flags indicating discontinuities between items partitioned across the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * - \blocked
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_discontinuity.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Specialize BlockDiscontinuity for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_discontinuity.cuh:     * Suppose the set of input \p thread_data across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * The cub::BlockRadixSort class provides [<em>collective</em>](index.html#sec0) methods for radix sorting of items partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:#include "block_exchange.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:#include "block_radix_rank.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * \brief The BlockRadixSort class provides [<em>collective</em>](index.html#sec0) methods for sorting items partitioned across a CUDA thread block using a radix sorting method.  ![](sorting_logo.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * \tparam BLOCK_DIM_X          The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * \tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * \tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * \blockcollective{BlockRadixSort}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer items each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:        // The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:    /// ExchangeValues (specialized for key-value sort, to-blocked arrangement)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:        Int2Type<true>  /*is_blocked*/)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:        // Exchange values through shared memory in blocked arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:        Int2Type<false> /*is_blocked*/)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:        // Exchange values through shared memory in blocked arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:        Int2Type<IS_BLOCKED>    /*is_blocked*/)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:    /// Sort blocked arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:            // Rank the blocked keys
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:            // Exchange keys through shared memory in blocked arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:            // Exchange values through shared memory in blocked arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:    /// Sort blocked -> striped arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:            // Rank the blocked keys
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:            // Exchange keys through shared memory in blocked arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:            // Exchange values through shared memory in blocked arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \name Sorting (blocked arrangements)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \brief Performs an ascending block-wide radix sort over a [<em>blocked arrangement</em>](index.html#sec5sec3) of keys.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer keys each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \brief Performs an ascending block-wide radix sort across a [<em>blocked arrangement</em>](index.html#sec5sec3) of keys and values.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer keys and values each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Collectively sort the keys and values among block threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \brief Performs a descending block-wide radix sort over a [<em>blocked arrangement</em>](index.html#sec5sec3) of keys.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer keys each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \brief Performs a descending block-wide radix sort across a [<em>blocked arrangement</em>](index.html#sec5sec3) of keys and values.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer keys and values each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Collectively sort the keys and values among block threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \name Sorting (blocked arrangement -> striped arrangement)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \brief Performs an ascending radix sort across a [<em>blocked arrangement</em>](index.html#sec5sec3) of keys, leaving them in a [<em>striped arrangement</em>](index.html#sec5sec3).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * are initially partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer keys each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \brief Performs an ascending radix sort across a [<em>blocked arrangement</em>](index.html#sec5sec3) of keys and values, leaving them in a [<em>striped arrangement</em>](index.html#sec5sec3).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * are initially partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer keys and values each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Collectively sort the keys and values among block threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \brief Performs a descending radix sort across a [<em>blocked arrangement</em>](index.html#sec5sec3) of keys, leaving them in a [<em>striped arrangement</em>](index.html#sec5sec3).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * are initially partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer keys each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * \brief Performs a descending radix sort across a [<em>blocked arrangement</em>](index.html#sec5sec3) of keys and values, leaving them in a [<em>striped arrangement</em>](index.html#sec5sec3).
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * are initially partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_radix_sort.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Specialize BlockRadixSort for a 1D block of 128 threads owning 4 integer keys and values each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     *     // Collectively sort the keys and values among block threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh:     * Suppose the set of input \p thread_keys across the block of threads is
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_radix_sort.cuh: * \example example_block_radix_sort.cu
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        T                               block_aggregate;                           ///< Shared prefix for the entire thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        // Compute block-wide exclusive scan.  The exclusive output from tid0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        ExclusiveScan(input, exclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        ExclusiveScan(input, exclusive_output, initial_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:                outer_warp_input, outer_warp_exclusive, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            temp_storage.block_aggregate                = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            // Retrieve block aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            block_aggregate = temp_storage.block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:                outer_warp_input, outer_warp_exclusive, initial_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            temp_storage.block_aggregate                = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        // Retrieve block aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        block_aggregate = temp_storage.block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  The call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a thread block-wide prefix to be applied to all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            T downsweep_prefix, block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            inner_scan.ExclusiveScan(upsweep, downsweep_prefix, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            // Use callback functor to get block prefix in lane0 and then broadcast to other lanes
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            block_prefix = inner_scan.Broadcast(block_prefix, 0);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            downsweep_prefix = scan_op(block_prefix, downsweep_prefix);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:                downsweep_prefix = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        InclusiveScan(input, inclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        T               &block_aggregate)               ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:                outer_warp_input, outer_warp_exclusive, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            temp_storage.block_aggregate                = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            // Retrieve block aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            block_aggregate = temp_storage.block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a thread block-wide prefix to be applied to all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            T downsweep_prefix, block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            inner_scan.ExclusiveScan(upsweep, downsweep_prefix, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            // Use callback functor to get block prefix in lane0 and then broadcast to other lanes
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            block_prefix = inner_scan.Broadcast(block_prefix, 0);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:            downsweep_prefix = scan_op(block_prefix, downsweep_prefix);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans3.cuh:                downsweep_prefix = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh: * cub::BlockReduceRaking provides raking-based methods of parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:#include "../../block/block_raking_layout.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh: * \brief BlockReduceRaking provides raking-based methods of parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:    /// Layout type for padded thread block raking grid
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:        typename BlockRakingLayout::TempStorage     raking_grid;         ///< Padded thread block raking grid
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:    /// Computes a thread block-wide reduction using the specified reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking.cuh:    /// Computes a thread block-wide reduction using addition (+) as the reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T                               block_prefix;               ///< Shared prefix for the entire thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T               &block_aggregate,   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:            warp_prefix = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        block_aggregate = scan_op(block_aggregate, addend);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        ApplyWarpAggregates(warp_prefix, scan_op, block_aggregate, Int2Type<WARP + 1>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T               &/*block_aggregate*/,   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Use the warp-wide aggregates to compute the calling warp's prefix.  Also returns block-wide aggregate in all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Accumulate block aggregates and save the one that is our warp's prefix
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        block_aggregate = temp_storage.warp_aggregates[0];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        ApplyWarpAggregates(warp_prefix, scan_op, block_aggregate, Int2Type<1>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:                warp_prefix = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:            block_aggregate = scan_op(block_aggregate, addend);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Use the warp-wide aggregates and initial-value to compute the calling warp's prefix.  Also returns block-wide aggregate in all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T               &block_aggregate,   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T warp_prefix = ComputeWarpPrefix(scan_op, warp_aggregate, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Compute block-wide exclusive scan.  The exclusive output from tid0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        ExclusiveScan(input, exclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        ExclusiveScan(input, exclusive_output, initial_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Compute the warp-wide prefix and block-wide aggregate for each warp.  Warp prefix for warp0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T warp_prefix = ComputeWarpPrefix(scan_op, inclusive_output, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Compute the warp-wide prefix and block-wide aggregate for each warp
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T warp_prefix = ComputeWarpPrefix(scan_op, inclusive_output, block_aggregate, initial_value);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a thread block-wide prefix to be applied to all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Compute block-wide exclusive scan.  The exclusive output from tid0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        ExclusiveScan(input, exclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Use the first warp to determine the thread block prefix, returning the result in lane0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:            T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:                temp_storage.block_prefix = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:                exclusive_output = block_prefix;                // The block prefix is the exclusive output for tid0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Incorporate thread block prefix into outputs
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T block_prefix = temp_storage.block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:            exclusive_output = scan_op(block_prefix, exclusive_output);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        InclusiveScan(input, inclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T               &block_aggregate)               ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Compute the warp-wide prefix and block-wide aggregate for each warp.  Warp prefix for warp0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T warp_prefix = ComputeWarpPrefix(scan_op, inclusive_output, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a thread block-wide prefix to be applied to all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        InclusiveScan(input, exclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Use the first warp to determine the thread block prefix, returning the result in lane0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:            T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:                temp_storage.block_prefix = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        // Incorporate thread block prefix into outputs
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        T block_prefix = temp_storage.block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans.cuh:        exclusive_output = scan_op(block_prefix, exclusive_output);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh: * The cub::BlockHistogramSort class provides sorting-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:#include "../../block/block_radix_sort.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:#include "../../block/block_discontinuity.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh: * \brief The BlockHistogramSort class provides sorting-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:    int         BLOCK_DIM_X,        ///< The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:    int         BLOCK_DIM_Y,        ///< The thread block length in threads along the Y dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:    int         BLOCK_DIM_Z,        ///< The thread block length in threads along the Z dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:    // Parameterize BlockRadixSort type for our thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:    // Parameterize BlockDiscontinuity type for our thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_sort.cuh:        // Sort bytes in blocked arrangement
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh: * cub::BlockScanWarpscans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh: * \brief BlockScanWarpScans provides warpscan-based variants of parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T                                           block_prefix;               ///< Shared prefix for the entire thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T               &block_aggregate,   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:            warp_prefix = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        block_aggregate = scan_op(block_aggregate, addend);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        ApplyWarpAggregates(warp_prefix, scan_op, block_aggregate, Int2Type<WARP + 1>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T               &block_aggregate,   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Use the warp-wide aggregates to compute the calling warp's prefix.  Also returns block-wide aggregate in all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Accumulate block aggregates and save the one that is our warp's prefix
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        block_aggregate = temp_storage.warp_aggregates[0];
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        ApplyWarpAggregates(warp_prefix, scan_op, block_aggregate, Int2Type<1>());
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:                warp_prefix = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:            block_aggregate = scan_op(block_aggregate, addend);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Use the warp-wide aggregates and initial-value to compute the calling warp's prefix.  Also returns block-wide aggregate in all threads.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T               &block_aggregate,   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T warp_prefix = ComputeWarpPrefix(scan_op, warp_aggregate, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Compute block-wide exclusive scan.  The exclusive output from tid0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        ExclusiveScan(input, exclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        ExclusiveScan(input, exclusive_output, initial_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Compute the warp-wide prefix and block-wide aggregate for each warp.  Warp prefix for warp0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh://        T warp_prefix = ComputeWarpPrefix(scan_op, inclusive_output, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        block_aggregate     = my_warp_scan.Broadcast(warp_inclusive, WARPS - 1);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Compute the warp-wide prefix and block-wide aggregate for each warp
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh://        T warp_prefix = ComputeWarpPrefix(scan_op, inclusive_output, block_aggregate, initial_value);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        block_aggregate     = my_warp_scan.Broadcast(warp_inclusive, WARPS - 1);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a thread block-wide prefix to be applied to all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Compute block-wide exclusive scan.  The exclusive output from tid0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        ExclusiveScan(input, exclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Use the first warp to determine the thread block prefix, returning the result in lane0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:            T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:                temp_storage.block_prefix = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:                exclusive_output = block_prefix;                // The block prefix is the exclusive output for tid0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Incorporate thread block prefix into outputs
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T block_prefix = temp_storage.block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:            exclusive_output = scan_op(block_prefix, exclusive_output);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        InclusiveScan(input, inclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T               &block_aggregate)               ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Compute the warp-wide prefix and block-wide aggregate for each warp.  Warp prefix for warp0 is invalid.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T warp_prefix = ComputeWarpPrefix(scan_op, inclusive_output, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a thread block-wide prefix to be applied to all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        InclusiveScan(input, exclusive_output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Use the first warp to determine the thread block prefix, returning the result in lane0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:            T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:                temp_storage.block_prefix = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        // Incorporate thread block prefix into outputs
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        T block_prefix = temp_storage.block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_warp_scans2.cuh:        exclusive_output = scan_op(block_prefix, exclusive_output);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh: * cub::BlockScanRaking provides variants of raking-based parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:#include "../../block/block_raking_layout.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh: * \brief BlockScanRaking provides variants of raking-based parallel prefix scan across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Layout type for padded thread block raking grid
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:        typename BlockRakingLayout::TempStorage     raking_grid;        ///< Padded thread block raking grid
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:        T                                           block_aggregate;    ///< Block aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.  With no initial value, the output computed for <em>thread</em><sub>0</sub> is undefined.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:        T               &block_aggregate)               ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            WarpScan(temp_storage.warp_scan).ExclusiveScan(input, output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                    temp_storage.block_aggregate = inclusive_partial;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            // Retrieve block aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            block_aggregate = temp_storage.block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:        T               &block_aggregate)   ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            WarpScan(temp_storage.warp_scan).ExclusiveScan(input, output, initial_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                WarpScan(temp_storage.warp_scan).ExclusiveScan(upsweep_partial, exclusive_partial, initial_value, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                    temp_storage.block_aggregate = block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            // Retrieve block aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            block_aggregate = temp_storage.block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Computes an exclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a thread block-wide prefix to be applied to all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            warp_scan.ExclusiveScan(input, output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            block_prefix = warp_scan.Broadcast(block_prefix, 0);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            output = scan_op(block_prefix, output);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                output = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                T exclusive_partial, block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                warp_scan.ExclusiveScan(upsweep_partial, exclusive_partial, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                // Obtain block-wide prefix in lane0, then broadcast to other lanes
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                block_prefix = warp_scan.Broadcast(block_prefix, 0);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                T downsweep_prefix = scan_op(block_prefix, exclusive_partial);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                    downsweep_prefix = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:        T               &block_aggregate)               ///< [out] Threadblock-wide aggregate reduction of input items
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            WarpScan(temp_storage.warp_scan).InclusiveScan(input, output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                    temp_storage.block_aggregate = inclusive_partial;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            // Retrieve block aggregate
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            block_aggregate = temp_storage.block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:    /// Computes an inclusive thread block-wide prefix scan using the specified binary \p scan_op functor.  Each thread contributes one input element.  the call-back functor \p block_prefix_callback_op is invoked by the first warp in the block, and the value returned by <em>lane</em><sub>0</sub> in that warp is used as the "seed" value that logically prefixes the thread block's scan inputs.  Also provides every thread with the block-wide \p block_aggregate of all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:        BlockPrefixCallbackOp   &block_prefix_callback_op)      ///< [in-out] <b>[<em>warp</em><sub>0</sub> only]</b> Call-back functor for specifying a thread block-wide prefix to be applied to all inputs.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            T block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            warp_scan.InclusiveScan(input, output, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            block_prefix = warp_scan.Broadcast(block_prefix, 0);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:            output = scan_op(block_prefix, output);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                T exclusive_partial, block_aggregate;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                warp_scan.ExclusiveScan(upsweep_partial, exclusive_partial, scan_op, block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                // Obtain block-wide prefix in lane0, then broadcast to other lanes
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                T block_prefix = block_prefix_callback_op(block_aggregate);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                block_prefix = warp_scan.Broadcast(block_prefix, 0);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                T downsweep_prefix = scan_op(block_prefix, exclusive_partial);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_scan_raking.cuh:                    downsweep_prefix = block_prefix;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * cub::BlockReduceRakingCommutativeOnly provides raking-based methods of parallel reduction across a CUDA thread block.  Does not support non-commutative reduction operators.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:#include "block_reduce_raking.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh: * \brief BlockReduceRakingCommutativeOnly provides raking-based methods of parallel reduction across a CUDA thread block.  Does not support non-commutative reduction operators.  Does not support block sizes that are not a multiple of the warp size.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:    /// Layout type for padded thread block raking grid
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:            typename BlockRakingLayout::TempStorage raking_grid;         ///< Padded thread block raking grid
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:        typename FallBack::TempStorage              fallback_storage;    ///< Fall-back storage for non-commutative block scan
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:    /// Computes a thread block-wide reduction using addition (+) as the reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_raking_commutative_only.cuh:    /// Computes a thread block-wide reduction using the specified reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_atomic.cuh: * The cub::BlockHistogramAtomic class provides atomic-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_histogram_atomic.cuh: * \brief The BlockHistogramAtomic class provides atomic-based methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh: * cub::BlockReduceWarpReductions provides variants of warp-reduction-based parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh: * \brief BlockReduceWarpReductions provides variants of warp-reduction-based parallel reduction across a CUDA thread block.  Supports non-commutative reduction operators.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:    int         BLOCK_DIM_X,    ///< The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:    int         BLOCK_DIM_Y,    ///< The thread block length in threads along the Y dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:    int         BLOCK_DIM_Z,    ///< The thread block length in threads along the Z dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:        /// Whether or not the logical warp size evenly divides the thread block size
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:        T                                   block_prefix;               ///< Shared prefix for the entire thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:    /// Returns block-wide aggregate in <em>thread</em><sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:    /// Computes a thread block-wide reduction using addition (+) as the reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:        // Update outputs and block_aggregate with warp-wide aggregates from lane-0s
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:    /// Computes a thread block-wide reduction using the specified reduction operator. The first num_valid threads each contribute one reduction partial.  The return value is only valid for thread<sub>0</sub>.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/specializations/block_reduce_warp_reductions.cuh:        // Update outputs and block_aggregate with warp-wide aggregates from lane-0s
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: * The cub::BlockHistogram class provides [<em>collective</em>](index.html#sec0) methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:#include "specializations/block_histogram_sort.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:#include "specializations/block_histogram_atomic.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: * \brief BlockHistogramAlgorithm enumerates alternative algorithms for the parallel construction of block-wide histograms.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: * \brief The BlockHistogram class provides [<em>collective</em>](index.html#sec0) methods for constructing block-wide histograms from data samples partitioned across a CUDA thread block. ![](histogram_logo.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: * \tparam BLOCK_DIM_X          The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: * \tparam BLOCK_DIM_Y          <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: * \tparam BLOCK_DIM_Z          <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: * \blockcollective{BlockHistogram}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: * #include <cub/cub.cuh>   // or equivalently <cub/block/block_histogram.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: *     // Specialize a 256-bin BlockHistogram type for a 1D block of 128 threads having 4 character samples each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: *     // Allocate shared memory for block-wide histogram bin counts
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh: *     // Compute the block-wide histogram
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_histogram.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Specialize a 256-bin BlockHistogram type for a 1D block of 128 threads having 4 character samples each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Allocate shared memory for block-wide histogram bin counts
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Initialize the block-wide histogram
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Update the block-wide histogram
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     * \brief Constructs a block-wide histogram in shared/device-accessible memory.  Each thread contributes an array of input elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_histogram.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Specialize a 256-bin BlockHistogram type for a 1D block of 128 threads having 4 character samples each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Allocate shared memory for block-wide histogram bin counts
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Compute the block-wide histogram
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     * \brief Updates an existing block-wide histogram in shared/device-accessible memory.  Each thread composites an array of input elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_histogram.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Specialize a 256-bin BlockHistogram type for a 1D block of 128 threads having 4 character samples each
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Allocate shared memory for block-wide histogram bin counts
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Initialize the block-wide histogram
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_histogram.cuh:     *     // Update the block-wide histogram
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * The cub::BlockReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:#include "specializations/block_reduce_raking.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:#include "specializations/block_reduce_raking_commutative_only.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:#include "specializations/block_reduce_warp_reductions.cuh"
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * reduction across a CUDA thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \image html block_reduce.png
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * <div class="centercaption">\p BLOCK_REDUCE_RAKING data flow for a hypothetical 16-thread thread block and 4-thread raking warp.</div>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * operators. \blocked.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \image html block_reduce.png
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * <div class="centercaption">\p BLOCK_REDUCE_RAKING data flow for a hypothetical 16-thread thread block and 4-thread raking warp.</div>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \image html block_scan_warpscans.png
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * <div class="centercaption">\p BLOCK_REDUCE_WARP_REDUCTIONS data flow for a hypothetical 16-thread thread block and 4-thread raking warp.</div>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * \brief The BlockReduce class provides [<em>collective</em>](index.html#sec0) methods for computing a parallel reduction of items partitioned across a CUDA thread block. ![](reduce_logo.png)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * \tparam BLOCK_DIM_X      The thread block length in threads along the X dimension
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * \tparam BLOCK_DIM_Y      <b>[optional]</b> The thread block length in threads along the Y dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * \tparam BLOCK_DIM_Z      <b>[optional]</b> The thread block length in threads along the Z dimension (default: 1)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * \blockcollective{BlockReduce}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: *     // Specialize BlockReduce for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: *     // Compute the block-wide sum for thread0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:        /// The thread block size in threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \brief Computes a block-wide reduction for thread<sub>0</sub> using the specified binary reduction functor.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Specialize BlockReduce for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Compute the block-wide max for thread0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \brief Computes a block-wide reduction for thread<sub>0</sub> using the specified binary reduction functor.  Each thread contributes an array of consecutive input elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Specialize BlockReduce for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Compute the block-wide max for thread0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \brief Computes a block-wide reduction for thread<sub>0</sub> using the specified binary reduction functor.  The first \p num_valid threads each contribute one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Specialize BlockReduce for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Compute the block-wide max for thread0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \brief Computes a block-wide reduction for thread<sub>0</sub> using addition (+) as the reduction operator.  Each thread contributes one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Specialize BlockReduce for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Compute the block-wide sum for thread0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \brief Computes a block-wide reduction for thread<sub>0</sub> using addition (+) as the reduction operator.  Each thread contributes an array of consecutive input elements.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * are partitioned in a [<em>blocked arrangement</em>](index.html#sec5sec3) across 128 threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Specialize BlockReduce for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Obtain a segment of consecutive items that are blocked across threads
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Compute the block-wide sum for thread0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * \brief Computes a block-wide reduction for thread<sub>0</sub> using addition (+) as the reduction operator.  The first \p num_valid threads each contribute one input element.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     * #include <cub/cub.cuh>   // or equivalently <cub/block/block_reduce.cuh>
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Specialize BlockReduce for a 1D block of 128 threads on type int
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh:     *     // Compute the block-wide sum for thread0
debug_tools/WatchYourStep/ptxjitplus/inc/cub/block/block_reduce.cuh: * \example example_block_reduce.cu
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_ptx.cuh: * \brief Returns the row-major linear thread identifier for a multidimensional thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_ptx.cuh:__device__ __forceinline__ int RowMajorTid(int block_dim_x, int block_dim_y, int block_dim_z)
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_ptx.cuh:    return ((block_dim_z == 1) ? 0 : (threadIdx.z * block_dim_x * block_dim_y)) +
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_ptx.cuh:            ((block_dim_y == 1) ? 0 : (threadIdx.y * block_dim_x)) +
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_ptx.cuh: * \brief Returns the warp ID of the calling thread.  Warp ID is guaranteed to be unique among warps, but may not correspond to a zero-based ranking within the thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh: * \brief Computes maximum SM occupancy in thread blocks for executing the given kernel function pointer \p kernel_ptr on the current device with \p block_threads per thread block.
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:    int                 &max_sm_occupancy,          ///< [out] maximum number of thread blocks that can reside on a single SM
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:    int                 block_threads,              ///< [in] Number of threads per thread block
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:    (void)block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:        block_threads,
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:    int block_threads;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:    KernelConfig() : block_threads(0), items_per_thread(0), tile_size(0), sm_occupancy(0) {}
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:        block_threads        = AgentPolicyT::BLOCK_THREADS;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:        tile_size            = block_threads * items_per_thread;
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_device.cuh:        cudaError_t retval   = MaxSmOccupancy(sm_occupancy, kernel_ptr, block_threads);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_debug.cuh:        printf("CUDA error %d [block (%d,%d,%d) thread (%d,%d,%d), %s, %d]\n", error, blockIdx.z, blockIdx.y, blockIdx.x, threadIdx.z, threadIdx.y, threadIdx.x, filename, line);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_debug.cuh:            #define _CubLog(format, ...) printf("[block (%d,%d,%d), thread (%d,%d,%d)]: " format, blockIdx.z, blockIdx.y, blockIdx.x, threadIdx.z, threadIdx.y, threadIdx.x, __VA_ARGS__);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_debug.cuh:              printf(format, blockIdx.z, blockIdx.y, blockIdx.x, threadIdx.z, threadIdx.y, threadIdx.x, args...);
debug_tools/WatchYourStep/ptxjitplus/inc/cub/util_debug.cuh:            #define _CubLog(format, ...) va_printf("[block (%d,%d,%d), thread (%d,%d,%d)]: " format, __VA_ARGS__);
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_CTX_SCHED_BLOCKING_SYNC = 0x04, /**< Set blocking synchronization as default scheduling */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_CTX_BLOCKING_SYNC       = 0x04, /**< Set blocking synchronization as default scheduling \deprecated */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_EVENT_BLOCKING_SYNC  = 1, /**< Event uses blocking synchronization */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK = 1,              /**< Maximum number of threads per block */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X = 2,                    /**< Maximum block dimension X */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y = 3,                    /**< Maximum block dimension Y */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z = 4,                    /**< Maximum block dimension Z */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK = 8,        /**< Maximum shared memory available per block in bytes */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK = 12,           /**< Maximum number of 32-bit registers available per block */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    int maxThreadsPerBlock;     /**< Maximum number of threads per block */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    int maxThreadsDim[3];       /**< Maximum size of each dimension of a block */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    int sharedMemPerBlock;      /**< Shared memory available per block in bytes */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:    int regsPerBlock;           /**< 32-bit registers available per block */
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:     * The maximum number of threads per block, beyond which a launch of the
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:     * IN: Specifies minimum number of threads per block to target compilation
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:     * registers) such that a block with the given number of threads should be
debug_tools/WatchYourStep/ptxjitplus/inc/dynlink/cuda_drvapi_dynlink_cuda.h:                                         unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ,
debug_tools/WatchYourStep/ptxjitplus/inc/helper_image.h:sdkReadFileBlocks(const char *filename, T **data, unsigned int *len, unsigned int block_num, unsigned int block_size, bool verbose)
debug_tools/WatchYourStep/ptxjitplus/inc/helper_image.h:    data[block_num] = (T *) malloc(block_size);
debug_tools/WatchYourStep/ptxjitplus/inc/helper_image.h:    fseek(fh, block_num * block_size, SEEK_SET);
debug_tools/WatchYourStep/ptxjitplus/inc/helper_image.h:    *len = fread(data[block_num], sizeof(T), block_size/sizeof(T), fh);
debug_tools/WatchYourStep/ptxjitplus/ptxjitplus.h: *     int tid = blockIdx.x * blockDim.x + threadIdx.x;
debug_tools/WatchYourStep/ptxjitplus/ptxjitplus.cpp:dim3 gridDim, blockDim;
debug_tools/WatchYourStep/ptxjitplus/ptxjitplus.cpp:    fscanf(fin, "%u,%u,%u %u,%u,%u\n", &gridDim.x, &gridDim.y, &gridDim.z, &blockDim.x, &blockDim.y, &blockDim.z);
debug_tools/WatchYourStep/ptxjitplus/ptxjitplus.cpp:    CUDAAPI cuLaunchKernel(hKernel, gridDim.x, gridDim.y, gridDim.z, blockDim.x, blockDim.y, blockDim.z, 
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/GTX480/gpuwattch_gtx480.xml:			<param name="block_size" value="64"/><!--B-->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_TITANX/gpuwattch_gtx480.xml:			<param name="block_size" value="64"/><!--B-->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_GTX1080/gpuwattch_gtx1080Ti.xml:			<param name="block_size" value="64"/><!--B-->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/GeForceGTX750Ti/gpuwattch_gtx750Ti.xml:			<param name="block_size" value="64"/><!--B-->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/SM6_P100/gpuwattch_gtx480.xml:			<param name="block_size" value="64"/><!--B-->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/deprecated-cfgs/QuadroFX5600/gpuwattch_quadrofx5600.xml:			<param name="block_size" value="64"/><!--B-->
configs/deprecated-cfgs/QuadroFX5600/gpgpusim.config:#8192 (registers per block as written by device Query and which used in this option in our other configurations but this break some benchmarks execution! it does not affect performance modeling though)
configs/tested-cfgs/SM7_TITANV/gpgpusim.config:-gpgpu_registers_per_block 65536
configs/tested-cfgs/SM7_TITANV/gpgpusim.config:-gpgpu_shmem_per_block 65536
configs/tested-cfgs/SM75_RTX2060/gpgpusim.config:-gpgpu_registers_per_block 65536
configs/tested-cfgs/SM75_RTX2060/gpgpusim.config:-gpgpu_shmem_per_block 65536
configs/tested-cfgs/SM7_QV100/gpgpusim.config:-gpgpu_registers_per_block 65536
configs/tested-cfgs/SM7_QV100/gpgpusim.config:-gpgpu_shmem_per_block 65536
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:			<!-- fetch_width determins the size of cachelines of L1 cache block -->
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width,associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity,bank, throughput w.r.t. core clock, latency w.r.t. core clock,-->
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:				<!-- the parameters are capacity,block_width, associativity, bank, throughput w.r.t. core clock, latency w.r.t. core clock,output_width, cache policy -->
configs/tested-cfgs/SM2_GTX480/gpuwattch_gtx480.xml:			<param name="block_size" value="64"/><!--B-->
doc/doxygen/gpgpu-sim.doxygen:# treat a multi-line C++ special comment block (i.e. a block of //! or ///
doc/doxygen/gpgpu-sim.doxygen:# The new default is to treat a multi-line C++ comment block as a detailed
doc/doxygen/gpgpu-sim.doxygen:# documentation blocks found inside the body of a function.
doc/doxygen/gpgpu-sim.doxygen:# If set to NO (the default) these blocks will be appended to the
doc/doxygen/gpgpu-sim.doxygen:# function's detailed documentation block.
doc/doxygen/gpgpu-sim.doxygen:# doxygen to hide any special comment blocks from generated source code
doc/doxygen/doxygen.css:	display:block;
Binary file lib/gcc-9.4.0/cuda-11060/release/libcudart.so matches
Binary file lib/gcc-9.4.0/cuda-10010/release/libcudart.so matches
Binary file lib/gcc-8.4.0_b/cuda-11060/release/libcudart.so matches
Binary file lib/gcc-8.4.0_b/cuda-10010/release/libcudart.so matches
Binary file lib/gcc-8.4.0/cuda-10010/release/libcudart.so matches
Binary file lib/gcc-7.5.0/cuda-10010/release/libcudart.so matches
libopencl/opencl_runtime_api.cc:	   ctx->func_sim->gpgpu_ptx_sim_memcpy_symbol( "%_global_block_offset", zeros, 3 * sizeof(int), 0, 1, gpu );
libopencl/opencl_runtime_api.cc:                    cl_bool             blocking_read,
libopencl/opencl_runtime_api.cc:   if( !blocking_read ) 
libopencl/opencl_runtime_api.cc:      gpgpusim_opencl_warning(__my_func__,__LINE__, "non-blocking read treated as blocking read");
libopencl/opencl_runtime_api.cc:                     cl_bool            blocking_write, 
libopencl/opencl_runtime_api.cc:   if( !blocking_write ) 
libopencl/opencl_runtime_api.cc:      gpgpusim_opencl_warning(__my_func__,__LINE__, "non-blocking write treated as blocking write");
libopencl/opencl_runtime_api.cc:                   cl_bool          blocking_map, 
Binary file .git/index matches
.git/hooks/update.sample:# An example hook script to block unannotated tags from entering.
Binary file .git/objects/pack/pack-7e59425edc22c81f95e75c325a19370de0d831b9.pack matches
